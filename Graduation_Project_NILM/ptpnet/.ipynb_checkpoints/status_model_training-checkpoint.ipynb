{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saPKpECRVlln"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, matthews_corrcoef\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPxd9q-PhAXB"
   },
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9g6B69jPJkz"
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('../ukdale/ukdale.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y3vM6zQz5DC"
   },
   "outputs": [],
   "source": [
    "def resample_meter(store=None, building=1, meter=1, period='1min', cutoff=1000.):\n",
    "    key = '/building{}/elec/meter{}'.format(building,meter)\n",
    "    m = store[key]\n",
    "    v = m.values.flatten()\n",
    "    t = m.index\n",
    "    s = pd.Series(v, index=t).clip(0.,cutoff)\n",
    "    s[s<10.] = 0.\n",
    "    return s.resample('1s').ffill(limit=300).fillna(0.).resample(period).mean().tz_convert('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_a-pqdClkRmO"
   },
   "outputs": [],
   "source": [
    "def get_series(datastore, house, label, cutoff):\n",
    "    filename = '../ukdale/house_%1d_labels.dat' %house\n",
    "    print(filename)\n",
    "    labels = pd.read_csv(filename, delimiter=' ', header=None, index_col=0).to_dict()[1]\n",
    "    \n",
    "    for i in labels:\n",
    "        if labels[i] == label:\n",
    "            print(i, labels[i])\n",
    "            s = resample_meter(store, house, i, '1min', cutoff)\n",
    "            #s = resample_meter(store, house, i, '6s', cutoff)\n",
    "    \n",
    "    s.index.name = 'datetime'\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6lRVLSIzk6B5",
    "outputId": "bc980e76-1dca-4841-cddd-f10d6ee69d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ukdale/house_1_labels.dat\n",
      "1 aggregate\n",
      "../ukdale/house_1_labels.dat\n",
      "10 kettle\n",
      "../ukdale/house_1_labels.dat\n",
      "12 fridge\n",
      "../ukdale/house_1_labels.dat\n",
      "5 washing_machine\n",
      "../ukdale/house_1_labels.dat\n",
      "13 microwave\n",
      "../ukdale/house_1_labels.dat\n",
      "6 dishwasher\n"
     ]
    }
   ],
   "source": [
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_1 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_1.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_1_train = ds_1[pd.to_datetime('2013-04-12 00:00:00+00:00'):pd.to_datetime('2014-12-15 00:00:00+00:00')]\n",
    "ds_1_valid = ds_1[pd.to_datetime('2014-12-15 00:00:00+00:00'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ukdale/house_2_labels.dat\n",
      "1 aggregate\n",
      "../ukdale/house_2_labels.dat\n",
      "8 kettle\n",
      "../ukdale/house_2_labels.dat\n",
      "14 fridge\n",
      "../ukdale/house_2_labels.dat\n",
      "12 washing_machine\n",
      "../ukdale/house_2_labels.dat\n",
      "15 microwave\n",
      "../ukdale/house_2_labels.dat\n",
      "13 dish_washer\n"
     ]
    }
   ],
   "source": [
    "house = 2\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dish_washer', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_2 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_2.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_2_train = ds_2[pd.to_datetime('2013-05-22 00:00:00+00:00'):pd.to_datetime('2013-10-03 00:00:00+00:00')]\n",
    "ds_2_valid = ds_2[pd.to_datetime('2013-10-03 00:00:00+00:00'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ukdale/house_3_labels.dat\n",
      "1 aggregate\n",
      "../ukdale/house_3_labels.dat\n",
      "2 kettle\n"
     ]
    }
   ],
   "source": [
    "house = 3\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = 0.*m\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_3 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_3.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_3_train = ds_3[pd.to_datetime('2013-02-27 00:00:00+00:00'):pd.to_datetime('2013-04-01 00:00:00+00:00')]\n",
    "ds_3_valid = ds_3[pd.to_datetime('2013-04-01 00:00:00+00:00'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ukdale/house_4_labels.dat\n",
      "1 aggregate\n",
      "../ukdale/house_4_labels.dat\n",
      "3 kettle_radio\n",
      "../ukdale/house_4_labels.dat\n",
      "5 freezer\n"
     ]
    }
   ],
   "source": [
    "house = 4\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle_radio', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_4 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_4.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_4_train = ds_4[pd.to_datetime('2013-03-09 00:00:00+00:00'):pd.to_datetime('2013-09-24 00:00:00+00:00')]\n",
    "ds_4_valid = ds_4[pd.to_datetime('2013-09-24 00:00:00+00:00'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ukdale/house_5_labels.dat\n",
      "1 aggregate\n",
      "../ukdale/house_5_labels.dat\n",
      "18 kettle\n",
      "../ukdale/house_5_labels.dat\n",
      "19 fridge_freezer\n",
      "../ukdale/house_5_labels.dat\n",
      "24 washer_dryer\n",
      "../ukdale/house_5_labels.dat\n",
      "23 microwave\n",
      "../ukdale/house_5_labels.dat\n",
      "22 dishwasher\n"
     ]
    }
   ],
   "source": [
    "house = 5\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge_freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washer_dryer', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_5 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_5.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_5_train = ds_5[pd.to_datetime('2014-06-29 00:00:00+00:00'):pd.to_datetime('2014-09-1 00:00:00+00:00')]\n",
    "ds_5_valid = ds_5[pd.to_datetime('2014-09-1 00:00:00+00:00'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1_train.reset_index().to_feather('../ukdale/UKDALE_1_train.feather')\n",
    "ds_2_train.reset_index().to_feather('../ukdale/UKDALE_2_train.feather')\n",
    "ds_3_train.reset_index().to_feather('../ukdale/UKDALE_3_train.feather')\n",
    "ds_4_train.reset_index().to_feather('../ukdale/UKDALE_4_train.feather')\n",
    "ds_5_train.reset_index().to_feather('../ukdale/UKDALE_5_train.feather')\n",
    "\n",
    "ds_1_valid.reset_index().to_feather('../ukdale/UKDALE_1_valid.feather')\n",
    "ds_2_valid.reset_index().to_feather('../ukdale/UKDALE_2_valid.feather')\n",
    "ds_3_valid.reset_index().to_feather('../ukdale/UKDALE_3_valid.feather')\n",
    "ds_4_valid.reset_index().to_feather('../ukdale/UKDALE_4_valid.feather')\n",
    "ds_5_valid.reset_index().to_feather('../ukdale/UKDALE_5_valid.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the feather dataframe resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status(app, threshold, min_off, min_on):\n",
    "    condition = app > threshold\n",
    "    # Find the indicies of changes in \"condition\"\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "\n",
    "    # We need to start things after the change in \"condition\". Therefore, \n",
    "    # we'll shift the index by 1 to the right.\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        # If the start of condition is True prepend a 0\n",
    "        idx = np.r_[0, idx]\n",
    "\n",
    "    if condition[-1]:\n",
    "        # If the end of condition is True, append the length of the array\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "\n",
    "    # Reshape the result into two columns\n",
    "    idx.shape = (-1,2)\n",
    "    on_events = idx[:,0].copy()\n",
    "    off_events = idx[:,1].copy()\n",
    "    assert len(on_events) == len(off_events)\n",
    "\n",
    "    if len(on_events) > 0:\n",
    "        off_duration = on_events[1:] - off_events[:-1]\n",
    "        off_duration = np.insert(off_duration, 0, 1000.)\n",
    "        on_events = on_events[off_duration > min_off]\n",
    "        off_events = off_events[np.roll(off_duration, -1) > min_off]\n",
    "        assert len(on_events) == len(off_events)\n",
    "\n",
    "        on_duration = off_events - on_events\n",
    "        on_events = on_events[on_duration > min_on]\n",
    "        off_events = off_events[on_duration > min_on]\n",
    "\n",
    "    s = app.copy()\n",
    "    #s.iloc[:] = 0.\n",
    "    s[:] = 0.\n",
    "\n",
    "    for on, off in zip(on_events, off_events):\n",
    "        #s.iloc[on:off] = 1.\n",
    "        s[on:off] = 1.\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Power(data.Dataset):\n",
    "    def __init__(self, meter=None, appliance=None, status=None, \n",
    "                 length=256, border=680, max_power=1., train=False):\n",
    "        self.length = length\n",
    "        self.border = border\n",
    "        self.max_power = max_power\n",
    "        self.train = train\n",
    "\n",
    "        self.meter = meter.copy()/self.max_power\n",
    "        self.appliance = appliance.copy()/self.max_power\n",
    "        self.status = status.copy()\n",
    "\n",
    "        self.epochs = (len(self.meter) - 2*self.border) // self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i = index * self.length + self.border\n",
    "        if self.train:\n",
    "            i = np.random.randint(self.border, len(self.meter) - self.length - self.border)\n",
    "\n",
    "        x = self.meter.iloc[i-self.border:i+self.length+self.border].values.astype('float32')\n",
    "        y = self.appliance.iloc[i:i+self.length].values.astype('float32')\n",
    "        s = self.status.iloc[i:i+self.length].values.astype('float32')\n",
    "\n",
    "        return x, y, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs\n",
    "    \n",
    "    def denormalize(self):\n",
    "        self.meter *= self.max_power\n",
    "        self.appliance *= self.max_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4989, -0.2884, -0.8189,  ...,  0.8163,  1.4442, -0.2479]],\n",
      "\n",
      "        [[ 1.1077, -0.3143, -0.8958,  ...,  1.8558,  0.4930, -0.4474]],\n",
      "\n",
      "        [[ 2.1839,  0.5686, -0.7599,  ...,  1.3347,  1.3162,  0.6596]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2727,  1.0946,  0.4361,  ...,  0.5417,  0.3412,  0.5118]],\n",
      "\n",
      "        [[ 1.9294, -0.9460, -1.4159,  ...,  0.3134, -2.4235, -0.7860]],\n",
      "\n",
      "        [[-0.8944,  0.3422, -2.1854,  ...,  0.6658,  1.5647,  0.7254]]])\n",
      "torch.Size([32, 3, 480])\n",
      "327619\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1**k, features * 2**k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2**k, features * 4**k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4**k, features * 8**k, kernel_size=3, padding=0)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features*8**k, features*2**k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features*8**k, features*2**k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features*8**k, features*2**k, kernel_size=20)\n",
    "        self.tpool4 = TemporalPooling(features*8**k, features*2**k, kernel_size=30)\n",
    "        \n",
    "        self.decoder = Decoder(2*features * 8**k, features * 1**k, kernel_size=p**3, stride=p**3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1**k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        tp1 = self.tpool1(enc4)\n",
    "        tp2 = self.tpool2(enc4)\n",
    "        tp3 = self.tpool3(enc4)\n",
    "        tp4 = self.tpool4(enc4)\n",
    "\n",
    "        dec = self.decoder(torch.cat([enc4, tp1, tp2, tp3, tp4], dim=1))\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        return act\n",
    "\n",
    "x = torch.randn(32, 1, 60 * 8 + 2 * 16)\n",
    "model = PTPNet(1, 3, 32)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the test loss as the model trains\n",
    "    test_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    # to track the average test loss per epoch as the model trains\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    #patience = 10\n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        ##################    \n",
    "        # test the model #\n",
    "        ##################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        #early_stopping(valid_loss, model)\n",
    "        #if (early_stopping.early_stop and (epoch > 80)):\n",
    "        #    break\n",
    "        \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'Validation loss decreased ({min_loss:.6f} --> {valid_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_activation(model, loader, a):\n",
    "    x_true = []\n",
    "    s_true = []\n",
    "    p_true = []\n",
    "    s_hat = []\n",
    "    p_hat = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, p, s in loader:\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p = p.permute(0,2,1)[:,a,:]\n",
    "            s = s.permute(0,2,1)[:,a,:]\n",
    "\n",
    "            sh = model(x)\n",
    "            \n",
    "            sh = torch.sigmoid(sh[:,a,:])\n",
    "\n",
    "            s_hat.append(sh.contiguous().view(-1).detach().cpu().numpy())\n",
    "            \n",
    "            x_true.append(x[:,:,BORDER:-BORDER].contiguous().view(-1).detach().cpu().numpy())\n",
    "            s_true.append(s.contiguous().view(-1).detach().cpu().numpy())\n",
    "            p_true.append(p.contiguous().view(-1).detach().cpu().numpy())\n",
    "    x_true = np.hstack(x_true)\n",
    "    s_true = np.hstack(s_true)\n",
    "    p_true = np.hstack(p_true)\n",
    "    s_hat = np.hstack(s_hat)\n",
    "\n",
    "    return x_true, p_true, s_true, s_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 30., 3.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather('../ukdale/UKDALE_%d_train.feather' %(i+1))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fridge             3515\n",
       "dish_washer          98\n",
       "washing_machine      54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ds_status[1].diff()==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fridge</th>\n",
       "      <th>dish_washer</th>\n",
       "      <th>washing_machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>192961.000000</td>\n",
       "      <td>192961.000000</td>\n",
       "      <td>192961.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.378014</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>0.011339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.484892</td>\n",
       "      <td>0.167736</td>\n",
       "      <td>0.105880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fridge    dish_washer  washing_machine\n",
       "count  192961.000000  192961.000000    192961.000000\n",
       "mean        0.378014       0.028975         0.011339\n",
       "std         0.484892       0.167736         0.105880\n",
       "min         0.000000       0.000000         0.000000\n",
       "25%         0.000000       0.000000         0.000000\n",
       "50%         0.000000       0.000000         0.000000\n",
       "75%         1.000000       0.000000         0.000000\n",
       "max         1.000000       1.000000         1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_status[1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(5+0)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_test  = [Power(ds_meter[i][int(0.9*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.9*ds_len[i]):],\n",
    "                        ds_status[i][int(0.9*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_train_seen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                ds_house_train[1], \n",
    "                                                #ds_house_train[2], \n",
    "                                                #ds_house_train[3],\n",
    "                                                ds_house_train[4]\n",
    "                                                ])\n",
    "ds_valid_seen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                #ds_house_valid[1], \n",
    "                                                #ds_house_valid[2], \n",
    "                                                #ds_house_valid[3], \n",
    "                                                #ds_house_valid[4]\n",
    "                                                ])\n",
    "\n",
    "dl_train_seen = DataLoader(dataset = ds_train_seen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_seen = DataLoader(dataset = ds_valid_seen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_seen = DataLoader(dataset = ds_house_test[0], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "ds_train_unseen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                  #ds_house_train[1], \n",
    "                                                  #ds_house_train[2], \n",
    "                                                  #ds_house_train[3], \n",
    "                                                  ds_house_train[4]\n",
    "                                                  ])\n",
    "ds_valid_unseen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                  #ds_house_valid[1], \n",
    "                                                  #ds_house_valid[2], \n",
    "                                                  #ds_house_valid[3], \n",
    "                                                  ds_house_valid[4]\n",
    "                                                  ])\n",
    "dl_train_unseen = DataLoader(dataset = ds_train_unseen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_unseen = DataLoader(dataset = ds_valid_unseen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_unseen = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_test[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dl_house_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 1.5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNIAAAKZCAYAAAB9QAZGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD2jElEQVR4nOzdd5xcdd3+/2vatvReIJVAIEAKSQgBBAKBJCDFnyggCHIrKhpbULxjASzfm1tvFERRlF6kqQhICSWQ0EJLCDWBEEJ6JWWzm60z8/tj5pw5MzuzU3bmzDlnXs/HI5Kdndmdnd0cz7n2en8+vmg0GhUAAAAAAACATvnL/QQAAAAAAAAANyBIAwAAAAAAAHJAkAYAAAAAAADkgCANAAAAAAAAyAFBGgAAAAAAAJADgjQAAAAAAAAgBwRpAAAAAAAAQA4I0gAAAAAAAIAcEKQBAAAAAAAAOSBIAwAAAAAAAHJQ0iDt+eef1+mnn66hQ4fK5/PpoYce6vT+ixYtks/n6/Bny5YtSfe74YYbNHLkSNXU1GjatGl67bXXSvhVAAAAAAAAACUO0hobGzVhwgTdcMMNeT3ugw8+0ObNm80/AwcONN93//33a968ebryyiu1bNkyTZgwQbNmzdK2bduK/fQBAAAAAAAAky8ajUZt+UQ+n/7973/rrLPOynifRYsWacaMGdq1a5d69+6d9j7Tpk3T1KlT9ac//UmSFIlENGzYMH3nO9/Rf//3f5fgmQMAAAAAAABSsNxPIJ2JEyeqpaVFhx12mK666iodc8wxkqTW1lYtXbpU8+fPN+/r9/s1c+ZMLVmyJOPHa2lpUUtLi/l2JBLRzp071a9fP/l8vtJ9IQAAAAAAAHC0aDSqvXv3aujQofL7Ox/edFSQNmTIEN14442aMmWKWlpadPPNN+uEE07Qq6++qiOOOEI7duxQOBzWoEGDkh43aNAgrVy5MuPHvfrqq/WLX/yi1E8fAAAAAAAALrV+/Xrtv//+nd7HUUHa2LFjNXbsWPPto48+WqtXr9a1116ru+66q+CPO3/+fM2bN898e8+ePRo+fLjWr1+vnj17duk5AwAAVJJ58+bplltu0Y9//GNt2LBBf//733XllVcmnWuh9NauXavx48erpqZGW7du1a233qof/OAHOu2003TPPfeU++kBAOAq9fX1GjZsmHr06JH1vo4K0tI58sgj9eKLL0qS+vfvr0AgoK1btybdZ+vWrRo8eHDGj1FdXa3q6uoOt/fs2ZMgDQAAIA+hUEiSVFtbq27dukmSAoEA51Q2q6urkyQFg0H17NmT7wUAAEWQy/JfJd21sxiWL1+uIUOGSJKqqqo0efJkLVy40Hx/JBLRwoULNX369HI9RQAAgIoRiUQkxU40q6qqJEltbW3lfEoVKRwOS4oFZ5LM9VyM7w8AACiNkjbSGhoa9NFHH5lvr1mzRsuXL1ffvn01fPhwzZ8/Xxs3btSdd94pSbruuus0atQoHXrooWpubtbNN9+sZ599Vk899ZT5MebNm6eLLrpIU6ZM0ZFHHqnrrrtOjY2Nuvjii0v5pQAAAECxxXilWHBjtNNaW1vL+ZQqEkEaAADlUdIg7Y033tCMGTPMt421My666CLdfvvt2rx5s9atW2e+v7W1VZdddpk2btyouro6jR8/Xs8880zSxzjnnHO0fft2XXHFFdqyZYsmTpyoBQsWdNiAAAAAAMVnbaQZQRqNNPsRpAEAUB4lDdJOOOEE87eW6dx+++1Jb19++eW6/PLLs37cuXPnau7cuV19egAAAMiTtZHGaGf5EKQBAFAejl8jDQAAAM6RrpHGaKf9CNIAACgPgjQAAADkLN0aaTTS7EeQBgBAeRCkAQAAIGfs2ukMBGkAAJQHQRoAAAByxq6dzkCQBgBAeRCkAQAAIGfs2ukMBGkAAJQHQRoAAAByxhppzkCQBgBAeRCkAQAAIGeskeYMqUGaz+eTlAg6AQBAaRCkAQAAIGeskeYMNNIAACgPgjQAAADkjDXSnIEgDQCA8iBIAwAAQM6sjTRGO8uHIA0AgPIgSAMAAEDO0jXSGO20H0EaAADlQZAGAACAnLFrpzMQpAEAUB4EaQAAAMiZEaSxa2d5EaQBAFAeBGkAAADIGaOdzkCQBgBAeRCkAQAAIGeMdjoDQRoAAOVBkAYAAICcpWukEaTZjyANAIDyIEgDAABAzqyNNNZIKx+CNAAAyoMgDQAAADljjTRnIEgDAKA8CNIAAACQM9ZIc4ZMQZrx/QEAAKVBkAYAAICcWRtpjHaWT2qQ5vP5JNFIAwCg1AjSAAAAkLN0jbRwOEyAYzNGOwEAKA+CNAAAAOQs3RppEq00uxGkAQBQHgRpAAAAyFm6XTslgjS7EaQBAFAeBGkAAADIWaZGGjt32osgDQCA8iBIAwAAQM6sjTQjxJFopNmNIA0AgPIgSAMAAEDOrI00ayuNIM1eBGkAAJQHQRoAAAByZm2kSTLXSSNIsxdBGgAA5UGQBgAAgJxZG2mSzEYaa6TZiyANAIDyIEgDAABAzlIbaYx2lgdBGgAA5UGQBgAAgJwZQZrRSGO0szwI0gAAKA+CNAAAAOTMCGpSG2mMdtorU5BmBJ0AAKA0CNIAAACQs9RGGqOd5UEjDQCA8iBIAwAAQM5SNxtgtLM8UoM04/tBkAYAQGkRpAEAACBnmTYbYLTTXjTSAAAoD4I0AAAA5Cy1kcZoZ3kQpAEAUB4EaQAAAMhZpkYaQZq9CNIAACgPgjQAAADkLNMaaYx22quzXTvZuRMAgNIhSAMAAEDOaKQ5Q3t7u6SOQZokgjQAAEqIIA0AAAA5Y400Z8jUSJMY7wQAoJQI0gAAAJCz1EaaMdpJkGYvgjQAAMqDIA0AAAA5y9RIY400exGkAQBQHgRpAAAAyBlrpDkDQRoAAOVBkAYAAICcZdq1kyDNXgRpAACUB0EaAAAAcpapkcZop706C9LYtRMAgNIhSAMAAEDO2LXTGWikAQBQHgRpAAAAyBlrpDlDapBmBJsSQRoAAKVEkAYAAICcZVojjdFOe9FIAwCgPAjSAAAAkDMaac5AkAYAQHkQpAEAACBnRpDGGmnlxWgnAADlQZAGAACAnBkhjdGAMkY7CdLslRqkSYnvCUEaAAClQ5AGAACAnGVqpLFGmr0I0gAAKA+CNAAAAOQstZHGaGd5EKQBAFAeBGkAAADIWWojjdHO8iBIAwCgPAjSAAAAkDMjpGG0s7wI0gAAKA+CNAAAAOTMaKQx2lleBGkAAJQHQRoAAABylqmRRpBmr86CNCPsBAAAxUeQBgAAgJylNtKMNdIY7bQXjTQAAMqDIA0AAAA5o5HmDARpAACUB0EaAAAAcsYaac6QLkgzwk2CNAAASocgDQAAADlLbaQZo50EafaikQYAQHkQpAEAACBnmRpprJFmL4I0AADKgyANAAAAOUttpBlBjhHswB4EaQAAlAdBGgAAAHKW2kgz/mvcDnsQpAEAUB4EaQAAAMhZaiONBe7LgyANAIDyIEgDAABAzmikOQNBGgAA5UGQBgAAgJwZgRmNtPIiSAMAoDxKGqQ9//zzOv300zV06FD5fD499NBDnd7/wQcf1Mknn6wBAwaoZ8+emj59up588smk+1x11VXy+XxJfw4++OASfhUAAAAw0EhzBoI0AADKo6RBWmNjoyZMmKAbbrghp/s///zzOvnkk/X4449r6dKlmjFjhk4//XS9+eabSfc79NBDtXnzZvPPiy++WIqnDwAAAAtrWEYjrbw6C9IINQEAKJ1gKT/4nDlzNGfOnJzvf9111yW9/T//8z96+OGH9Z///EeTJk0ybw8Ggxo8eHCxniYAAAByYA3LaKSVF400AADKw9FrpEUiEe3du1d9+/ZNun3VqlUaOnSoRo8erfPPP1/r1q3r9OO0tLSovr4+6Q8AAADyQyPNGayvNUEaAAD2cnSQds0116ihoUFf/OIXzdumTZum22+/XQsWLNBf/vIXrVmzRp/5zGe0d+/ejB/n6quvVq9evcw/w4YNs+PpAwAAeAqNNGcw2mgSQRoAAHZzbJB2zz336Be/+IUeeOABDRw40Lx9zpw5+sIXvqDx48dr1qxZevzxx7V792498MADGT/W/PnztWfPHvPP+vXr7fgSAAAAPIVGmjNkCtL4XgAAUHolXSOtUPfdd5++9rWv6R//+IdmzpzZ6X179+6tgw46SB999FHG+1RXV6u6urrYTxMAAKCiWAMaI7ShkWY/GmkAAJSP4xpp9957ry6++GLde++9Ou2007Lev6GhQatXr9aQIUNseHYAAACVyxqWGaENLSj7EaQBAFA+JW2kNTQ0JDXF1qxZo+XLl6tv374aPny45s+fr40bN+rOO++UFBvnvOiii/SHP/xB06ZN05YtWyRJtbW16tWrlyTphz/8oU4//XSNGDFCmzZt0pVXXqlAIKDzzjuvlF8KAABAxaOR5gwEaQAAlE9JG2lvvPGGJk2apEmTJkmS5s2bp0mTJumKK66QJG3evDlpx82//e1vam9v17e//W0NGTLE/PO9733PvM+GDRt03nnnaezYsfriF7+ofv366ZVXXtGAAQNK+aUAQNm9//77ev3118v9NABUMBppzkCQBgBA+ZS0kXbCCSd0+tvJ22+/PentRYsWZf2Y9913XxefFQC4z759+3Tsscdq9+7dWrhwoWbMmFHupwSgAtFIcwZrkGa8/ta/E6QBAFA6jlsjDQDQ0ZNPPqldu3YpGo3q/PPP17Zt28r9lDyroaFBf/rTn3TXXXeV+6kAjkMjzRmMIM3v95uvv/G2xPcCAIBSIkgDABd48MEHzb9v3rxZX/ziF/Xss8+qpaWljM/Ke26++WaNHj1a3/nOd3ThhRdqzZo1Ge/La49KRCPNGYwgzTrWKRGkAQBgB4I0AHC41tZW/ec//5Ek3XjjjaqpqdHixYt10kknaezYsdqwYUOZn6E3NDY26utf/7q2b99u3masSbdgwQL94he/MC9Of/7zn6t379568803y/JcgXKhkeYM2YI0Qk0AAEqHIA0AHO7ZZ5/Vnj17NHjwYF1yySV67rnndOGFF6pfv35au3atfvOb35T7KXpCQ0ODefH51a9+VZK0dOlSRaNRXXTRRbrqqqv0zDPPSJL+9a9/qbm5WW+99VbZni9QDjTSnIFGGgAA5UOQBgAOdf/99+vaa6/VTTfdJEn63Oc+J7/fr6OOOkp33HGHHnjgAUmxccStW7fqd7/7nY444ggtWLCgnE/btZqbmyVJtbW1OuqooyTFdp9esWKFuSbd0qVLtW/fPn3wwQeSCA5Qeaw/80aQRiPNfu3t7ZKkYDB53zCCNAAASq+ku3YCAAqzdetWnXfeeUkXrZ///OeT7jNjxgxNmzZNr776qmbPnq3ly5dLkk499VT9+te/1vz585MWoUbnjCCtpqZGkydPlhQLzp577jnzPsuWLdO7775rXqRysYpKQyPNGWikAQBQPjTSAMCBjB06/X6/qqurddhhh+m4445Luo/P59NPfvITSTJDtGnTpikajeqnP/2pHn30UbuftqtZg7RDDz1U1dXV2rNnj2699VbzPsuWLTNfa4mLVVQeIywzAhuJRlo5EKQBAFA+BGkA4EDGjpADBgxQfX293n77bYVCoQ73++xnP6vx48dLkubNm6clS5bojDPOkCStWrXKvifsAdYgraqqynxdly1bZt7n448/1uLFi823uVhFpTGCNGvblUaa/YxjjzXQlAg14Q5tbW269tpr9dFHH5X7qZRVS0uLPvvZz7LWLeBCBGkA4EBGkFZdXa2qqqqMI5p+v1+PP/64Hn/8cV1zzTXy+Xzq1auXJC6k8tXU1CQpFqRJ0pQpU8z31dTUaOjQoZKkBx980Lyd1xiVJl2AQ3hjv3TNQOvbfC/gZHfccYfmzZun7373u+V+KmX10ksv6bHHHtM111xT7qcCIE8EaQDgQNZ2VDb77bef5syZw8LfXZT6mhvrpEnS9OnTNW3atKT7SbzGqDw00pwhUyONIA1u8Oqrr0qKBUmV/LO6Zs0aSdKOHTvMX6ACcAeCNABwIGsjLV9c1BYmNUizNtKOP/54TZo0qcNjKvkCAJWps0Yaxxz7EKTBzZYuXSpJqq+vN3fBrkRGkCZJmzZtKuMzAZAvgjQAcKB8GmmpuJAqTOprPm7cOPPvxx9/vI444ogOj+E1RqXprJFmfT9KiyANbtXS0qJ3333XfNtop1Uia5C2cePGMj4TAPkKlvsJAAA6KkYjjQup/KQGaaFQSH/+85/1wQcf6LjjjtPWrVs7PIbQAJWms0aa8f7UnSRRfARpcKv33ntPbW1t5tuvvvqqvvKVr5TvCZURQRrgXgRpAOBARpBGI80+6VqAF198sfn3IUOGaPDgwdqyZYt5G68xKg2NNGcgSINbGTthh0IhtbW10UiLY7QTcBdGOwHAgYxQp5BGGpsNFCaXcdqLL75YgwcP1gknnCCJ1xiVJ5dGGkqPIA1uZQRpZ599tiTp7bff1r59+8r5lMqiqakp6RdzNNIAdyFIAwAHYrMB+xlBWm1tbcb7/M///I82b96skSNHSuJiFZWHRpozGMce6/dB4vgP5zOCtDPOOENDhgxROBw2b6skn3zySdLbBGmAuxCkAYADsdmA/fJ5zXmNUanSBTg00uxnBGU00uAm7e3teuuttyRJkydP1rRp0ySVZsOBTz75RF/96ld17LHH6tBDD9XChQuL/jm6wjrWKRGkAW5DkAYADsRmA/bLJ0hjfBaVKl2AYw3SaELZg9FOuNGKFSvU3NysHj166IADDihpkHb11Vfr1ltv1UsvvaT3339ft99+e9E/R1cYQVq/fv0kEaQBbkOQBgAOxGYD9qORBmSXrpFmDXP4N2EPgjS40RtvvCFJOuKII+T3+zVx4kRJsYCt2F5++WVJ0oknnihJWr9+fdE/R1cYo53HHnuspFiQxi8iAPcgSAMAB2KzAfs1NTVJIkgDOkMjzRkI0uBGDz74oKREeGSsN7pu3bqifp76+nq99957kqRvf/vbJfkcXWU00o4++mhJsV+g7ty5s5xPCUAeCNIAwIHYbMB+hTTSeI1RaWikOUOmII1fpKCcnn76aX35y1/usP6XJG3btk1PPPGEJOmCCy6QJA0bNkxSLPjavXt30Z7Ha6+9pmg0qlGjRmnq1KmSpA0bNigcDhftc3SV8RodfPDB6t+/vyRp06ZN5XxKAPJAkAYADsRmA/ZjtBPIjkaaM9BIgxNde+21uvvuu3XCCSfo448/Tnrfvffeq3A4rCOPPFIHH3ywJKlbt25miFTMxtgrr7wiSTrqqKM0ZMgQBQIBtbW1aevWrUX7HF1lBGmjRo3S0KFDJbFOGuAmBGkA4EBsNmA/gjQgOxppzkCQBidqbW2VFAvFjj76aJ144omaPXu2nnjiCd15552SpAsvvDDpMcOHD5ckrV27tmjPwxqkBYNB7bfffkX/HIWIRqN6/PHHtWDBAu3atUtSLEgznh9BGuAewXI/AQBAR2w2YD+CNCA7GmnOQJAGJzL+/YdCIW3dutVsgD355JPm7eecc07SY0aMGKFly5YVrZEWjUaTgjQpFtatW7dO69at0/Tp04vyeQqxePFinXbaaebb/fv3V/fu3QnSABciSAMAB2KzAfsRpAHZGRfKNNLKK10zUOLYhPIyfu7+/Oc/q0ePHopEIlqyZIn+9Kc/KRqN6rTTTjNHOQ3FbqStXr1an376qaqrq81dQUeMGKEXX3yx7BsOpAZlxxxzjCQRpAEuRJAGAA7EZgP2yydII6xEpUrXhKKRZr90zUDr2xybUA7Gz12vXr30hS98QZJ03nnn6Ytf/KLuvfde/ehHP+rwmBEjRkgq3hppRhvtiCOOUFVVlaREWFfuIM3Y7OCkk07Stddea+5aSpAGuA9BGgA4EJsN2M94zWtra7Pel9cYlSpdI814OxqN8m/CJox2woky/Vwee+yxOvbYY9M+ptgh18svvywpMdZZis9RqPb2dkmxc7vDDz/cvN3YbIBdOwH3YLMBAHAgNhuwH6OdQHaZLpSNYI1Gmj2yBWl8H1AOmZqSnTEaacUY7YxGo1qwYIEk6fjjjzdvd0qQZjTSAoFA0u1GI23Dhg22PycAhSFIAwAHYrMB+xUSpHGxikqTqZHGccdeNNLgRJnW7uuMEXJt3rzZ3PWzUCtWrNCaNWtUXV2tmTNndvgc5Q7SjEZaMJg8FDZmzBj5fD5t375dW7ZsKcdTA5AngjQAcKCubDbAhVRhaKQB2dFIcwaCNDhRpp/LzgwYMEA1NTWKRqNdbmT95z//kSSdeOKJ6tatm3m7EaTt3LlTDQ0NXfocXZGpkda9e3eNGzdOkvT666/b/rwA5I8gDQAcqCujnVzQFqapqUkSQRrQGRppzkCQBicqZLTT5/MVrTH26KOPSpI++9nPJt3es2dP9erVS5K0fv36Ln2OrsjUSJOkqVOnSpJee+01W58TgMIQpAGAAzHaaT8aaUB2NNKcgSANTlRII00qzjppn376qbnRwGmnndbh/UZYV4y12AplBGmpjTQpEaTRSAPcgV07AcCBGO20VzgcVltbm6TcgjQjNOA1RqWhkeYMBGlwokLWSJOyr2EWiUR08803q76+XvPmzZPf71c4HNaSJUu0YMECffLJJ/L7/YpEIjr88MPNYM5qxIgReuedd8q6Tpox2pmukXbkkUdKigVp0Wg042v4zjvvaPHixfrMZz6jww47TJ988ol27dqlyZMn5/26AygcQRoAOFAujbS/Ll6tl1Z/qhsvOEJ1VYnDORdS+TNebyn9a/7Rtr1avb1Rsw4dLInXGJWLRpozZAo0CflRTqVopDU0NOiiiy7Sgw8+KCk2mnn55ZfrrLPO0htvvNHh/qljnQYjrHvllVd07LHH6pBDDrE9eOqskTZ+/HhVVVVp586dWrNmjUaPHi1Jam1tVVNTk3r16qWXXnpJs2fPNtd5CwQCZjg3d+5cXX/99YRpgE0Y7QQAB8rWSAtHovrjsx/p+Q+368VVO5LeR8iTP+P1ljoGadFoVJfcuVTfuGuplq3bJYnXGJWLRpoz0EiDExWyRpqUCLlef/11vfzyy2ZDvLGxUccdd5wefPBBhUIhSdL111+vQw45RG+88YZ69uypL33pS/rFL36h008/Xcccc4y++c1vpv0cRlh322236dBDD9Vll11W0NfYFZ010qqqqjRhwgRJsdehtbVV119/vYYOHao+ffroxBNPNEO00aNHq66uTuFw2DxP/NOf/qQf/ehH/DIDsAlBGgA4TDQazbrZwEfbGtTQEvvN5qptyTtQ0QzJnxGkBQKBDie4n3y6T2t2NEqSXl+zU1LiIoHXGJUm0+gWxx17EaTBiQod7TzooIMkSW+//baOOeYYTZ48WZs2bdL3vvc9vfnmmxowYIAWL16s6667TpK0d+9ejRs3TsuXL9ff//53XXHFFXrkkUf04osvmqFcqnPPPVezZs3S/vvvL0lp22yl1lkjTUqMd/773//WlClT9L3vfU+ffvqpotGonnvuOTU0NOjEE0/UO++8YzbXGhoa9Ne//lWS9Lvf/U7/+te/7PligArHaCcAOEx7e7t5MZpptHP5+l3m3z9KCdK4kMqfEaQN+P9+potufU1/sYzLvrBqu3m/5et3S+I1RuXK1DghXLYXQRqcqNDRzqOOOkp//vOf9eSTT2rRokV65513NGnSJG3btk0+n0/333+/pk+frunTp6u2tlYrVqzQVVddZe7EmYvhw4drwYIFWrBggebMmaO9e/fm9RyLobNGmpTYcOD++++XJA0YMEC/+tWvNHPmTN1///2qr6/XFVdcobq6OknSyJEjJUlf//rX9eKLL+quu+7SO++8o7PPPrvEXwkAgjQAcBjrmGGmRtqb63abf1+1LXYyeP3CVbrhuY/0xT6x8I0Lqdw1NzfLX9Nd1aOnavGH2/Wjf7ytP31pknw+n16wjM6+RZCGCpetkca/CXsQpMGJCh3t9Pl8uvTSS3XppZdqzZo1Oumkk7RmzRpJ0k9/+lPNmDHDvO/Xv/71Lj3HHj16SJLq6+u79HEKka2RZgRpknTooYfqiSee0LBhwyRJP/nJTzr92L1795aUCOsAlBZBGgA4jHXh+0xBmtGMkmKNtEgkqgfeWK+W9ohWNXeXxIVUPpqbm+ULVplvP/bOZo1b1FPfOG60Xln9qXn7pj3N2lbfzMUqKhaNNGcgSIMTFdpIsxo1apReeOEFXXjhhRo8eLCuvPLKYj09SVLPnj0lyZGNtIMPPljnnnuuwuGw/va3v5nhWC6McI4gDbAHQRoAOIzRSAsGg2l/a9nQ0q4PtsZOAAN+n5rbInpj7S5t2NUkSdraRiMtX83NzfIFQkm3XfPUB9rX2q69Le3qUxdS/+7VWrWtQcvX7+ZiFRWLRpozZAvSCDRRDoWukZZqv/3208KFC4vxlDpwciPN7/fr3nvvLehjE6QB9mKzAQBwmNSNBpav3633Nu0x3//2ht2KRqX9etfqwIGx9tm9r60z37+5Ndas4kIqd9ZGWp+6kM6fNlzRqHTDc6slSUeP6a8jhveRFPt+EBqgUtFIcwYaaXCiYjTSSs1opLW0tJi7g9olWyOtK4wgzQjrAJSWc49yAFChjCCtbv+DdcHNr+qsG17SZ//4on731AdqD0fM9dEmDu+tMfEg7bF3NpuPb44EFOw1iAupPDQ1NZlBWnUwoCtPP1RTR/Yx3/+ZMf01YVhvSdJbG2ikoXIZQRmNtPIiSIMTFbpGmp2MRppk/3hntkZaVxjhHI00wB7OPcoBQIVqbm6Wr7qbak//mV78aIcCfp+iUemPz36kU69/Qf94Y70kadKw3jpwYOyEsLU9flEVv7atGnIQF1J5iDXSYqOd1SG/qoJ+/fn8ydqvd63qqgKacfBATYwHaW+v3yP5/JLPz2uMisNIoTNkCjQJ0lBOxRrtLKVQKGTuiG73eKcdjTSCNMAerJEGAA7T0tKiQG1P+QIhVQf9embe8Xpz/W795MF39OHWBvN+k4b31tb6lqTHnjJusBa8t0VVQw5UJLLN7qfuWrE10oxGWuxCdECPai34/mfU1BrWwJ416tetSrWhgPa2tOuPm0dq2Pfu1b5tT5fzaQO2o5HmDDTS4ERuGO2UYq205uZmTzXSCNIAexGkAYDDNDc3S/ETotqqgIb1rdOwvnU6+oB+emHVdr3+yS71qQvpiOF9tHp7Ilgb2a9OJx0yUAve26LqIQcpsntLub4E12lubpYso52GHjUh9aiJNdWCAb+OPqCfFq7cpvaoX/7qbtpb1a8szxcoFxppzkCQBidyw2inFFsnbfv27TTSABSMIA0AHKalpUU+fzy8sZyM9u9erc9N2l+fm7S/eduIft0U9PvUHolqysi+5jpeVYMOUGTX87Y+bzdLGu0MZr4A+P05E/Xuxj368V3Pa0NLtSKEBqgwNNKcgSANTuSmRprkrTXSCNIAezn7KAcAFailpUW++AlRVaDzdUZCAb9GD+gmSZo6so8OGNBdVf6o/FW1aq7qVfLn6hXNzc3yB2O7pFaHMv9fY6/akI4Z01+1gViYQI6GSkMjzRkyfR8INFFOblgjTUrs3OnFRhq7dgL2IEgDAIdpbm6W/LGTrGAg+2H68lkH65wpw3TGhP0U8Ps0tCZ2ErWvZkBJn6eXWBtpNcHsvyk2rhEiZAaoMDTSnIFGGpyIRlrn2LUT8A5GOwHAYWKNNCNIy/5b3ZnjBmnmuEHm272rotI+qT1QXbLn6DWxdenia6R10kgzGLuj0r5BpaGR5gwEaXAiN62RJnmzkUaQBtjD2Uc5AKhAzc3N8vmN0c78D9NGM4Tr2dzFGmkdNxvIxG+0b0r6rADnoZHmDARpcCK3jHaWu5FGkAa4H0EaADhMS0uLlEcjLZXZlirmk/K4XDcbMPgZ7USFopHmDJkCTYI0lBOjnZ0zQi42GwDcz9lHOQCoQLFdO+NBWgEno36zkcYFba6SG2nZX3On/7YdKBUaac5AoAknYrSzc3Y00thsALCHs49yAFCBmpubzTXSChvtjP2X66jcNTU1yReIN9JCuYx2xv5LIw2VhgDHGRjthBPRSOtcKRtpbDYA2MvZRzkAqEAtLS1SfI20wkY74420oj4rb8u3keZnHTpUKBppzkCQBidyyxppXm6kEaQB9iBIAwCHsTbSQgU00vzxuhRtqdzlH6TF/sulKioNjTRnIEiDE9FI6xxrpAHe4eyjHABUIOsaaaECGmnGI7iczV0svDQ2G8h9104yA1QaGmnOQJAGJ2KNtM7RSAO8w9lHOQCoQEmjnYVsNuA3RjudPVrhJEmNtFAumw3E/kuOhkqTaXSLRpq9CNLgRG4Z7aSRBqCrCNIAwGGs7ahQDmOGqWhL5a/QNdIYn0WlydQ4oZFmL4I0OJFbRju93Ehj107AHs4+ygFABWppaZEvfkIU8ndlswFSnlzFgrQ8RjsL+L4AXkAjzRkyBZoEaSgnt4x2erGRxq6dgL2cfZQDgArU3NwsxddIK2jXTj+NtHzFWoC5N9ICRvumpM8KcB4aac6QKdDk+4Byclsjrbm5WW1tbbZ9XtZIA7zD2Uc5AKhAsUZaF3btZP2uvCU10nJYIy0xPkszDZUl02YDNNLsxWgnnMhta6RJ9rbSjCCNNdIA9yNIAwCHie3aGR/tLChIY7OBfCWvkZbLaGfsv0QGqDSZAhyaUPYiSIMTuaWRFgqFVFNTI8neIM0IuWikAe7n7KMcAFSgpNHOLq2RhlzlO9rJhg6oVDTSnIEgDU5j/bfv9CBNSrTS7NxwwI5GGpsNAPZw/lEOACpM0mhnIbt2skZa3vLebICwEhWKRpozEKTBaaxBmtNHO6XEOmleaaSx2QBgL4I0AHCYlpYWs5HWlV07xWhnTtra2hQOhxOjnbmskeYnSENlopHmDNmCNL4PsJs1vKWRlh5rpAHe4fyjHABUmNiYYeyEKNilNdK4kMpFc3OzJMkXMBppjHYCmdBIcwYaaXAat412eq2RRpAG2KukR7nnn39ep59+uoYOHSqfz6eHHnoo62MWLVqkI444QtXV1RozZoxuv/32Dve54YYbNHLkSNXU1GjatGl67bXXiv/kAaBMurxrp5/NBvJhBml5bDYQMF5jF4yvAMVEI80ZCNLgNDTSsqORBnhHSY9yjY2NmjBhgm644Yac7r9mzRqddtppmjFjhpYvX67vf//7+trXvqYnn3zSvM/999+vefPm6corr9SyZcs0YcIEzZo1S9u2bSvVlwEAtrJuNhAKsNlAqTU3N0s+vxle5tVIK+kzA5yHRpozGIElQRqcwvoz54Y10owgjUYagEIU/1+xxZw5czRnzpyc73/jjTdq1KhR+t3vfidJOuSQQ/Tiiy/q2muv1axZsyRJv//973XJJZfo4osvNh/z2GOP6dZbb9V///d/F/+LAACbtbS0qNofH+0sYI00oy2lqPNPZJ2gpaXF3GhAkmpCOWw2YG7owGuMykIjzRmM0CLT94EgDXZzWyPNGO30WiONXTsBe5Q0SMvXkiVLNHPmzKTbZs2ape9///uSpNbWVi1dulTz58833+/3+zVz5kwtWbIk48dtaWmJLd4dZ+cBE0CFWvuytOhqqb0174c+8YWogn0flr/qWY14o5u0sjqvx39r+x59oSosja6Xbnkz78/vSnX9pM/+XuoxOPN9muulR74j7d2SdPOQffv0/IVVqq66SpJUc9f1WT/dd7ft1vlVEflG1ku3LOvKM3eu/mOk06+X/MU/4Yd70UhzBkY74TRuWyPNa400du0E7OWoIG3Lli0aNGhQ0m2DBg1SfX29mpqatGvXLoXD4bT3WblyZcaPe/XVV+sXv/hFSZ4zAKT1+s3SmucLeugxw/ySNsf+7FLsTx5GShrpl9RN0votnd/ZS8acJE39aub3r1ksvf9Qh5u7STp2f5+kD2M3rM/+qUZLGu3113j9K9LUr0lDJ5X7mcBBaKQ5A0EanMZto53l2GzAaIsx2gm4n6OCtFKZP3++5s2bZ75dX1+vYcOGlfEZAfC8cLyJNvkr0gEn5fyw1rY2nXvuuep1zLmqGjhaFx89UkeN7pfXp77nhfe1eG2ToltW6G8/vCCvx7rSkj9J61+VIlnGGYzvyYBDpBk/MW/+cNUq/fRX/6u+p3xLtaGArjtnYtZPeefid/XS+hZpy0r99Yfnd+HJO9Rj86TG7VKYEREko5HmDARpcBq3jXbavdlANBo1XyM2GwDcz1FB2uDBg7V169ak27Zu3aqePXuqtrZWgUBAgUAg7X0GD848zlNdXa3q6vxGowCgS4xWxpAJ0rgzcn7Yvt279e+V7Ro0cZxqIuP12WGTpHFD8/rUK9/qoycj9QrXt+X1uV3rvX/H/putCWO8v1v/pNdly47n9fDaOg2NHKl+wSpp3MlZP+XKpT31ZKRR0b3t3nyNn/55LEhjOwWkoJHmDASacBq3jXba3UizBlw00gD3c9RRbvr06Vq4cGHSbU8//bSmT58uSaqqqtLkyZOT7hOJRLRw4ULzPgDgCMYJpS+/w6y5nmP8hKiQXTsDlXZBa7zG0SwXjhm+J62trfIFqyTltmOnZNlsIPdn6S65vqaoOAQ4zkAjDU5DI61z1oCLzQYA9yvpUa6hoUHLly/X8uXLJUlr1qzR8uXLtW7dOkmxkcsLL7zQvP83v/lNffzxx7r88su1cuVK/fnPf9YDDzygH/zgB+Z95s2bp5tuukl33HGHVqxYoUsvvVSNjY3mLp4A4AhGAFFgkOaP7yIZLOBkNBHyOH+NkqLIOUhL/z1paWmRLxAP0nLYsVOyXiR49DUmSEMGNNKcgSANTuO2NdLq6uokSfv27bPl81kDLjYbANyvpKOdb7zxhmbMmGG+baxTdtFFF+n222/X5s2bzVBNkkaNGqXHHntMP/jBD/SHP/xB+++/v26++WbNmjXLvM8555yj7du364orrtCWLVs0ceJELViwoMMGBABQVgUGac3NzZIkf7whFcqxIWUVcMEJbFF1MUiLNdJiwWWujTTjNfZsZECQhgyMi+XUC2UaafYyAkuCNDiF24I0o8FlV/hvVyONIA2wR0mDtBNOOKHTg9Ptt9+e9jFvvvlmpx937ty5mjt3blefHgCUkHHsy+9k0mykBWKH55A//5NRo5EmF5zIFoX5dWY7GTZGO5Nfl5aWlrxHOwPmyK1XX+P410W7CCmyBTg00uyRKdDk+4ByyXRscCq7Q+dSN9KMIC0SiSgajboizATczB1HOgBwmwIbacaIgT8QH+0M5H+Y9sU/J6OdKTptpBlBWm6/JQ4Yr7FXT1RppCGDTKOdNNLsxWgnnCZTuOtUdv9bsauRJvHvH7ADQRoAlEKBQVpjY2PsYfFRw8I2G6i0NdKM9lRX1kiLj3aG8t1swKOvMUEaMsgW4NCEsgdBGpwm08+kUxnP065RSKOR5vf7SxI2WoM0xjuB0nPHkQ4A3MYMbfI7WWpoaIg9zB8f7SygkRYoYBzU3XIcQ4ymH7ftyq6dnpXzuCwqDY00ZyBIg9Mw2tk5I9wqRRst9eOycydQeu440gGA2xihTaGNtPgJUbCARprZDMn7kS5ltqeyBWmdNNLyHe30fCMtx5YfKg6NNGcgSIPTuLWRZvcaaaVYHy3149JIA0rPHUc6AHCbLjbS1IVGWqIt5dGQJ1Xea6Sla6TluWunSy4UCpZrOImKQyPNGQjS4DRuWyPNuji/HexspBGkAaXn8SsBACiTLjbSovHHhQoIbAIVt2tn1zYbiK2RFm+k5bhGmhmkefU1Zo00ZEAjzRkI0uA0NNI6V+pGGkEaYC93HOkAwHXSr8eVjdFIM4K0QkY7A+Zop0dDnlT5rufVaSMtz9HOPINS98hx3TlUHBppzkCQBqdhjbTOlbqRZn3dCdKA0nPHkQ4A3KaLu3ZGfbETrcI2G6iwQ3sRGmnKe7MB434eDStppCEDGmnOkCm0IEhDubhttNNrjTSfz2d+TWw2AJRehV1tAYBNCgzSGhoakh4TKmSzAR+jnWll+J60trYmRjtzXiPN468tQRoyoJHmDJlCC74PKBdGOztX6kaalAjpaKQBpeeOIx0AuE0XNhvwBULm28GubDbg81dIOyTHHSbN93cy2hnKdbQz3r7xaljJrp3IgEaaMzDaCadhtLNzpW6kSYmQjiANKD13HOkAwG26stmAPxHmFNJIsy6EXxEXtbnuMJnhe9LS0iJfnqOdAa/vjGq+RhXw84O80EhzBoI0OA2NtM7Z0UgjSAPs444jHQC4TRdGO32BxG8rC9u1M7F+V0VcTOXanupstNMI0vJspHk+SKORhhQ00pyB7wOchjXSOkcjDfAWgjQAKIUubDZgBGl+n2VMMw+BeIvN56uUIK3rmw0Y47Q5N9KMkVu/R8dnCdKQAY00Z6CRBqehkdY5GmmAt7jjSAcArmOEK/mvkWaMdhayY6dkHe30V8bFVM6//TZGO9OtkZbvaGeFjM96+WtDQWhCOQNBGpyGNdI6Z2cjjV07gdJzx5EOANymwM0GGhsb5fPHTrIKDdISmw14POQx5N1IS/6exNZIMxppuY52JjZ08OQFa67rzqHi0EhzBoI0OI1bRzvtam8Z4Ra7dgLeQJAGAKVQ4GYD1jXSggVsNCBJfl+iLVURF1NdHO1sbW2VL2CskZbfaKdnx2cZ7UQGNNKcIVP7hyAN5cJoZ+eMcIs10gBvcMeRDgDcpguNNHWxkWau3+XVtlSqnIO04u3aGTQvFDz6GhOkIQMaac6QLdCUCDVhL0Y7O8dmA4C3uONIBwBu06VGWnyNtAI2GpAS63d5ti3VgbFrZ5aLxmj6detia6TlN9ppjs/6vRqkGa8RF+JIRiPNGTKN0Vm/L548NsGx3NZIM0InNhsAUAh3HOkAwG0K2LWztbVV7e3tltHOLjbSVCFBWjF27SxwswHPhpU00pABjTRnyKWRxvcCdnLrGmlebKSx2QBQegRpAFAKBQRpDQ0Nsb+Yo50FNtJ8bDaQVqdrpMUaaTV5rpHm2fFZgjRkkClIo5FmL4I0OI3bGmnlWiONzQYAb3DHkQ4AXCf9GGFnjCAtVF0T+y9rpOUm5zFEY9w23a6dRiMt1107Exs6eDM4yHFcFhUn08UyjTR7EaTBaVgjrXOskQZ4izuOdADgNgU00hobGyVJtd26S+rKrp2xx3l27DCVEYx1pZGW72YDxm+Uvfoa00hDBjTSnCFboGm9D2AHRjs7xxppgLcQpAFAKXRhtLOmtpukLjTS/JXWSOviGmmt7fL5YyefuTbSjPXrfF59jXMNJ1FxaKQ5A400OA2jnZ2jkQZ4izuOdADgNmZok/43sx988IEOOeQQ3XbbbeZtRiOtpi4epBV4Mmp+Sq+2pVKZQVq2XTszNNIiicdV57hGmrlrp2eDNBppSI9GmjPkEqTxvYCd3DraGY1Gbfm3QiMN8BZ3HOkAwG3MJdLSB2k33XSTVq5cqT//+c/mbWYjLR6kFTraaQ3SKuNCKsf1vDK8v92SFVXl2AIM+BMbOngzSHPHaA7sRyPNGTKFFjTSUC5ubaRJ9oTO7NoJeIs7jnQA4DZZRjufeuopSdLbb7+ttrY2SYlGWlVNnaTCRzsTa6R5tC2VqgujndFoVG3xm4N+X6Jplu1T+mikoTLRSHMGRjvhNG5dI02y598Ku3YC3kKQBgCl0EmQtnnzZr3zzjuSYgvdv//++5ISjbTqmlpJUqiLmw1IFXIhlXOQZuzamfietLW1yReInXhW5bjRgFQBGzoQpCEDGmnOkCm0IEhDubi5kWbHvxXWSAO8xR1HOgBwGzOA6BiGPf3000lvL1u2TFKikRaKB2nBAk9GzVKVV9tSqcwLyWxNGCNIS3xPWlpaJH/spDaUYxtNkgI+j4925joui4pDI80Z2LUTTuPWNdIke4In1kgDvMUdRzoAcJ2O7SeDEaRVVVVJSgRpRiOtqjreSMujIWXl83zIk6ILo52tra3mjp2BPEZpfZaw0pPBAY00ZEAjzRk6a//YvRshIDHamQ2NNMBbCNIAoBQyjHZGIhEzSLvoooskSUuXLpVkaaRVV8f+m0dDyqryGmmFB2mxRlrsxDOfUVo/a6ShQtFIcwaCNDiN20Y7rc0wr6yRRpAG2McdRzoAcBsztEm+2HznnXe0detWdevWTd/+9rclScuXL1c4HDYbacGqmth/u7hGmq9Sdu3saiMtvkZaPqO0xl1ZIw2VhkaaM+QSpHExDTu5ebTTK40042OzaydQeu440gGA26RZ2F6Snn/+eUnScccdp8MOO0zdunVTU1OTPvjgA0uQFm+kFbhrZ2Ls0KMhTybZQkPz/clrpPniJ9P5BJfeb6Tluu4cKg2NNGegkQancVsjzYu7dtJIA+zjjiMdALhNhtHOPXv2SJKGDRumQCCgSZMmSYqtk2aMdgZDXQvSPB/ypOpiI83YbCCYxyit3+vr0BlfH400pKCR5gydtX+4mEY5sEZa51gjDfAWgjQAKIUMjbSWlhZJiY0GjjjiCEmxIM1opPlDsfflE+xYJc5hPRrypDKDtGyNtPRrpPnMNdLyGO30+jp0jHYiAxppztBZ+4eLaZSD2xppdu9wSyMN8JbSReIAUMkyBBCtra2SpOr4hgLWIM04qQsEQ5IK37XT822pVDmPIRrhZuKW1tZWyVgjrYDRTp9XgzTjRSITQQoaac7QWfuHi2mUg9vWSPP5fOZasjTSAOTLHUc6AHCd3BppEyZMkCS99dZbiUZaMPa+wnftZLOBtDI10nyxE89APpsNeD2spJGGDGikOQONNDiN20Y7JXvXE7SjkcZmA4B9CNIAoBQyrJFmNNKMIO2QQw5RMBjU7t279cEHH8QeYjakCm2kyfzcngx5UuUcpHUMN2ONtPhoZx7BpXVDB08GBwRpyCBT64RGmr0I0uA0bhvtlOwN0mikAd7iniMdALiJGaQlhzNGI80Y7ayurtYhhxwiSdq7d68kyR8P0grftdPjbalUXW2k+QsY7fR7fLSTIA0ZZGqd0EizF7t2wmncNtoplaeRRpAGeIN7jnQA4CYZNhtIbaRJifFOkxmkdXGzAa+GPB0Y63ll22zAeH/idW1tbTU3GwjmcfIf8HpYmfO6c6g0mUY7aaTZi0YanIbRzs4ZjTQ2GwC8gSANAEohy2in0UiT0gRpPiPY6foaaRVxUduFRlpra6tkBGl5bTaQ+NyefI1ppCGDTAEOjTR7EaTBaRjt7ByNNMBb3HOkAwA3yRCkpW42IKUJ0uIndoXv2mn8zaPrd6Xq6minsSZdHif/xm/cff4AQRoqCo00Z+hsjI6LaZSDm4M0O/6t0EgDvMU9RzoAcBMzgEi+2MylkRb1GYvfFxqkeXzsMFXOY4gZNhuIN9LyGaW1lgU9ecJqBmkVEMQiLzTSnIFGGpyGNdI6Z0cjjV07Afu450gHAK6Sfo20dI20gQMHavDgwZJiJ0Hh+EPzGTW0su4oWVFBWtY10jpuABHbbCB20RnIY5TWb/kYYS+/xjTSkIJGWvlZw0qCNDiFG9dIM/6tsEYagHwRpAFAsVkDnRzWSJMSrbTu3burLRw7oSt0187EGmkeXb8rVZfXSMt/l9TkIM2DDRxGO5EBjbTysx7X04UWXEyjHNw82mlnkMYaaYA3uOdIBwBuYQ0fUi5y0jXSpOQgrT1eSSt0187KG+3MN0hL30jLZ3MH63VCOOzB15jRTmRAI638rK8xjTQ4BaOdnTP+PdJIA7zBPUc6AHCLpEZa9jXSpESQ1q1bN7ORls/i91bWHSUrox2S62in8f7E96S1tTWx2UBea6R5fLQz53XnUGlopJUfQRqcyI2jnTTSABSKIA0Aii2pkZZ9jTRJmj17tiZOnKgLLrhAbUYjrcBdO1Vxa6Tl2kjruG5dS0uLWS/LJ7i0Bmntnm6kefBrQ5fQSCs/gjQ4EaOdnbOjkcZmA4B9SheJA0Cl6iRIMxppqUFa37599eabb0qSTrl2sSQplMeooRVrpGWQYY00nz//Rpo1Q/BmI40gDenRSCs/gjQ4EaOdnaORBniLe450AOAWSeFD+jXSUkc7rYxGWrCLmw1IFXIyZQZj2S7gMzXSYieeBW824OlGGqEIktFIKz+CNDgRjbTOsUYa4C3uOdIBgGtk37UztZFmldi1s9BGWuLvnmxLpTIu6AvYbCDWSIudeAbyaABa7+vN1zjH1xQVh0Za+VlfY4I0OAVrpHWORhrgLQRpAFBsOayR1lkjLbFrZ2GHaOtJbCRSARe1XRjtjDXSYie1+YzSJoWVnm6kefBrQ5fQSCu/bI00O8MBwEAjrXM00gBvcc+RDgDcooA10qzMXTuL0Ehrr4ggLd9GWsoaafETz3xGaa0hQtiLDRyCNGRAI638GO2EE7FGWudopAHe4p4jHQC4RVKQlhyGGUFa52ukGaOdxVgjrRKCECNIy3I38wI/8fq0tLSYmw3kM9oZ+3ix1zbsxbDSfCk8+LWhS2iklZ/1NU43RsfFNMrBzaOddvxbYddOwFsI0gCg2KLp10hrb283TzQ7a6QZLbJQgb/V9fyOkqm6uGtnYrOBfIO02PfJk2EljTRkkKl1QiPNPgRpcCJGOztHIw3wFvcc6QDALTIEaUYbTcqtkVb4aKfXF8JPkXeQltpIi4925n3yHw/SvPgas2snMsjUOqGRZh/r94AgDU7BaGfnWCMN8Bb3HOkAwC0yjHYaGw1ImRtp0WhUbV3ebCDx94rabCDXMcTUcDMQ32wgz+DSFyVIQ+WhkVZ+2Zo/XEyn98ILL+jss8/Whg0byv1UPMmNjTTj3wqNNAD5cs+RDgBco+NaXFKikebz+TKeSFnX28p71DDOn7RrpwdDnlR5bzaQeH1aW1vNRlqg4EaaF4ODHF9TVBwaaeVHkFaYG2+8Uf/617/0z3/+s9xPxZPcvEYajTQA+SJIA4BiS7MWl5RopFVVVWU80TTaaFJ+u0haMdqZQZrvS0tLi7lGWt6jtKyRhgpEI638so3QcTGdnvH/wXv27CnzM/EmNzbSvLZGGpsNAPZxz5EOANwiQ5BmNNI622igzXIyV3gjLfF3b7alUnRxswFfgZsN+CpijTQPfm3oEhpp5UcjrTDG67F3794yPxNvYo20ztFIA7zFPUc6AHCLNCOEUuK34Z1tNNBuaaQVvmtnhTXSzDHELKFhtOPIbWyzgdhvcPMf7YzxZFZp/gx58YtDVxgXy6lBGo00+xCkFcZ4Perr68v8TLyJ0c7OsUYa4C0EaQBQbMaFZCGNtPiYoN8n+f1dOBmNh3kVtdlAgY00Y7QzlPfrTSMNlSdTiEMjzT4EaYWhkVZajHZ2zvj5I0gDvME9RzoAcIssa6R11kgzgrRCd+xM5cmQJ1WuO0xmWCPNFzDWSMvvNTd27fRkcECQhgxopJVftuYPF9PpEaSVFqOdnTMaaYx2At7gniMdALiFGT6k37Wzs0aaMdrZ5SAt6uUdJVPkPIbYsSkYa6TFfjuc92YDXt6105fjuCwqDo208svW/DFu52I6GaOdpUUjrXM00gBvcc+RDgBcI/1oZz6NtPxDnWTGQvgVcVFrhj65jnamrpEWb6TlOdrp83KQphxfU1QcGmnlx2hnYWiklRZrpHXOjkYau3YC9ildJA4AlaqzNdJ8fkWHHKqbX/hYe5ra9P8dsb9G9e+WuI8RpHX5N7peDnlS5L1GWuIk37pGWqGvuSfHZ3Mdl0XFoZFWfrkGaXwvkhGklRaNtM7RSAO8hSANAIotw66dra2t6nboDNVPPF+/fmyFJOnj7Y264fwjzPts2dMsSerfPfP4Zy58ikVpFdEOyTlISw44w+GwwuGwuWtnKM8WYGKNNA++xqyRhgxopJUfjbTCEKSVlpvXSLPj3wprpAHeYsuR7oYbbtDIkSNVU1OjadOm6bXXXst43xNOOEE+n6/Dn9NOO828z1e+8pUO7589e7YdXwoAZJchSGtpaVGw10BJUl1V7GRn056mpPt8tK1BkjRmYPcuPgcP7yiZKufNBpKDNGPNOmOzgUCBo50RLwYHZpvSg18buoRGWvllCyy4mE6PNdJKi9HOztFIA7yl5EHa/fffr3nz5unKK6/UsmXLNGHCBM2aNUvbtm1Le/8HH3xQmzdvNv+8++67CgQC+sIXvpB0v9mzZyfd79577y31lwIAuelktNMXqpEkHTSohyRpZ2Nr0n2KFaR5e/2uVHmukRa/v7FmnTHame8GD8algidf41zXnUPFoZFWfjTSCmM0gpqbm1lDqgQY7cwsGo2an4NGGuANJT/S/f73v9cll1yiiy++WOPGjdONN96ouro63XrrrWnv37dvXw0ePNj88/TTT6uurq5DkFZdXZ10vz59+pT6SwGA3JiNtI6bDfiCsY0G9utTKylNkLY9FqQdMKCLjbS4imiH5L1GWkojzVgjrcBdOz35GjPaiQxopJUfQVphrK8H453F58bRTrvWE7T+7JWykcZmA4B9Snqka21t1dKlSzVz5szEJ/T7NXPmTC1ZsiSnj3HLLbfo3HPPVbdu3ZJuX7RokQYOHKixY8fq0ksv1aeffprxY7S0tKi+vj7pDwCUTErzydDa2ip/VSxI2z8epO1tbldre+z+0WhUq4vWSIvxZFsqVYFBmtFI8wViJ575bjbg6deYIA0Z0Egrv2wjdARp6VlfD64Fio9GWmbWYItGGuANJT3S7dixQ+FwWIMGDUq6fdCgQdqyZUvWx7/22mt699139bWvfS3p9tmzZ+vOO+/UwoUL9Zvf/EaLFy/WnDlzMh40rr76avXq1cv8M2zYsMK/KADIKv1op7WRNqhHjbkm1659sWbU9oYW1Te3y+9T0k6eXXkOnly/K5V5MZntazW+L7H7JxppRpDGGmkJxminF782dAWNtPKjkVYYGmmlxRppmdnVSOPfPmAfR+/aecstt+jwww/XkUcemXT7ueeea/798MMP1/jx43XAAQdo0aJFOumkkzp8nPnz52vevHnm2/X19YRpAEonw2indY20btUB9akLaUdDqz5taNWgnjXm+mjD+tapJtS131iaIY8X21Kpct5sIP1opwoe7YyhkYZKQiOt/AjSCkOQVlo00jKjkQZ4T0mPdP3791cgENDWrVuTbt+6dasGDx7c6WMbGxt133336atf/WrWzzN69Gj1799fH330Udr3V1dXq2fPnkl/AKBkOlkjzR+KNdJqQgH17VYlKbFOmjHWWaz10SSvtqVS5LowfobRzsI3G/BwWMlmA8gg0zpINNLsQ5BWGEY7S8uNa6SVI0ijkQZ4Q0mPdFVVVZo8ebIWLlxo3haJRLRw4UJNnz6908f+4x//UEtLiy644IKsn2fDhg369NNPNWTIkC4/ZwDoMjOw6bhGmtFIq6sKJoK0+Gjn6u2Nkrq+Pppk3bWzAi5qc26kJY/c7tu3L/5m7MQzkPdoZ0zEi2GTzz0XQrBXpvEtGmn2IUgrDI200mK0MzPrz14pg0b+7QP2KfmZ8rx583TTTTfpjjvu0IoVK3TppZeqsbFRF198sSTpwgsv1Pz58zs87pZbbtFZZ52lfv36Jd3e0NCgH/3oR3rllVf0ySefaOHChTrzzDM1ZswYzZo1q9RfDgBkZ1xHppxMtrS0yBdvpNWGAurXLfb3nQ2xZpQx2jmmCI00M+TxYluqg1wbaeY3RlKs+Sx/4jfDoXw3G/AZa6Tl9TB3oJGGDGiklV+25g8X0+kRpJUWo52ZGY20QCBQ0qCRXTsB+5R8jbRzzjlH27dv1xVXXKEtW7Zo4sSJWrBggbkBwbp16zoccD/44AO9+OKLeuqppzp8vEAgoLffflt33HGHdu/eraFDh+qUU07Rr371K1VXV5f6ywGA7DpbIy2+2UBtVcfRTiNIO6AIjbTK2mwg3107YyexDQ0NZhtNyn+NNHbtRCXK1Dox3qaRVnrZAgvjdoK0ZIx2lhajnZkZP3ulXB/N+vH5tw+Uni2bDcydO1dz585N+75FixZ1uG3s2LEZT8Rqa2v15JNPFvPpAUBxmeFDx9FOf3y0szYUUJ94kPZpY6saWtq1pb5ZEo20vOUdpMXuH2ukdT1I82RYSZCGDDJdLNt1QQpGOwtFI620aKRlZjTESrk+msS/fcBO7jnSAYBbdLLZgDHaWVcVUD9LI83YaKB/92r1qgt1+SmYbSkvhjypzGZMtq/VWCPN0kgLJE5qg/mOdnp5swFzXNaLXxu6gkZa+eUapBFqJiNIKy3WSMuMRhrgPQRpAFB0yYvaG1paWxNrpKWMdprrow3sVtRn4s2QJ0WXGmmxv/t8BWw2EL87jTRUEiMoy7TZAOFN6WULLLiYTo/RztJycyOt1P9WaKQB3uOeIx0AuEWGRlpzS5u5JldtSiPto+3x9dGKMNYpeb0tlaLAIC22RlrspDbfjQYkj4/PEqQhg0wXyzTS7MNoZ2FopJUWa6RlZlcjjc0GAPu450gHAG6Rsqi9obk9cYFZGwqob/eOo51jirLRQCLkiWYdd/QAa2DZ2UV8NLkp2NjYaAab+bbRJI+Pz5qvqQe/NnQJjbTyI0grjDVcIEgrPkY7M7O7kSZxLAZKjSANAIotmn60s7k9fvGjqEIBv/rWxYK0XftatarYQVr8PNaTO0p2YDlp7zRIS94EoqGhQYqvkZbvRgPWz+rNRpqxRhon4khGI638CNIKQyOttNw42mnXeoJ2r5Fm/ZwASsM9RzoAcIsMo52t4dgFZlX8PMfYtTMSldbsaJRUzEZaJY12WoO0Tk6G06yRZjTSQoECRjuNrCnvR7oAQRoyoJFWftlG6AjS0mONtNJitDOzcjTS+PcPlJZ7jnQA4BYpzSdDS3y0syp+5A0F/OpZkzip6lYV0OCeNUV5CmZbqhLaIUmjnZ0FaclNwYaGBqkLo51+L4eV5hppHvza0CU00sqPRlphaKSVlhsbaXavkUaQBniHe450AOAWZmCTHM60xs/Tqi1jhP26V5t/P2Bg96KtLVJZQZr1Nevs603+vsQaacZmAwWMdpq7dub9UBcwGmme/OLQBTTSyo8grTAEaaXFGmmZGY00RjsB7yBIA4CiS79GWmskdnJZHUycZPaNj3dK0pgi7dgpWUIeb6Y8yXJupCVvAmFtpAULGe2Mh02eDCvZtRMZ0EgrP4K0/KUGJYx2Fh+NtMzsaqRZPz47dwKl5Z4jHQC4RYY10tqisQvNmmDi9j51iSDtgCKtj5b0VCrhmjbvIK3jGmkFbTZgNtI8+CITpCENa0hGI618CNLyl/paNDY28rNaZKyRlhmNNMB73HOkAwC3SGk+GdqjsUNubShx6O1naaQdUMxGWvy/YS+GPKmSgrTOdu3suEaaz9i1s5DRTnONtLwf6nzma1oBPz/ImTVIo5FWPtlG6AjSOkr3WjQ0NJThmXgXo52Z2dVI8/l85uvPv3+gtAjSAKDYoulHO9vjh9zaqsRvDPt2t4x2FrGR5vdyW6qDPHftVGKNNHO0s4DfoPt9Xh7tZNdOdEQjzRmyNdKM27mQTkj3WjDeWVyMdmZmVyPN+jn49w+UlnuOdADgFhlGO8OKndzUhhInUkYjLej3aUS/uqI9BXOzAdZI6/g+ayMtHqSFChntjP/Xm0Eao53oiEaaMzDamT/ra1FbWyuJDQeKjdHOzOxqpEn8+wfs4p4jHQC4RUrzyRD2xYO0qsSJlLHZwMj+3RQqYMH7TDy9fleqAoK0aDQab6TFvheBAkY7jYd48hUmSEMa1otNGmnlk2uQxvciwRoq9OnTRxJBWrHRSMvMzkaaEdax2QBQWu450gGAW2RqpMWDtO41IfO26Qf00wEDuuncqcNK81Q8mfKk8OX6f2WJkdvm5mZFIhH5Al3YtdMMK/N+qPOZQZoXvzgUikaaM2Rr/tBI6cj6WvTu3VsSo53F5uY10kr9b4VGGuA9pf/XDAAVp+MaadFoVFF/LECrq04ceof0qtXCy04o+jMwPnNlNNLyXCPN54u10STJV/hop7fXoTPqdl782lAoGmnOwGhn/qyvRc+ePSXRSCs2GmmZsUYa4D3uOdIBgFuk2bWzra1NvlC1JKl7TVW6RxWVuVZ8JQQhuQRp1tfB5zd3a6uqqZFU2GYDPhmbDeT9UOdjswGkQSPNGQjS8mcNMgjSSsONa6TZNQZNIw3wHvcc6QDALcxdOxMBT0tLi3yhWGhjZ5DmzbZUGtlGEVOCNKORVlPbTVJss4d8+b0cVpptSg9+bSgYjTRnIEjLn/FaBAIB9ejRQxKjncXm5tFOGmkA8kWQBgDFFu042tna2ip/MNZI61Fb+iDNHO2smGvaLA2qlNuNRlp1bWyn1GAhu3Z6eo00GmnoiEaaM2QLLLiQ7ihdkEYjrbgY7cyMRhrgPe450gGAW6TZbKClpUW+qlgjra46lO5RReXz8o6S6WTbZdJ6u6WRVlVTK6mwzQY8vUYau3YiDRppzkAjLX/WIKO2Nnbcb2pqKudT8hw3jnZ6sZHGrp2APdxzpAMAtzDDh8SFZmtrq3zxRlpdVelPpIzP7MmQJ508g7TEGmnxIK2A0U6zgSP3jLHkjCANaVjbZqlBGo00+xCk5c/aSOP1KQ0aaZlZf/5KjZ9vwB7uOdIBgFtkaqTFNxuosSNI8xkL4VfIRW3WNb3Sr5FWVW0EaYU30jz5Emdbcw4VyXqxmXqxTCPNPgRp+bMGGUZjh9enuFgjLTOjHcZoJ+AdBGkAUHQZ1kiLB2m1ITuCtNh/Pbl+VzrZ1vRKaqT5zEZaqDr2PQkVsEaa39NhJWukoSMaac6QbYSOC+mOaKSVHo20zGikAd7jniMdALhFlkaaHaOdZj+rUi5qC1wjLVQdW7eukM0GKqORRpCGBBppzpAtsDBu50I6gSCt9FgjLTM2GwC8xz1HOgBwCzNIS1kjLRQLbexspHky5Ekn2yhiNP1oZygebhYy2unp1l/WUVlUIhppzsBoZ/4I0kqP0c7MyrHZAD/fQGkRpAFAsaVppFk3G6i1ZY202H89GfKklSU5TGpWJUY7g1VGkFb4aGfUi2FTxSWxyEVnF8o00uxDkJY/grTSY7Qzs3KMdrJrJ1Ba7jnSAYBbGOGD5WKzqbnZ1jXS/JXWDslrjbREIy0QqpIkBQMFNNLi/41E3fPb95xlez1RkYzjSbogjUaafXIN0gg1EwjSSo/RzswY7QS8xz1HOgBwizSNtMbmNvPvdVWlP5EyPnPFNNKyrpGWPNppNtKMIK2QRprfw8EBa6Qhjc4CHOM2T/57cJhsI3RcSHeUbtdOGjvF5eZGWqn/rdg52sm/f8Ae7jnSAYBbmBeSiYucvU0t5t+rg6U/9BoXWBEvjh2mk3VNL2uQ5jMbaf6g0UjLP0gLmKOdXmykZVlzDhUpl0aa9X4oDUY780cjrfRYIy0zdu0EvIcgDQCKzhjttDTSWmKNNF+4zWwylVJiiSv3nNB2SV6jnYk10vyhkCQpVMBop7lrZ96PdANGO9FRLo006/1QGgRp+bM2gnh9SsONjTS7xqAZ7QS8xz1HOgBwi05GO/1Re0ZJzJCnUpohWUc7k78n5hppgdhJbSGjnYk1ofJ+qPMx2ok0aKQ5A0Fa/miklR5rpGVWjl07GV0GSss9RzoAcAsztElcWO5rjZ3Q2BWkmSGPLZ/NAbKNIkaTW4JGI80XiDXSAl1YI82TUVPWUVlUos4ulGmk2SdbYGFt2RBqxhCklR6jnZkx2gl4D0EaABRbmkbavpZYgBaM2nNiY+RCkYq5iMp1tDN2P6OR5oufcBY22unlRhqjneioswtlGmn2ybWRZr1vpSNIKz03jnayayeAQrnnSAcAbhHtuEZaU1vsJC1gU3/J0yFPOnmOdpqNNH98tLOAzQY8vUYao51Ig0aaM+QTpHExHUOQVnqMdmbGrp2A97jnSAcAbpGmkdYUH+0M+mwO0mz5bA6QdbQz/Rpp8sdOOAtZI83TYaXlZ9ebXyAKQSPNGQjS8mdtBLGGVGnQSMuM0U7Ae9xzpAMAt0gZI5Sk5vbYhWXIpiAtsWunLZ+u/MyL+ExfcPo10qLxt4MFnPgb66p58iUmSEMaNNKcgSAtfzTSSo810jKzc7TT+Bz8fAOlRZAGAEVnhDbWIC12khay6ahrnMhWzOVstjW9LBtAtLa2qq2tLf52PEgraLTTCNLcc9FQEMY7EUcjzRmyBRbWgI2L6RiCtNKjkZZZOUY7aVwCpeWeIx0AuEW60c74ZgPVBQQ2hfBXXCMt2xppiUaaOdapRCOtS5sN5P1IF0hqpBGkIYZGmjPQSMsfQVrpsUZaZox2At7jniMdALhFtGMjzdhsoNamSpqn1+9KJ+saaYnviRGkhUIhheM3BwpZI61SRju9+RWiADTSnIEgLX8EaaXHaGdmBGmA9xCkAUCxpWmkNbfHTmhqQ6U/iZI83pZKJ49dO4310bp166b2eJIW6spopxdfZBppSINGmjNka/7wveiIIK30GO3MzBiztGONNH6+AXu450gHAG4RTV7YXpJa4psN1NWEbHkK5pJhtnw2J8hxjTQlGmndu3dXWzh2e9c2G3DPb99zZm0UEKQhjkaaM+QSWHAxnYwgrfTcPNpZ6p8FGmmA97jnSAcAbpGmkdYSv6lbtT1BmqfbUukU2EgLR2IvULCQ0U4vh5U00pCGcaGcLkijBWUfgrT8WYMMoxXEYuzFRSMtMzuDNH6+AXu450gHAG5haT8Z4pt2qntNlS1PwVi/q2IuZ83gJ1OslWgJtra2SpJqamrUbgRphWw2UClrpFVMGotsOrtQppFmH4K0/Fl3TeS1KQ03rpFm/Cww2gkgXwRpAFB0HUc726Kxv3evrbblGVReIy3H0U6fT21tbZJiJ7TmaGdBa6QZ31/3XDTkjtFOdNRZI816O0FaaRGk5Y/RztKjkZYZo52A97jnSAcAbpFmtDMcP9z27FZjy1Pwu+g3wkWRc5DmN38zHAqFzNHOEGukJWO0E2lku1A2gjRGO0uLIC1/BGml5+Y10gjSAOTLPUc6AHALS/vJEFbsxKZnnT1BWsVtNmCukZbhK7YEacmNtNj9AwWskRZgtBMVJlsjzbgopZFWWrmM0HExnYwgrfTcONrJrp0ACkWQBgDFliZIi/hjJ0+9etTZ8hQCFTfamS1IS4zbWk9o2+Mnz6FCRjs93Uizfk2V8kOEbGikOQONtPwRpJUeo52ZlWOzAX6+gdJyz5EOANwimrxGWmtrq3zB2CYDvbvbE6R5eiH8tLKNdkbN+1lHO9vDXd9swJN8rJGGjmikOUMuI3SERckI0kqP0c7MyjHaya6dQGm550gHAG6REqQ1NjYmgrQe3Wx5CpW32YDRSMtvswGjkRYsZLQz/jk92UiTsr+mqDjZLpRppNmDRlr+jNchGAyajR2ChuKikZaZnaOd/HwD9nDPkQ4A3MIMHmIXlbEgLbZbp11rpJmNNBetVdIl5ppemZLDDKOdZiON0c4Oso3LouJkWwOJRpo9cgksjPcRpMXQSCs91kjLjNFOwHsI0gCg2FJ27WxoaJAvFAvSakOlP4mSJKNgVTHXsznv2pkY7QyGQmqP79oZLOA36N7fGTXLa4qKQyPNGWik5Y8grfRopGXGaCfgPe450gGAaySPdlqDtJoqew67PnPssELkPNpp3bUzZL67kM0GjBabZ1t/jHYiBY00ZyBIyx9BWumxRlpmjHYC3uOeIx0AuEXKrp276xvMYKvGpkZawOtjh6nyCNKMk8tAPNyUEq9XPhIXCx59jQnSkIJGmjMQpOWPIK303DzaWeqfhXKMdhKkAaVFkAYAxZYy2rl7b6P5LvtGOyts185s63lZNoAwGmmBkLWRlv//HQa8/hpnXXcOlYZGmjPkElgYF+yEmjEEaaXHaGdmBGmA97jnSAcAbpGya+euvftib0fCBQU2hfAX0LDyhGyNNCXWSPPHd1KVCty107hYcNFv3/OSbd05VBwaac5AIy1/1iCDxdhLg9HOzBjtBLzHPUc6AHCLlNHO+sYmSZI/at9JO420FGlGO4OWRlpBo50Bj4/PsmsnUtBIcwaCtPyZI/2WRhpBQ3G5sZFmV3OTRhrgPe450gGAW1jaT1IiSAvYGKQlgiGPhjypso4hdhzt9Mc3Gwj6fQWt6RLw/Bpplbb1K7KhkeYMuTR/CNKSMdpZem5eI40gDUC+CNIAoNhSRjvr9zVLkgI++y4u/f5Ka6RlGUM0G2mJk0t/IB6kFbBjp2RdI809Fw35YbQTyWikOQONtPwRpJUeo52ZMdoJeI97jnQA4BrJQVpDU6skKWhjkOb9kCdF1l07E9+TxK6dsTXSggWe9Ht/jTR27UQyGmnOQJCWP4K00rKG5wRpHZWjkcbPN1Ba7jnSAYBbpKyR1tDcIkmqsvGI6zOaIfZ9yvLKGqQl1kgzRzsDsZPNQhtp/oDHx2cJ0pCCRpozEKTljyCttKxBFKOdHTHaCXgPQRoAFJsltJGkxubYyYydQVpi7Xz3nNB2SdbNBjo20nzGaCeNtPSyrjuHSmMEZJkulGmk2YMgLX/pgjSJn9Visb6ONNI6YrQT8B73HOkAwC1S1kjb1xprQFUH7TvkBiqtkZZtPS/LBhDmGmnxzQYK/b4kXmOvBmmskYZk2QIcGmn2yCVIM95HkBZjDdKsYQavT3Ew2tk5GmmA97jnSAcAbpHSSGtqjZ1A1YTsO+T6K3XXzjxGOyPxRlptVWEntsFAhTTSCNIQRyPNGXLZHZFGWjLjdQgGg0lhBmFDcdBI6xxBGuA97jnSAYBbWNpPktTcFjuBqg2V/gTKUHGNNPOCMtNX3HG0M+qPbTZQV2CQFvB6WJltXBYVh0aaMzDamb9Mo528PsXBGmmdY7QT8B6CNAAotpRGWks4dlFZV136EyiD50OeVIU00vyx70ehAaffuFhw0UVDXmikIQWNNGfItnuqRJCWiiCttGikdY5GGuA97jnSAYBrGO2n2EVlS3vs7dqqkG3PwAx5KiZIM9bzyrTZQGInVePk0gjSCm6keX20U1leU1QcGmnOQCMtfwRppeX2NdJK+XMQjUbNf7MEaYB32HKku+GGGzRy5EjV1NRo2rRpeu211zLe9/bbb5fP50v6U1NTk3SfaDSqK664QkOGDFFtba1mzpypVatWlfrLAIDcWEIbSWqLv9m9tsq2p2CskRb1asaTKo9GmnFyGfbFgs1Cm4Lmrp1e/Z0Umw0gBY00Z8gnSON7EUOQVlqMdmZm/RljtBPwjpKf/d9///2aN2+errzySi1btkwTJkzQrFmztG3btoyP6dmzpzZv3mz+Wbt2bdL7f/vb3+r666/XjTfeqFdffVXdunXTrFmz1NzcXOovBwCyS9m1sy0a+2/3GjuDNOPw7p4T2i7Jtp5XNNESNEY7w77YxVRdgaOdZpDmoouGvBivaeWstIcsso0U0kizB420/FmDNOvrxutTHIx2Zmb9GaORBnhHyY90v//973XJJZfo4osv1rhx43TjjTeqrq5Ot956a8bH+Hw+DR482PwzaNAg833RaFTXXXedfvazn+nMM8/U+PHjdeedd2rTpk166KGHSv3lAEB2KUFae7wW1rNbTaZHFF1iswGPhjwdZGlPmRf2idHOsOJBGrt2pkcjDSmy7RZJI80eBGn5M477xutihA28PsXh1tFOO5qbBGmAN5X0SNfa2qqlS5dq5syZiU/o92vmzJlasmRJxsc1NDRoxIgRGjZsmM4880y999575vvWrFmjLVu2JH3MXr16adq0aRk/ZktLi+rr65P+AEDJWMYIw+GwuRZXjzo7gzSPhjuZFDTaGTuhra0qcLQz4PHWH5sNIAWNNGcgSMtf6mLvxn8JG4rD7aOd0Wi0ZMct688Yo52Ad5Q0SNuxY4fC4XBSo0ySBg0apC1btqR9zNixY3Xrrbfq4Ycf1t13361IJKKjjz5aGzZskCTzcfl8zKuvvlq9evUy/wwbNqyrXxoAZGYGDz7t27dP/mC1JKl39zrbnoLnxw5TZR1DTLQEjdFOY+S24M0GvP4aZxuXRcWhkeYMBGn5yxSk8foUh9uDNKl0vwCgkQZ4k+O6t9OnT9eFF16oiRMn6vjjj9eDDz6oAQMG6K9//WvBH3P+/Pnas2eP+Wf9+vVFfMYAkMLSfmpsbJQvFAvS7GykmZsNeLUtlaqARlpXg7TEaKfj/q+0OGikIQWNNGfIFmhKBEWpCNJKy/oz6dYgrVS/ACBIA7yppGf//fv3VyAQ0NatW5Nu37p1qwYPHpzTxwiFQpo0aZI++ugjSTIfl8/HrK6uVs+ePZP+AEDpJNpPDQ0N8gVjmwzUFriofSESo53uOaHtkmzreVl2UjUaae0ygrTCRi38lkaaN4MD1khDMhppzkAjLX8EaaWVLWR3KjuDNJ/PZ8vrQ5AG2KOk/5qrqqo0efJkLVy40LwtEolo4cKFmj59ek4fIxwO65133tGQIUMkSaNGjdLgwYOTPmZ9fb1effXVnD8mAJSUJbSxNtJqC2w+FcLzY4epcg7SEo201kjsMcVopHkyOKCRhhQ00pwhl9CCoCgZQVpp5dKSdCI7grTUjS5KjSANsEfJVzycN2+eLrroIk2ZMkVHHnmkrrvuOjU2Nuriiy+WJF144YXab7/9dPXVV0uSfvnLX+qoo47SmDFjtHv3bv3f//2f1q5dq6997WuSYgfo73//+/r1r3+tAw88UKNGjdLPf/5zDR06VGeddVapvxwAyM4S2jTsbZAvvkZajY2NtNS2lNtObvOWbT0vy06qidFOn6RowQGnEVb6vNpIy7ruHCoNjTRnyKWRZryPoCiGIK20cvmZdCI7G2kEaYC3lDxIO+ecc7R9+3ZdccUV2rJliyZOnKgFCxaYmwWsW7cu6SC2a9cuXXLJJdqyZYv69OmjyZMn6+WXX9a4cePM+1x++eVqbGzU17/+de3evVvHHnusFixYoJoa+9YfAoCMUtdIi4921oTsO8EMVFqQZo4hZgrSEhtAGKOdrfHrJxppGRg/MjTSEEcjzRkY7cxfaphhhA28PsXBaGdmBGmAN5V+D15Jc+fO1dy5c9O+b9GiRUlvX3vttbr22ms7/Xg+n0+//OUv9ctf/rJYTxEAisdsP/nU0NAgvzHaaWsjLbFGWiQScd3Jbd5y3mzAZ55ctsRvKnjXzkAirPRmkMaunUhGI80ZCNLyl6mRRthQHIx2Zmb8jBkBV6lZQ+LK+EUqUB4ev7ICgDIwg4dyrpEWO3HyebUtlSrrGGLH0c5EI62wk9ugP2B+TE++xgRpSEEjzRkI0vJnvA5GyMDrU1yMdmZWrkaa9XMDKD53He0AwA0so531exvkC4Qk2b1rp8fbUqmyNtISQZox2tkSLyJ0tZHm2bCSzQaQgkaaMxCk5Y810krLC0FaqX4Wyhmk0bgESsddRzsAcIVEaLOnocm81c7NBjw/dpgq5107LaOd4dj3qdCmYNA4Kfbsa5zlNUXFoZHmDPkEad48NuWPIK20WCMts3KNdlo/N4Dic9fRDgDcwNpI25cI0qqD5dhswF8ZF7U5r5EWG+30BavNIdBCRzuN8VnPBmk00pCCRpoz5LIeFUFRMoK00nLrGmnW5+vF0U6CNKB0CNIAoNgs7af6xmZJUiAatvUE0+8z1kjzaMiTKtt6Ximjnca6dVLhI7fGrp0+f8Cbr3HWdedQaYxQPtOxjEaaPXJp/xAUJTMCBYK00nDraKdU+vam3UGa9fMQpAGl476jHQA4naX9tLepRZIU9NkbtCRGOz26flcHOY52Kjba6a+qlSTVhPyJZlmerCerYS++xtnGZVFxsl0s00izB2uk5S81zLDubIiuc+top5R4zl4Z7fT7/ebXRJAGlI77jnYA4HSW9lNTa+wkJuSzt6FReZsN5LpGmtFIq5FU+FinpKQArj3swdeY0U6koJHmDARp+cs02knQUBxuHe2USh+k2d1IkxKhHT/fQOkQpAFAsVnaT63xa5ig3+YgzWikqVKCtGxjiIlws7293Rzt7MpOqnbs9lVW2cZlUXGytU5opNmDIC1/rJFWWm4e7SRIA1AI9x3tAMDpLO2n1vjOkCGbf0lrbaRVRDsk6xppse9JNN7Q85uNtMJPbL3fSGO0E8mytU5opNmDIC1/BGmlRZCWmd2jndbPRZAGlI77jnYA4HhG+8mn1kjsgrMLeU1BjCCtcjYbyG20M2KMphUhSLMurRb2YpBmrjtHKIIYGmnOQJCWP4K00mKNtMxopAHe5L6jHQA4nWWNtNb4eVmVzUdbM+SplM0Gsq3nZTTSjG9NVSxIq+1SkGZtpHnwYow10pDCuCDMdLFMI80euQRpxvsIimII0kqLNdIyI0gDvIkgDQCKzRzt9Kk9/tfqgL0nl+bJbMU00rKNdsb+E4nGXhdjtLNbFzYbsF4veHO0M9u6c6g0bW1tkqRQKJT2/TTS7JFLaEFQlIxdO0uL0c7MGO0EvMl9RzsAcDrrDpHR2GG2Kmh3kGb8hUaa9fbU0c5iNdLCXnyNaaQhhRGkVVVVpX0/jTR7MNqZPxpppcVoZ2Y00gBvct/RDgCczjLa2R5vQFXbHKQZIY+vUjYbUK5rpMXvHd+1s0ubDXg+SGOzASRrbW2VRCOt3HIJLQiKkmUK0ggaioPRzswI0gBvIkgDgGIzgwef2uOH2ZqgvYfbxEL4FTbamXEMMXa7EaQFa7pJkuqKNNrpyc0Gso3LouLQSHMGGmn5iUaj5s+kETDw+hSXF0Y7S/WzwGgn4E3uO9oBgNNZRjvD8cNste1BWqWukZbbaGegqlZS10Y7Y22/2Mf1ZiON0U4ko5HmDPkEaXwvkgMSRjtLwwtBGo00APlw39EOABwvMdoZVuzEqSuBTSEqb420+H+zBWnxSlqgOhakdevq9yUezHlys4Fs47KoODTSnIFGWn4I0krPzWuklTp0JkgDvMl9RzsAcDpLIy1iNtLsDdJopKUwmmPx63u/2Ujr4qhF/OLBm6OdBGlIRiPNGQjS8mMNEwjSSoM10jJjtBPwJoI0ACg2M0jzyTgtqypTkFYxmw2YQVqG90eNNdLivzWviu3a2ZXNBqwft92LwUHWdedQaYwgjUZaeRGk5SddI80IGnh9ioPRzszK0UhjMw2g9Nx3tAMApzN37fSZDaiQzUFa5Y125rpGWvzuoVgjretBWrzp5slGGpsNIJkx2kkjrbwI0vLDaGfpuXm004tBGo00oPTcd7QDAKeLJtZIi8bXmaoO2Vfplyy7dlbKaGe29bxS1kjzh6olSbWhrp7Yxkc7vfgaM9qJFDTSnCGXMTqCooTOgjSChuJgtDMzRjsBbyJIA4BiM4MHnyLxgCdk4wmUZDmZrbRGWpYxxLBRSYsHaXVFWiPNCOg8hUYaUtBIc4Zc2j8EaQnW18B4zXh9iovRzsxopAHe5L6jHQA4nWWzAaORVtXl5lN+zDXSVCEXtfmOdgbjQVo1jbSMsr2mqDg00pwhl9DCeB9BUeI18Pv9ZthLkFZcBGmZEaQB3uS+ox0AOF3Srp2xk/aaqvQNjlLxW9ZIq4iL2mxjiMZaZsaJshGkdXGNNF/847Z7cY20bOOyqDhGIy1TkEYjzR6skZafdEEGr09xsUZaZox2At7kvqMdADhexzXSytVIq5g10nJtpMUradFALAio6+radVEaaagcRiMt02gnjbTSs762BGm5IUgrPdZIy4xGGuBNBGkAUGyW9pPPHztxqspw4VkqvkrbbCDbel5m4BWV5JOCsSCttqu7dlbCaGeWdedQOWiklZ/1tSVIy026IMMIGnh9ioPRzswI0gBvct/RDgCczjruFw/S7N+1M75GWsVsNpDjaGc0Kl8oEQJ0dbTTDOjCHgybaKQhBY208iNIy19njTSChuJgtDMzRjsBb3Lf0Q4AnC5+QtlmCdLsXiOt4hpp2dbzsox2+kI15s21XR659XIjLf5fgjTE0UgrP4K0/DHaWXpeGO0s1c8CjTTAmwjSAKDY4kFae3u7fL54I832zQaMNdIqZbOBbKOdxrhtVP54kFYbCsjv79pJvy9pZNRjaKQhBY208ss3SCPUTAQZ1kYQQVpxMdqZGUEa4E3uO9oBgNPFg4e29rAUP0ELBe3dbMD6S+GKuJDKup5XIvAyGmldHuu0fFxPvsZmkFbepwHnoJFWftaQsrP2D0FRAo200nNzkFbq0JnRTsCb3He0AwCnswRpvkDsZCYUsPdwa66R5g9UxkVtrmukRSLyVcUbacUI0rzcSMs2LouKQyOt/BjtzB9BWumxRlpmNNIAb3Lf0Q4AHC8+2hlO7NoZ6OIIYb78lqZCRVwoZBtDtGwK4A9WSyrG+miSz9NrpDHaiWRGkEYjrXwI0vJnhAkEaaXjhTXSCNIA5IMgDQCKLR48tLa1SfE10kIBu4O0xN89GfKkyrpGmqU5Fm8JVgWL8X+BFRCkMduJOGO0k0Za+RCk5S9dkGEEDbw+xeHm0U67du0kSAO8xX1HOwBwuniQ1t4eli9+ghaw+eTS+lvh9rAHQ55UWRtpic0GfP7YCWawCOO2vnheEPHiaCeNNKSgkVZ+BGn562y0k6ChOBjtzCzdZhelRpAGlJ77jnYA4HTxE8q29rDZfgraPNqZtNlARbRDcl0jLSzFL6CK8z2Jj/F6MTjItu4cKg6NtPIjSMsfa6SVHqOdmTHaCXgTQRoAFJt1swGfE9ZIq4AgpJBGWhGDNBppqAQ00srP+tp2FloY4QBBEUGaHRjtzIzRTsCb3He0AwCnM9dIa5fiJ2iskVZi5gVlpkCr4wYQxdhJ1dhswJOtv2zrzqGihMNhs2lGI618cg0sCIoSCNJKjyAtM0Y7AW9y39EOABzPGO1st+zaae/hNqmRVhFBWrbNBhKNNMW/J8EihJvGGmlhTzbSGO1EgtFGk2iklVOuI3QERQkEaaXHGmmZMdoJeJP7jnYA4GSWIKetPZwIbcq4Rlo47MGQJ1W20McI0sIR+cx164q3a6c3gwOCNCQY66NJmYM0Gmmll2tgUYygKBwO67777tPGjRsL/hhOwK6dpccaaZkx2gl4E0EaABSTJXRobWsz1+Mq5xppnhw7TJW1kZYY7ZQ52lmERlo8SPNmI41TBCRYG2mZRjtppJWenaOdDz/8sM477zx9//vfL/hjOAGNtNJjtDOzco528vMNlI77jnYA4GSWIK2tLWyukWZ3I43NBlLEb2+PRBKbDRRhjTSDp4M0GmlQopEWCAQyXizTSCs9O4O0119/XZI82Ugz/k5jpzi8MNpZqtCJ0U7Am9x3tAMAJ7NcQLa0JdZIK2Zok4uK22wg2xhi/PsSG+2MN9KKEG76vDzayRppsDAaaZnaaBKNNDvYGaS99957kqS9e/cW/DGcgEZa6THamRmjnYA3EaQBQDFZQocWy5pCdo92+jyw2cBNN92k//u//8ut3ZJrIy0ckcxGGqOdnaKRBgsjSMu0PppEI80O+QZpUuHfD4I05MrNo53Gz4IXRzsJ0oDSse9fNABUAkvo0NyaOEG3e7Qz9lyiks/nyiCtublZl156qcLhsA477DDNmTOn8weY63llumC0NNKK2BI0vqueXIcu27pzqCjGaCeNtPIqJEgLh8N5X8Q3Njbq448/luSdIM36GhCkFZebgzR27QRQCPcd7QDAyayNtPbECbrdjbT4k4n9rwuDkG3btpknnz/72c+yfw15rJGm+K6dxRjtNHg7SCMUAY20Ylm9erXuu+++gl+jQoO0fK1YscL8u9uDtHSjdQRpxeWFNdIY7QSQD/cd7QDA0RIXR02tiROYkM1rpMWeitHCct9F7bZt28y/L1u2TA8++GDnD8i2npc52hk2G2mBIpzwe3q0M9u6c6goNNKK4+tf/7rOO+88PfXUUwU93q4gzRjrlGLf+5aWlrw/hlOkawSxq2FxsUZaZox2At5EkAYAxWQJHVrbEifo5SikJUIe913UWoM0Sfr5z3+uxsbGzA8wg7QMgVb89vZw1Ny1M1SUNdJiIl4M0rKOy6KS0Egrjo8++kiStHjx4oIeb7y22QKLYgZpkrtbaayRVnqMdmbGaCfgTe472gGAkyVtNhA7efJFI2X6LW1ykBYOh/Wvf/1Lu3fvLsNzyc/WrVslSdOnT1ffvn21YsUKHXfccdq0aVP6B2Rbz8topLWHJXONtCLu2unF4IBdO2FBI63rotGoeWx75ZVXCvoY5WikSd4N0ggaioPRzswY7QS8yX1HOwBwMkug0mwEaeVq9MSfi9GW+tvf/qazzz5bl112WXmeTx6MRtpBBx2kRx99VP3799eyZcs0ffr0DEFgbqOd4XBEvvgaacFijHbGP60nRzsJ0mBBI63r9uzZY45IvvbaawUFXLkGadb3E6TRSCs1RjszY7QT8CaCNAAoJssFZGt7eYO01B0lFyxYIEl66qmnHH+ha7Q2Bg4cqOnTp+vVV1/V8OHDtW7duvTrpeW62UA40Ugr6minw1/PgrBrJyxopHXdli1bzL83NjZ2CKtyYUcjraGhQWvXrpUk9enTRxJBGjrHaGdmjHYC3uS+ox0AOFma0U5/2daYSiyEH4lE9MILL0iSNmzYYF4kOZXRSBs0aJAkafTo0fqv//ovSdIjjzzS8QE5rufVFo6Ymw0Ei7ABhDnaWcRGWnNzs+rr64v28drb2/X//t//05IlS/J7IEEaLGikdZ01SJMKG+/MNbDw+XxmsJlPWNTW1mY+r8GDB2v48OGSCNLQOYK0zBjtBLzJfUc7AHAyM0jzmY208gVpMeFIRO+++6527dpl3maEak5lbaQZzjjjDEmxRl1TU1PyA/JopBmbDQSLsANEsRtp7e3tmjp1qkaNGqUPP/xQ0WhUV111lWbMmKGGhoaCPuZzzz2nn/3sZ/rOd76T3wOzvaaoKEYjrbMgjUZa54zjmqGUQZqUf1j0j3/8Q3379tXJJ58sSRo3bpx69OghiSANnWONtMwY7QS8yX1HOwBwtHig4vObu3aWY8dOSfKZa6RFOuwQ5/QgzWikWYO0iRMnatiwYWpqatLChQuTH5BtPS8zSItIAWO0s3hrpBWrkPbEE0/o3Xff1c6dO/XFL35R11xzjX7xi19o0aJFWrRoUUEf09ig4eOPP87zkayRhgSjkdbZaCeNtM4ZjbRevXpJUv4tUZUuSHv//ff1la98RQ0NDfL5fBowYIAuueQSzwZpRtBAkFYcXlgjrVQ/C4x2At5EkAYAxWSEDj6f2sJGkFb+0c7nn39ekjRt2jRJhQdp9913ny655BItWLCgpBcgqaOdUuwE3WilPfzww8kPyLprZ+z2tnZLI62Ya6QVKUn761//av79rbfe0uWXX26+vXnz5oI+5o4dOyRJu3btyu9iOMdxWVQGGmldZwRpp512miRp5cqVSU3hXJQiSGtsbNQXvvAF7du3TzNnzlRra6u2bdumc88917NBGo204mK0MzNGOwFvct/RDgCczAzS/GozRzvLwxryGEHaT37yE0mxC7jt27fn9fGi0ai+9a1v6eabb9acOXM0duxYrV69uphPOf58I+ZzszbSpMR453/+85/kk95cRzvbLZsNFGPXzvh/w0U4AV+7dq0ef/xxSdIf//hH83YjuOhqkCZJ69evz/2BjHbCgkZa1xlB2mGHHaYDDjhAknTvvffm9TFKEaR9+9vf1vvvv6/Bgwfr7rvvThpBM4K0QkfLnaCzII2goTgY7cyM0U7Am9x3tAMAJ7MEacYaaUUoPhX6ZCRJqz/+WNu2bVNNTY1mzZqlQw89VJL04osvpn1Ue3u7Tj/9dHNxf8O2bdu0a9cu+Xw+9e3bV6tXr9YZZ5xR1IXxJWnnzp3mieeAAQOS3nfCCSeoR48e2rp1q5YtW2Z5T66jnWH5AsVrpFVXxUKFhsbGnB/T1NSU9uT25ptvVjQa1Yknnqi5c+fq3nvv1U9/+lP94Ac/kFR4kPbpp5+af1+3bp0kafv27dkbJtnGZVFRctlsgEZa54w10gYPHqxzzjlHUizE+v3vf28GEZFIRCtXrsz479O4Xy4jdEZY1Nn347bbbtMdd9whv9+v++67L6kFLIlGGnLi5tHOXP6ddAWjnYA32ReNA0AlMJsYPrW1G82B8jwVY0fJN95YKkk66qijVF1drc985jN677339MILL+hzn/tch8e98847evTRRyVJV199tXlh9cEHH0iSRo4cqRdffFFTp07V+++/r/PPP18PPfRQ0U4SjYvNvn37dmi/VFVV6dBDD9Urr7yijRs3asqUKfEvNttoZ+x70dZe3F076+pqpDZp157ki8y2tjY98MADOvXUU9WnTx/z9rVr12r8+PE6+eST9c9//tO8PRKJ6JZbbpEkfeMb35AknXvuuZIS457GWmf5sjbS1q1bp08//VSjR49WMBjUr3/9a33zm99M/72jkQYLY7STRlrhjEba4MGDddFFF6mhoUHXX3+9LrvsMt1111067bTTdP/99+ujjz6Sz+fT2LFjNWXKFB1++OFat26dli1bZu64nE8j7cEHH9SePXv07rvvatu2bRoxYoSGDRsmv9+vG264QZL0y1/+Uscff3yHj0GQhlww2pkZQRrgTQRpAFBM1tHOcOzvgTL/hvaj+Pjl0UcfLUmaPn26brzxRr355ptp7//OO++Yf1+6dKlOPfVUSYkgbezYsRo6dKgeeughHXfccXr00Uf11FNPac6cOUV5vuk2GrDq1q2bpNi6PibzNc50AR+7vT0cloq4a2fPulppj7R7b/LY04033qjvfve7OvbYY7V48WLzRP2f//yn6uvr9eCDD2rr1q1mSLlt2zZt3rxZPp9PZ555ZtLHGjJkiKTijHauW7dOb7zxhjmmNXfuXN1www369re/rS9/+cvq2bNn4oFmI41QBDTSisEI0gYNGiS/36/rrrtOw4cP189+9jMtX75cy5cvlxQLK9va2rRy5UqtXLky7ceaPHly1s9nXLgbI/2G119/PentWbNmaf78+Wk/hheCNCNMsI7WEaQVF6OdmaX7+Ss1gjSg9AjSAKCYkoK08o52pkZLhx12mKRYo0zKvF7W22+/bf79jTfeMIM044Lu4IMPliRNnTpVZ599tu6++24tX768aEGa0UhLHTEydO/eXVLKmj05rpHW1haWL34BVYwgrXePbtKeiOobmpJuf+KJJyTFxmdvu+02ffWrX5UkLViwIPZ0olH95z//0de+9jVJ0saNGyXFvubq6uqkjzV06FBJxWukvffee5KkMWPGaMeOHVqxYoXmzp2r3/72t1q1alUiKKGRBotcGmlGkEYjraNIJGL+kmDw4MGSYq/XZZddpq985Su67bbb9NJLL+mUU07RhRdeqMbGRi1dulRvvPGG3n33Xe2///6aMmWKRo4cqX79+mnMmDFZP+eXvvQl3XPPPRo7dqwOO+wwHX744Ro8eLDWrl2rjRs3KhqNasCAAfrmN7+ZMQDxQpBGI630aKRlRiMN8CaCNAAoBZ9f7fFGWjECm4KeghJjppLMtdGGDx8uSdqwYYOi0WiHNU1SG2kGayPNcMghh0hSxtZEIXJtpCUHabmvkWY00kJFGO3s06uHtGGP9u5rNm9ra2tL2hX18ssv1xlnnKG6ujpz0wdJeuSRR8wgzQjJ9ttvvw6fw2ikbd26VZFIJO8LldQ10owg5Etf+pLmzZunO++8Uz/+8Y+1bt06rVy5UuPHj4/fm0YaEnJppJX6gtTNPv30U4XDYfl8vg5rP/br108//OEP9cMf/tC8rVu3bpozZ06XfkFx7bXX6tprry348ZJ3gzQjaCBIKw43r5FGkAagEO77tQEAOJnZSJMZpAXKFKSZfD4FAgEzANtvv/3k8/nU0tKSdudOa5D2xhtvmH9PF6QZ7bRSBGnZGmnJo53Z1kiL/actHLaskdb170v/3rFRyNZw1Hw+S5cuVUNDg/r06aPx48dr586duvzyy7V48WK1traaQeDTTz9tPsZopKUL0gYNGiSfz6f29vakdlkuIpFIhyDt3XfflRRrKPbq1Uvf+c53NGnSJEky22qSEq9pxnFZVBIaaV1jjHX269ev09fQabwapNFIKy4aaZkx2gl4k/uOdgDgZJbRzvZw7GKyXEGa2Ujz+XXggQeaI4OhUMgcLTJ2cTTs2LHDXIvL7/dr06ZN2rx5s1pbW7VmzRpJyUGa8feVK1cW7eLZGO3M1Ejr2mhnu2QEaUU44e/ZvVaS5A9Vma2y5557TlJsh9G//vWv8vl8uv322/WrX/1KknT++edr5MiRam5u1tNPPy2p8yAtGAyar0W+45179uxJulDcsGGD3n//fUmJhqL178b7JDHaiSQ00rrGutGAm3g9SCNoKA7WSMuMRhrgTe472gGAk1mDtPhJWTGaT4UwPqvP50sKTSRp2LBhkjquk2a00UaPHm22zZYuXarVq1crHA6re/fu5qihFFtny+/3q76+3rxQ7KrCNhvIMUhrD8tnjnZ2/ftSHYydGPsCVWYYZgRpM2bM0FFHHaWvf/3rkqRXXnlFkjRnzhxzQ4GHH35YUiIgM9ZDS1XohgNGg62urk6BQEBtbW1qaGhQKBTSgQceaN5v3LhxkjI00gjSIBppXUWQVj400kqP0c7MCNIAbyJIA4BiiiZaYO2R2N+L0XwqhHk66/PnHaQdfvjhmjJliqTYeKd1rNN6olxdXa3Ro0dL6ny8MxqNauHChVq8eLE++eSTTp93QZsNKNc10iKJzQaKsEZaTSh+YhwMaePGjWptbdWLL74oKRakSdLVV19thoLBYFAnnniiGaQZmw901kiTEkFavo00Y6xz4MCBSR/7oIMOSgpEjJ+P5CAty2uKikIjrWuyHdeciiANuWC0M7NyjHZaG5f8YgMoDfcd7QDAyczQwaewEaSVa9tOQ5pGmrHhQC5B2tKlS9Ouj2bIZZ20W265RTNnztQJJ5ygUaNG6ZJLLsl4cpetkdbpaGfG9bxit7e2tZubDRRjE4jqYOzz+oPV2rhxo1599VU1NTVpwIAB5mvep08fXXfddZKkU045RT179tTUqVMlxVoqO3fuzBqkGU21Qhtp/fv3N7/nkjr8PBhvf/TRR2ppaYndmG3dOVQUGmld49ZGmnG83bt3r2u/r50FadFo1LVfl5Mw2plZORtpEr/YAErFfUc7AHCypDXSjF07y9VIM9pxuY92vv3225Kk8ePHa/LkyZKk1157TW+99ZakwoO0v/zlL5JibQy/36+bb75Zd9xxR9r7ZttsoOujnbGT2WLs2lkdb6T54o20RYsWSYq10azNvfPOO09vvvmm7r77bkmxi9P9999fUmwTByNIK9VoZ7YgbciQIerVq5cikYgZmjLaCSsaaV3j1iDNaKRFIhE1NTWV+dkUprNdO63vR+G80Egrxc9BNBo1X5tyBWmMdwKlYcvR7oYbbtDIkSNVU1OjadOm6bXXXst435tuukmf+cxn1KdPH/Xp00czZ87scP+vfOUr8vl8SX9mz55d6i8DAHKQGO2M7zVQlBHCQhg5TjCYvB6WlAjSrJsNRCIRc7Tv8MMP18SJE9WjRw9t27ZN9957r6TCgrTly5dr2bJlCoVCevfdd/XrX/9akjR37lytWrXKvF9LS4uuu+46MyDLr5FmjCFm2rXTCNLapUC8kVaEpmBNvJGmYGyNNGOs8/jjj+9w34kTJ6pPnz7m28br9uabb2rXrl2SsjfSOhvt3LBhg/7rv/4raTzTCNL69evXaZBmXUcv8XhGO5GQS5BGIy0zY7TTbUGacbyV3Dve2Vkjzfp+FI410tKz/myVY9dOiSANKJWSX93df//9mjdvnq688kotW7ZMEyZM0KxZs8zGQapFixbpvPPO03PPPaclS5Zo2LBhOuWUU8zf1htmz56tzZs3m3+MizwAKCuzkeZTvJBWlOZTIYzT2f2HDetw8ZvaSKuvr9e3vvUtNTY2qrq6WmPGjFFdXZ0efPDBpECrkCDtlltukSSdddZZ6t+/vy6//HIdf/zxamxs1Lx58yRJTU1Nmjhxon7wgx9Iko499tikCzgro5GWX5AWuz0SVaKRVoTfnBuNNH+wSuvWrdPLL78sSTrmmGOyPtZ43Z599llJUm1trXr37p32vrk00n7605/qtttu02WXXWbeZqyRlq2RJiU2HDB37jQviAhFkNtoJ420zIxGmtvWSPP7/eYxlyANmXihkVbqII1GGuAtJT/a/f73v9cll1yiiy++WOPGjdONN96ouro63XrrrWnv//e//13f+ta3NHHiRB188MG6+eabFYlEtHDhwqT7VVdXa/DgweYf62/5AaBsLKOdxhppoWB5RztHjBzZ4X1GqLJp0yZt27ZNEyZM0F//+ldJ0s9//nPzJGzmzJl6++23de655+qMM87Q4Ycf3uFjGYHQunXrksctJTU3N+vvf/+7JOlrX/uapNjJ5P/+7/9Kiq2/JsXWZlu5cqW6deumv/3tb3ruuef0k3+/q6/f+YZa25NPbo2ArZDRzliQVrxGmrFGmi9YpaVLl6qhoUE9e/bUYYcdlvWxxutm7PI5dOjQjL/Nz7bZQH19vf7xj39Ikp5++mmz/ZJutLOqqkpjxozp8DE6NNJYIw0WNNK6xmid9uvXr8zPJH9u33AgW5BG0NB1bl4jzfhZIEgDkI+SHu1aW1u1dOlSzZw5M/EJ/X7NnDlTS5Ysyelj7Nu3T21tberbt2/S7YsWLdLAgQM1duxYXXrppeZv3dNpaWlRfX190h8AKIloYl2ycPzvIRtPnqyM5sghh4zr8L5BgwYpFAopEonommuu0SeffKL99ttPzz77rH760592uO+9996rhx9+OO2JYL9+/dS/f39J0ocffpj0vocffli7du3S8OHDk/6/wAhyNm/erKamJn388ceSpCOOOEKXXHKJ2iI+3fvaOj31/lbd9crapI/Z6WYDGYM0o5EWlYxdO4u42YAvEDJPwo8++uicTpiNIG3nzp2SMo91SonRzi1btqQNKR544AFz/aJIJKL77rtPUnKQNnnyZHXv3l2nnHJK2hETI0hLNNJYIw0JNNK6Zs+ePZKknj17lvmZ5M8I0pJ3SnYPGmmlx2hnetYQy84gzRpoEqQBpVHSIG3Hjh0Kh8MdauyDBg0yK+7Z/PjHP9bQoUOTLsBmz56tO++8UwsXLtRvfvMbLV68WHPmzMn4f4RXX321evXqZf4xRpoAoOiiiTXS4oU0hYLlCdKGDI4de0899bQO7/P7/WZw87e//U2SNG/ePM2YMaOgz2WEQitWrEi6fdmyZZKkM844I+nErl+/fubF2dq1a80gbdSoUZKk+uY2877XL1yl3ftazbfTbjaQbT2v+O1R+eWLB0TFWLuuxtxsoNq87dhjj83pscZrZugsSDP+f7StrS3tL46MlrfxMY0WoDVIGzx4sDZt2qSHHnoo7ecwRjvNnTt9rJGGBBpphQuHw2YI5eYgza2NNCNIIEgrHUY70yvXGmk+n8/8fARpQGk4+mj3v//7v7rvvvv073//WzU1Nebt1hGjs846S48++qhef/11c7e0VPPnz9eePXvMP6m71AFA0Zihg0/haOyislxrpBkXCoEMJ2/GLxWMpsTZZ59d8Oc66KCDJMVCGCtjTa/UX2D4fD4zNFuzZo3WrFkjSRo9erQkqb4pEaTtaWrTH59NfFxrIy1qCS5jMl3AxxtpvsT3opijnf6q/IO0oUOHJq0D11mQVlVVpQEDBkjqON65YsUKLVmyRIFAQPfff78CgYBef/11rVq1ygzdjHGyHj16ZPyt+NChQ9WtWzeFw2GtXbuW0U4koZFWOGuTq1evXmV8JoVxe5BmhBnWIMMa+BCkdZ2bRzu9uEaaJII0oMRKerTr37+/AoGAuVaLYevWrVl3Lbrmmmv0v//7v3rqqac0fvz4Tu87evRo9e/fv8MFnKG6ulo9e/ZM+gMAJWFZI81opFWF7PstpJURE0Uy5CDWxeenTZuW9Ha+jFHNTEFaumO+NUjL1EgLxcOu21/+RLe8uEbRaNQMnyKRiJqbm2MfLMc10qK+xIlsMTcb8AVi4UIoFNLUqVNzeqzP50tqpRnjm5lkWifNGOM89dRTNX78eJ188smSpLvvvjupkZbL8zECt927dzPaiSQ00gpn/LKiqqpK1dXVWe7tPF4J0qxBhs/nM98mSOs6GmnpWUMsu18bgjSgtEr6L7qqqkqTJ09O2ijA2Dhg+vTpGR/329/+Vr/61a+0YMECTZkyJevn2bBhgz799FPzIgMAysYMbPyKxi8qyzXaaSwBFslwUWttiXWljSZJBxxwgCRp9erVSbcbY/zpjs/pgrREIy124jd2cA99YfL+Ckei+tWj7+vSu5epuqbW/BjmeGe2MUQz4Ex8L4rRSKsxNpKIB2mTJ09WXV1dzo+3BmmdNdKs70/dxfqDDz6QJHMs94ILLpAk3XbbbUm7dubCaMvELvwZ7UQCjbTCGWvzuvUXuV4M0qxvE6R1HWukpZeuDWkXgjSgtEoejc+bN0833XST7rjjDq1YsUKXXnqpGhsbdfHFF0uSLrzwQs2fP9+8/29+8xv9/Oc/16233qqRI0dqy5Yt2rJli1mLb2ho0I9+9CO98sor+uSTT7Rw4UKdeeaZGjNmjGbNmlXqLwcAsrCEVv7YSXpV2YI0ox2S/v3FDNKyNdI6C9JWrVpljtybQVq8kdazJqTfnj1evzzzUFUF/Frw3ha9uPpT1dbGwjRzZCrbGGI0zWhnMTYbCFkaDsEqHXfccXk9Pp8gzfh+pS5PsHZtbDOGESNGSJI+//nPq1+/flq/fr15YZDrToFGkJbUSMs4LotKQiOtcEaQ5saxTsn9QVq6NdKsbxOkdR2NtPQyhbh2IEgDSqvkR7tzzjlH11xzja644gpNnDhRy5cv14IFC8yFk9etW2deaEnSX/7yF7W2turss8/WkCFDzD/XXHONpNiB6O2339YZZ5yhgw46SF/96lc1efJkvfDCC66sywPwmHh7JyKffP7YSUy5Rjv9WS5qJ0yYIEk65phjNHLkyC59LqORtm3bNvOisbW11WxEdTba+cILLygcDqu6utq8n7FGWs+akHw+ny6cPlJfmLK/JOm5ldvNDQc6BGmKpg/TzKZg7HsR9PuK8ptzY400Sfrvn/5cP/zhD/N6fD6jnZmCtE8++URSIkirqanRf/3Xf5nv79mzZ6fhh1Xv3r0lxRtpjHbCgkZa4SqxkbZr1y69+eab5s9NOWV6/Y1wg6Ch61gjLb1MIa4dCNKA0rLl6m7u3LmaO3du2velbhBgXBBkUltbqyeffLJIzwwAisyy2YAvfnJWrkaazxztTP/+Y445Rk8//bQOO+ywLn+uXr16acCAAdq+fbtWr16tSZMmmetjhkIh9e3bt8NjjCBt586d5tvGCW19c+zEr2dt4v+mZowdqL+/uk7PfbBN3bp3144dOyyjnZaT92g08cWbt8UDTnPHzuKMn4QCfgX8PoUjUX1/3g81oGdN9gdZFBKkrVu3zrytubnZHJ81gjRJ+sY3vqH/+7//k5T7WKeUMtpJkAYLGmmFM9ZIq6QgbdasWXr99dfVrVs3HXHEEerevbvC4bA2bdqktrY23X333Tkt31IMxuuf2gikkVY8jHamx2gn4F3lqUkAgFcZI4TyJUY7y9xIy7RGmiTNnDmzaJ/vgAMOSArSjLbxoEGD0v6WOrUFZwRrUnIjzXD0mH6qCvi1YVeT6gaPkj75JGk3PFM0og6FayMMircEi7HRgKE66Ne+1rCa2/I/CT/kkEP0+c9/XkOGDMnaqjY2g7A20oy/19XVJY1vHnDAAZo1a5aefPLJnMc6pdTRzviNBGkQjbSuqLRG2tq1a/X6669Liq1j+cILL3S4zznnnKM333zTlteEIK30GO1Mj9FOwLvcd7QDACczRgijUfniQVqx2k/5Ms5nOwvSiil1nbTONhqQpO7du2vAgAHm28b6aJJljbTaxEV7XVVQ00bHmm2B/WK7OadtpKVd0yt2W7TIjTQpMd7Z0p7/xZjf79c///lP/fGPf8x6X+top9H4sa6PltoE+NGPfqRAIKAjjzwy5+eTfrQz54fDw2ikFa7S1kh7+umnJUlHHXWU3nzzTd1zzz267bbbdNttt+nxxx/XiBEj9PHHH2ecVik2grTS88JoZyl+DhjtBLyLRhoAFFP8ZDIqn7lDZDEWtS+ET51vNlBsqUFaZxsNGEaNGqXt27ebfzcYu3b2qk1uv8wYO1AvrNqh9oFjJaVbI03pG1RmUzC+RlqgeCf7NaGApDa1tJe2hbP//rE14vbt26ddu3apb9++HTYasDrppJO0bt26pLAyG0Y7kUkuQRqNtPQqrZH21FNPSZJmz56tiRMnauLEiUnv//vf/67jjjtOd911l77whS/o9NNPL+rztWpra1NTU5OkjkGaETQQpHUdjbT0GO0EvMt9RzsAcDKjkSaffPHfQAbKdGKZWCPN3kba6tWrJSWCtHQbDRis4Vn6RlryyeeMgwdKkpp77CdfVa0lSLOElWmDtPht8QCtmOGm0UhrbivtxVhNTY0ZihkjnZ0FaVJs3bXORvFSJY12GrOdBGlQbqOdNNLSc/saacbz3rVrV9b7hsNhPfPMM5KkU045Je19jjnmGH3rW9+SJP373/8u0rNMz3jtpcybDRCkdZ2b10gzfg4Y7QSQD4I0ACiqePMpGjXXSAuVa7TTXCPNns+X72inlBykWf++J80aaZI0qn83Detbq6gvoOohB2XYbCBzkBYxdu0s6mhn7Ptc6kaa1HHDgWxBWr7YtROZ0EgrnNsbacZ6lh9//HHWkHTZsmXatWuXevXqpalTp2a836xZsyRJL730UtGeZzpGkFZXV9chBCZIKx4aaekx2gl4l/uOdgDgZOYaaT75fEYjrVxBWvwp2dQOOeCAAyRJGzZsUFNTU06jndYWWtrNBmo7tl8G9ojtjJncSEvZtTOVMXIb/54Uc7OBmlDha6TlK3XDgWIHaWlHO1kkreKFw2HzIpNGWv7cvkbaAQccIJ/Pp71795q7MWdijHWeeOKJnY6zHX300ZKkDz/80BzvL4VM66NJiXCDoKHrvLBGGqOdAPLhvqMdADiZOdops5FWrjXS/D5710jr16+febHy8ccf5zXa2bdv36QLnfrm2IlfaiNNSgRXvmB1Hmukxb8vJWykFbJrZ76sGw5IpQvSYrt20khDjDHWKdFIK4TbG2nV1dXmMWbVqlWd3tcI0jKNdRr69u2rcePGSZJefvnlIjzL9HIJ0mikdZ2bRzvZtRNAIQjSAKCYjEXto3LAGmnGaKc9SZrP50taJy2X0c5jjjlGJ598sn74wx+at0WjUUsjreNvcWviwZU/VJUY7VRua6RFzXCzeN+TahsbadYgLRwOa8OGDZJKNdrJGmmIsQZpNNLy5/Y10iTpwAMPlNR5kNba2qpXXnlFUmyzk2yOOeYYSaUd7yRIswejnekx2gl4l/uOdgDgZMZaXFE5oJGmxHOxiRGkvf3222aQ1lkjrba2Vk899ZTmz59v3tbUFlZ7/Emnb6TFXteMjbS0o4gpo52lWCPNxkbaunXrtGnTJrW3tysYDHYaVubDOtoZNTcbIBSpdMb6aBKNtEK4fbRTkg466CBJsVHMTFauXKnW1lb16tXL/P+CzhCkeUcljHZGo1E9+eSTuvPOO3P+2Ix2At7lvqMdADiZZbTTWCOtmGOE+fDb3EiTYuviSNLNN99stlg6C9LSqW+K/wbX71NdVcff4iaCtKoMmw2kWyPN2GzA+J4Uv5FW6l07peRGmjHWOWzYsKL9ttu42IxEItrX3By7kUZaxTP+Lft8vk5/1mikpef20U4pt0ba8uXLJUkTJkzIacTPCNLeeOMNNRvHmyLrLEgzggaCtK7zeiPtww8/1OzZszV79mxddNFF5s96Nox2At7lvqMdADiZgxppPps3G5Cks88+W6FQyAx5+vXr12mDJZ36ZmPHzmDaizFzjbSQtZGW62hnfI20In5PqoPGaGfpAydjs4GNGzdqzZo1koo31inFdrYzTr4bGvfFbiQUqXi57Ngp0UjLpFJGO41wYeLEiTl9zAMOOEADBw5Ua2urli5d2tWnmBaNNHt4eY20aDSqU0891Vz/T5LeeuuttPf99NNPdc8995jBMKOdgHcRpAFAUcXXSItE5fOXd420RCPNvs/Zt29fzZkzx3w73zaa1PmOnVJyIy2nIM0aBJmjncXctTM+2mlDkDZkyBD5/X61tbXptddek5R/kNbaHtGXbnpF8x5YrnDKD4fP5zMvOBsa4m0/GmkVz2ikdbY+mkQjLZ1wOGw2Z70SpGUKHIxwIdcgzefzma20WbNmaeLEiXr22We7/mQtKjlIi0Qi2rFjh22fS/JmI+3jjz/W6tWrVVVVpc997nOSpPfff7/D/bZt26Zjjz1W559/vv70pz9JYrQT8DL3He0AwMksmw1UYiNNks4//3zz74Ws3WU00nplDNISu3YmNhuQZZfJlK/X8nbEX4pdO+0b7QwGgxo6dKgk6Z577pGUf5C2YnO9Xl79qR5ctlG/f/qDDu83NhzYawRpadecQyWhkVa4vXv3mn93c5A2cuRIBYNBNTc3a+PGjR3eH41G826kSdKXv/xlVVXFxvTfeust/fznPy/SM44xgjTjuGZlBGleDRrmzZunAQMGaOHChSX/XF5eI+2NN96QFPu5PvnkkyV1DNJ27typU045RStXrpQkPfPMM5IY7QS8zH1HOwBwMstop9FIK/8aafZ+3tNPP109evSQVGCQFl8jLd1GA5JUm66RJlmCtNRGWuLtxGhnEddIC9rXSJMS66Tt3LlTAwYM0EUXXZTX49ft3Gf+/YbnVmvBu5uT3m80N/bSSEOcEaTRSMufsT5adXW1qqury/xsChcKhTRq1ChJ6cc7N2zYoJ07dyoYDGrcuHE5f9zPfe5z2rNnj1577TX5fD69/PLL+uSTT4r1tCu2kbZmzRqzFXXvvfeW/PN5ebTz9ddflyRNmTLF/NlODdJ+/OMf66233jLPfV5++WWFw2FGOwEPI0gDgGIyG2lRs5EWKPuunfZe1NbW1urss8+WJPPCKx/mGmm16UchzNFO6xppsVti/+ksSCvJrp3GGmn2XIwZo1AXXnihVqxYodGjR+f1+PW7YkGa8by/efcyzfz9Yt3+UmzNNUY7kcoY7aSRlj8vrI9mMMY70+3cabTRDjnkkLwDw5qaGk2dOlUnnHCCJOm+++7r0vO0qtQg7eqrrza/rqeffrrk4bYXRjsz/RwYjbSpU6eaQdqaNWu0b1/il1KLFi2SJN11113q2bOn9u7dq7feeovRTsDD3He0AwAnMxtpUfniJ2fFbD/lw2+2Q+z/3L/73e90zTXX6Hvf+17ejzXXSMvQSKtOt2unZNm5M/ULTrxtNNKKGW4awV5zmz3hwW9/+1tt2bJFd9xxh/r165f349fvbJIkffXYUfr/Ju2nUMCnj7Y16Kr/vK/Ne5rMEaj6vfGQknZRxaORVjgv7NhpOOiggySlb6QVMtaZ6ktf+pKkxNh6MVTirp3r1q3T7bffLin2b3LdunVavXp1ST+nV0c7w+GwuRHGlClTNGDAAPXv31/RaFQffBBbGmHPnj366KOPJEnHHnusjj76aEnSCy+8wGgn4GHuO9oBgJOl2bWzXI00nznaaf9FbZ8+fXTZZZepb9++eT+2vjk+2plpjbR4k8ofjDXSzIv2nEY7i7/ZgJ27dkqx7+ugQYMKfvz6+GjnqP7d9PtzJuqNn52sAwd2lyQtX7fbMtppBGm0iyodjbTCGUFauiDHbYxG2pNPPqnvfe97+v3vf6/t27dLyn+jgXQ+//nPKxQK6Z133tG7777b5ecrVWYj7ZprrlFbW5tOOukkfeYzn5GUWLOrVNzcSDN+DtIdtz788EM1NDSorq5OhxxyiCR1GO9ctmyZpNh6pf369TNf8xdeeIHRTsDD3He0AwAni4cO4UhUPnM9rvJuNmD3GmldlWikpR+FqK1KNNIikYhaWlpi78ghSIv4YuFcMb8n1fHND1ps2GygGIzRzuF96yTFNnWYOioWeC5fnwjS6gnSEJfrZgM00jryUiNt7NixkqT33ntP119/vS677DLtv//+mjFjhhnUTJgwoeCP36dPH5166qmSireuVyUGaS+++KIk6Vvf+pZmzpwpyb4gzWtrpBnrox1xxBHmz0tqkGaMfk6ZMkWSdNxxx0lKbqQx2gl4D0EaABRVfI20SGKNtPJtNhD7bzkaaV2xxwjSMjbSEmukSUqsk+bLvkZa4ntSvP/7M55Ps02NtHzta23X9+97U/95a5PCkag27oqNdg6LB2mSNHFYb0mxIM0c7ayP7zZIkFbxjEZattFOGmkdeWmNtOOPP16XXHKJPv/5z2vevHmaPHmyWltbtWjRIu3Zs0ehUKhLjTRJ5vqaTz31VBGesbR7925J+QVpmzdv1rhx4/SrX/2qKM/BbuvWrZMUaxAaQdqzzz5b0sDQzY20zo5b1vXRDKlBmjH6OXnyZPO+1dXV2rZtm1asWCGJRhrgRfbH4wDgZUm7djpljTR3BWnmZgMZ1kgz1iQLVNVIigVp/fv3tzTSUr5ey9uReEuwqJsNOLyR9srHn+qh5Zv02pqdmjS8t9ojUYUCPg3qWWPexwjS3tm4R0f2jjfS9jZI/aWOa86h0tBIK5yXRjuDwaD+9re/Jd321ltv6b333tO2bdt0+OGHF7Ruo5XR5nnzzTfV2Niobt26Ffyx2tra1NQU+8VBZ0FaatDw+OOPa8WKFbryyis1Z84cs2nkBo2Njfr0008lScOHD1e3bt3Us2dP7dq1S2+++WbJvhYvrpEWjUb12muvSVLS65YpSDPuU11drSOPPFIvvPCCnnvuOUnlDdK81rgEnIIgDQCKyTLa6ZQ10tx2TVvfZKyRlmnXzvgaafFGmrnhgNlISw3SEifHgapaScUNN6vjjTS71kjL167GWDC5aU+zXv14pyRpv961ST+XBwzoru7VQTW0tKu1NnYhzGYDMNBIK5yXRjvTmTBhQpfGOVMNHz5c+++/vzZs2KDXXntNM2bMKPhjGW1AKf3rn6mRZuxKGo1GNXfuXL388suuCYjWr18vKfb1GuHhCSecoEceeUSLFi0qapDW2NioJUuWaM+ePWYz3M2jnevWrdNnP/tZRSIR7du3T++995527NghKX0j7aOPPtLWrVvNjQaOOOII8z4nnniiXnjhBbPRxmgn4D3u+H8FAHALI0iLln+NNL9b10jLsZHmTx3tVPbRzuqa2DhjMRtpRrDX7NBGmvF6StKDb26QlDzWKcXC3sP3i1107fLFLjiNAIDRTtBIK5zXg7RSMHY9fOmll7r0cYwgra6uLm0InC1Ik6RXX31Vf/3rX7v0POxkjHUOHz7cvM0IeIwxw3xFo1H9/Oc/15lnnqmLL75Y55xzjiZMmKDevXvr5JNP1tlnn6133nlHkjsbafvtt58kqampSY899pieeOIJLV68WDt27FBNTY2+/OUva8yYMeb9Bw8erN69eysSiejPf/6zJGnkyJFJbcyvfOUrSaEio53wkueee07du3fXXXfdVe6nUlbuO9oBgJNFrWukxUc7y7ZGWvl27ewKY7OBXpnWSIsHaQqmrpFm/F9a5q+3qi62O2UxvydGI63VoY00Y805SXp5dWzkJzVIk6SJw3tLkra0xUY+99Q7s5H273//W4MGDdLTTz9d7qdSMWikFc5La6TZ5ZhjjpFUvCAt01htptE3I0ibPXu2pNii/RdccIE2b97cpedjh3RBmrFJxAcffFDQx3zmmWf061//Wo888ohuv/12PfDAA3r77bfV3t6uESNG6LDDDpMUC9KtgZNbjBw5UsuXL9e9996rm2++Wbfddpvuu+8+s2135513JoViPp9PJ554oiTpl7/8pSR1aPqNHDlSp59+uvk2QRq85LbbblNjY6Puv//+cj+VsmK0EwCKKR46xHbtjC9sX6bf0BphkVNHDtOJRqOqbzZGOzMFafHXMxB7f2K0M/uunVW1dWpVsUc742ukOfR1NkZlpUQmNqxPxyBtwv69JUlrG2I/N7sd2ki77777tG3bNl111VU6+eSTy/10KgKNtMJ5aY00uxhB2pIlSxSJRApuOWUL0tI10sLhsFavXi1Juv7663X99dfrhhtu0N///nc9+eSTuv/++80QxYlKEaRdffXVkqQzzjhDRx11lKqqqnTwwQfr8MMPNz/Phg0b1NLSogMOOKArT79s8h1Rvvnmm7V582YtWbJEUmKjAatvf/vbeuSRRyR5b7Szvb1db731lnw+n/r375/08wbvM37JUWjL1SsI0gCgmBy0Rlq/brGL3p2NLWX5/IXY1xqOvXbKPtoZC9J82rkztu5XtiAtEo2quqZODSr2aKexRprzRzsNw9M00ibFG2nr69vlC9VoT/1eSX7HBWlGW+Tll1/We++9p0MPPbTMz8j7KrWRtnz5ct122226+OKLC96NktHO/E2YMEHdunXTnj179P7775uNp3wVEqStX79eLS0tqqqq0ujRo/XHP/5RF110kS655BItX75cJ598ss4880y1t7dr4MCBOumkk9StWzc9++yz2rFjhw4++GAdeuihOvTQQ3XAAQeYnyMajSoajZZ89DFdkHbQQQdJknbs2KGdO3eqb9++OX+8V155Rc8995yCwaD++Mc/ZgxM9t9//y48a/fp06ePnn76aZ1zzjl65pln9NnPfrbDfWbOnKmDDjpIH374oecaad/97nf1l7/8xXz70ksv1R/+8Ies/x/hdRs2bND//M//qKmpSYMHD9b06dN1yimnqKamJvuDXWLLli36+OOPJUlr1qxRc3Ozp76+fBCkAUBRGY20iGXXzjIFad1jo4+fNrSW5fMXYu2n+yRJPWuCieZZitpQ4oTUF6wyF/pNbDaQPkiLRqWqmvhmA4HiN9Ka25wZHlhHOw3D+tZ2uG1Qzxr1rAmqvrldwZ4D1NCwQVIPRwVp0WhUq1atMt+++eabde2115bxGXlHNBrNuFB4pTXSmpqa9OMf/1g33HCDIpGI7rjjDj3zzDMFLdTOaGf+gsGgpk2bpmeffVYvvfSSrUGaEdSPGTPGfP+UKVP08ssv61vf+pZuv/12/fvf/zbvf8stt2T8/D6fTzU1NQoGg9q3L/b/bePGjdP06dN11VVXaciQIQV9XZ1JF6R169bN3MDhgw8+0PTp03P+eEYb7ctf/jKtoxTdunXTo48+mjFI8Pv9+slPfqKvfOUrBf8Md0Upg7Qnn3xSkjRw4EBt375df/nLX/T+++/r//2//6cJEybonnvu0SOPPKJDDz1UF110kblBgyEajerRRx9VVVWVTjrpJAUCAa1du1Z1dXUaOHBg0Z+vHTZu3KgTTjjBbLQaevToof3331/t7e2aMmWKfvazn5mvx+bNm/XPf/5T7777rurr69W3b1994xvf0OGHH65PPvlEe/fu1fjx48vx5WRkHbmPRqP68MMPHfcc7UKQBgDFZDbSZK5CGSjTGmlGI21Ho3uCtPc3x9ob44b2zHhRX5MSpJkLQ5uNtNRdO+Pr1kWlYFUsXCxmuFkdMkY7HdpIs6w5Z4Rq6UY7pdg4bX1zu3zV3RQxi4zOCUW2bNmSGOWVdOedd+rqq6+u2N+GdlV7e7v+9a9/6ZprrtE777yjSZMm6aSTTtKPfvSjpPDBCNIqpZH2k5/8RH/84x8lSUOHDtWmTZt08skn6+mnn04K06LRqF588UWtX79ewWBQzc3Nqq+vV48ePTRx4kSNHj2aIK1AxxxzjBmkfeMb3yjoY+QapFmDBuP/T4wWl6G2tla33nqrvvCFL+jDDz9UXV2dVq1apWeeeUbNzc064YQTNHz4cK1YsULvvfeeVqxYoaamJjU1NSV9nHfeecf88/zzzxd95C9dkCbFxjvzDdI++OADPfLII/L5fPrxj39c1OfpJZ39/89FF12kmTNnliQ0zaZUQdrOnTvNRtLKlSv14osv6vzzz9fixYt17LHHyufzmb9Meeyxx/Tb3/5WkydP1oUXXqhTTz1VAwcO1De+8Q3dd999kqQBAwaotrZW69atU9++ffXee+9p8ODBRX3OpbZ9+3bNmDFDq1ev1qhRo3TJJZdo7dq1evTRR7Vx40ZzBHLVqlW67777dOihh6q1tVWrVq3q8IunP//5zxo0aJC2bt0qKbYu7FlnnWX3l5RR6tqVK1asIEgDABRBPEhrt2yVGSrTGmmJRpp7Rjvf3xQP0oZkXk8o4PcpFPCpLRyVL1SVaChlHe2UQlU1Untxg7Sa+GYDbeGowpFo2UZ5MzHWnDth7AA9vHyTelQH1bsufSDSoyYkqUm1PXorsid+o4Maacb3esSIEYpEIlq/fr2++93v6tJLL9X48ePLMj7jVtFoVJ/73Of06KOPmre98soreuWVV3T//ffrX//6l8aPH69oNKrm5mZJldFI27t3r9kyuvvuu3XmmWdq9uzZeumllzRjxgw99NBDOumkk7R161Z961vf0oMPPpjTx2WNtPzMmDFDv/rVr/TYY4+ptbU1689eOkaQ1rt377Tv76yRlhqkSbGf71NPPVWnnnpq1s8dDoe1Y8cONTU1qa2tTXV1dWpvb9fSpUt18cUXa8mSJfrNb36jn/70p/l+WRlFIhFt2BDbmTldkLZw4cK81kkzRvdOO+00c5015M/YFdRu/fv3lyS9//77Rf24y5YtkySNHj1affr00emnn65XX31Vv/rVr/TEE09o9+7dGjlypC6++GItW7ZMjz32mJYuXaqlS5fqe9/7ngKBgMLhsILBoHr37q3t27ebH3vnzp268sorXbVTrhT7t7Jq1SqNGDFCzz33nEaMGCEp9m/yzTffVH19vdra2nTjjTfq3//+t959913zsdOnT9fMmTPVt29fvfLKK/rnP/9phmiSNG/ePM2ZM0fV1dW2f13pGEFar169tGfPnopeJ40gDYDrffTRR1q9erVOPvnk8m+9Hg8d2trDUvy8v1zBSv/usSfgptHO9zfHLnzGDe28vVETDKgt3C5fsFoffvhhbCxNWUY7JQWqqmJBWjFHOy0jqC3tYdVVOev/Wo1G2lkT99Py9bt19AH9M7b9elTHnnu33v3UviF+o4OCNOMi9+CDD9ZJJ52kyy+/XDfddJNuuukmVVVVacyYMTrooIN04IEHqlevXgoGgxo5cqSmTp2quro6bdu2TT179tSIESMyvgaV4pFHHtGjjz6q6upqzZ8/X5/73Oe0bNkyXXnllfroo480adIkBYNBs40m5d5Ia2xs1PPPP28uTO4md9xxh/bu3auxY8fqvPPOk9/v1xNPPKGzzjpLzz77rE499VSNGzdOK1euVHNzs4LBoI499lhFIhHV1NSoZ8+e2r59u5YvX24GOfvtt58OPPDAMn9l7nLcccdpyJAh2rx5s5544gmdeeaZeX+MQkY7jbA+XZCWj0AgoEGDBnW4fcSIEWpsbNSFF16oq666SnPmzNERRxzRpc9l2L59u1paWuT3+zV06NCk9+W74UBjY6Nuv/12SbFF8+E+p556qnw+n1599VVt3LixaIHeG2+8ISl5p9JDDjlE99xzj9rb2/XJJ59o5MiRZiNux44duu+++3Tvvfdq6dKlamlp0X777acHHnhAU6dO1fPPP69IJKJIJKLZs2fr5ptv1ne/+11XrX+6ePFiSdL8+fPNEE2K/X+idSOKU045RR988IHWr1+vUCikUaNGdQi9169fr08++URjx47VpEmTtGbNGv3hD3/Q5Zdfbs8X04l9+/aZQeoFF1ygG264QStXrizzsyofZ53tA0CewuGwTjrpJK1bt05Tp07Vn/70Jx155JHle0LxJsae+r1SfHqu3GukNbWFta+13XEBT6poNGpppGUJ0qoC2tvSrmB1rfbt2qRNmzZpP6OR1mEUMTHaGQjGLuqLudlAdTDRgmppi6jOYbmBEaSN7N9Ni380o9P79qiJ/YwMHjZK696xvCMaTaxBV0bGRe6BBx6oH/7whxoxYoTuv/9+PfHEE2pqatL777+f02/f+/btq8MPP1xjxozRwIEDFY1GFYlEFI1GVVtbq0MPPVRHHnmkRo4cWeKvqDza2trMk/LLLrtMV155pSRp/PjxOv300/XlL39ZTzzxRFKIZqxl0xkjsKivr9fxxx+vSZMmaenSpa4JLSORiP70pz9Jkr7zne+YwWCPHj30+OOP64ILLtA///lPLV++XJI0ceJE3XbbbWk3IohGo+ZoX48ePVwXKJZbIBDQl770Jf3ud78zm4H5yhakGRf66RpppQw+L7jgAj388MP617/+pauvvlr/+Mc/ivJxjbHOoUOHdgi9cw3S3n77bQ0aNEiPPvqo9uzZo9GjR+uUU04pyvODvYYMGaLp06fr5Zdf1kMPPVS0QHTp0qWS0u9UGgwGNWbMmKTb+vfvr7lz52ru3Llq+//bu/P4qMq7/ePXzCQz2fcVwhJ2kLAvxqUuICDo4/aoKApSKz8VaxVrqz4udWm1mwpqXdqqtXVHoGLRGkFRFAHZQfY9IXvInkxmMuf3x2SODCQwgWzC5/16RTNzzpy5zySHZK587+/tcmn37t3q1q2bOSX28J8rV1xxhebPn697771XixYtapHxtra6ujpz9dZzzz33uPv37dv3mBWeXbp0UZcuXSR5exROmzZNTzzxhKZNm9ZoON+WVq1aJbfbrU6dOmnChAl64YUXqEgDgB+rpUuXmr88rlq1SqNHj9bQoUM1ceJEud1u1dXVqUuXLurRo4fS09OVlJSkHTt2aPfu3crIyNCQIUNatoqtoXqnvNLbx8kiydpOQVq43SZHkFVOt0fFlXUKi+vY/+TnlNaovNatYJtFvZIijrmvbyGCzl3TtStvl7Zv367O5mIDR/ZI+2FqpzUoWJKhoBb8mh8+1dTp7jjVW5J39dgKp3dqZ1TI8b/+EQ379Oo/UHv/c9iGDhak9enTRxaLRddcc42uueYa1dfXa//+/dq+fbt27NihnTt3qrq6Wk6nU1u2bNH69evlcrkUHx+vsrIylZSUaOnSpeZfkRtjsVj0zTff6Mwzz2yr02szr7zyirZv367ExMSjeh/Fx8dr0aJFysnJkWEYcjgccjgcCg0NPW5FWrdu3fT2229r3rx5WrBggdauXavvv//+R1NZkJWVpW3btikyMlJTp0712+ZwOPTOO+8oKytLbrdbvXv3Vu/evZv8+WGxWBQWFqawsMb7EeL4brjhBv35z3/WwoULVVpa2uQUzaY0tyLN6XRq7969kk6+Iu1YLBaL7rvvPn3wwQfmHwFCQ49eAKa5muqPJv0QpO3cuVP19fWNToN//vnn9fOf/1x2u93s6Xfrrbe2f6U/TtgVV1yhb775RvPmzWuTIO14goODjxki/f73v9fChQv18ccfa8WKFRo9evQJj7OtrFmzRjU1NYqPj1f//v1b9Ng33HCD5syZo9WrV+tvf/tbi04FPxFffvmlJG8PS9+5btu2rcl/U051HftdFQAch69Z6VVXXaWIiAi9+eabWrt2rdauXRvQ4xMTEzVgwAB16tRJNTU1KiwsVG1trTwej3r16qXx48dr+PDhSk5OlsViUVFRkVJSUszeE0fxLTbQkOUEt+AUwuayWCxKiHAop7RGRZVOdYnr2G/ofNVovZMiZQ869uvm60uW1q27dq30BiwXBNAjzWoLllSnoBZeAMLRMNW01tWxFhyoqP1hxc6o0OMvS++rSEvr0VuewwNJ47DVM9pRU9UiNptN6enpSk9P1/jx4496nMvlksViUVBQkJxOpzZu3KitW7dq586dOnTokKxWq/lx6NAhff7559q9e7deffXVUy5Iq6qq0qOPPipJevTRR5tsgn+i04AmT56syZMna/z48fr000+VlZX1ownSfL3Rpk+frsjIyKO222w2TZgwoa2HddoaPHiwBg4cqE2bNumPf/yjMjIy5Ha75XA4NGDAAL/vq8LCQj388MNaunSpKioqFBYWFnCQtmrVKk2dOlVxcXHyeDyKjIxs9cqP4cOHq0uXLjpw4ICysrL0P//zPyd9zGMFaV27dlVISIhqa2u1d+9epaWlyW63m9Wib775pn7+859L8lbYFBUVyeFwaPr06Sc9LrSfK664Qvfee6+WLl2qoqKipn93DVBxcbH27NkjSS02JflwvXv31vXXX6833nhDzz333I8iSPOFS+eee26LV19brVbdfvvtuvnmm/XOO++0a5Dm8XjM6d4TJkxQ9+7d5XA45HQ6tW/fPvXo0aPdxtZeCNIA/Gi5XC598MEHkqTbb79dF154of70pz/p/fff15o1axQRESGbzab9+/drz5492r17t0pKStStWzd169ZNq1evVmFhYZNVKWvXrm10ykV8fLx27Nih2NjYRh5lNPzXGzq0d+P5+Ai7ckprfhR90nwrdvY/zrROSQq1e98ApXbx9qLYvn271LWpHmmG+T9Lwxunlg44HUFWVTrV4SrSymu81WhhdltA5xzh8IZtUXFJioqKljlNtgP0SfN4POay8s2ddnV4JZXD4dCIESP8+rscacmSJRozZozmzp2r559//pSalvfiiy+qsLBQPXr00M9+9rNWe55x48aZQdpdd93Vas/TUqqqqsyFF46sRkP7sFgsuuGGG3Tffffpd7/73VHbhwwZop/85CcyDEP/+te/dOjQoUaPExcX1+j9viBtyZIlfvf7Kl5bk8Vi0ZVXXqnZs2dr3rx5rR6kWa1W9e7dWxs3btSNN96ob7/9VnFxcRo6dKi5mqfkndJ83XXX6bXXXtO555570sEL2lfPnj01ePBgrV+/XrNnz9bVV1+tnJwcff/990pNTdXYsWOVlJRk7u/xeOR2u5v8mefrj9WzZ88mfgc+eXfeeafeeOMNvffee/rTn/7U4Vfw/OqrryQFNq3zRFx55ZW69dZbtWnTJm3atEkDBw5slec5nk8//VS7d+9WdHS0Jk+eLJvNpj59+mjjxo3asmULQRoA/Jh89tlnKikpUXJyss477zxJ3l4Mt912W5OPcblc5pvquro6rV69Wnv37tXBgwcVHh5uLsNtGIZWrVqlrKws7d69WwUFBZK8v4wWFxfrtdde06xZs45+Al9o09D4vr36o/nEhzcsOFDV8VfuNPujHWehAemHirSk1DRJDUFaN19Fmv/UztraGoXIW5Emq/fHXkt/XRwNFXROd8eqSCtr6I8WFXL8ajTph4q0Sme9zv3JeZK+aNjS/qswZmdnm83dD2/m2xrOO+88paSkKC8vT1lZWZo0aVKrPl9bqa6u1h//+EdJ0v/93/8dd6rmybjoooskSV988YWcTmeHWXGsKb4pdunp6a1SaYET89Of/lQffPCBysvLlZycLIfDoerqaq1cuVLr1q0z+9VJ3h5/jz32mDp37qzc3Fx98sknqqurM78Xj+SbKmq323XTTTdp69at+vLLL1sk1AqEL0j78MMP/X43aa5Vq1Zp/fr1Zp+mxoI0yTu9c+PGjeZ+xcXF+uyzz8ztP/vZz/Tss8/KarUqMzPzhMaCjufKK6/U+vXr9cQTT+iJJ544avuoUaM0efJk5ebm6rXXXlNxcbHS0tI0YMAAnX/++brgggs0fPhwBQUFndS0zkANHz5cmZmZWr58uV5++WWzh2dzuVwuVVVVqbq6WrW1tWYVZkvyeDxatmyZpNYL0mJiYnTxxRfrww8/1LvvvttqQdrOnTv1/vvva9++fRo5cqSGDBkit9ut0NBQZWRkmKv4Tp8+3WxZ0L9/fzNIO1V+T2oOgjQAP1q+aZ3XXHNNwHPzD/9F1W63KzMzs8lfGCdNmqTf/OY3kn7oofLqq69qxowZ+stf/qK77rrr6N4hvmmEDUGarYWnEDaXb8GBoh9RRdrxFhqQflgpMyE5VVJD76xxDX3VjqieKi8r/SFIa6gyaOmKtJBg7/dfrav9K7cOV94wtTMqNLAf9z8EaW6df8GFUsEX3g0doCLN1x+tZ8+eZpPw1mKz2XTNNddozpw5evvtt380vyB6PB4tWrRIf//735WRkaFHHnlEVqtVCxYs0N69e7Vnzx4VFBQoPT1dN954Y6uOJSMjQ8nJycrPz9fy5ct1/vnnt+rznSxf9fHVV1/9o1kc4XSQmJiolStXHnV/cXGxPvjgA+3du1cej0e9e/fWtGnT/P5tuPTSS4957FtvvVVRUVGaMGGCWU1xMoFWc5199tlKSkpSQUGBFi1apEsvvVQWi0V1dXV+0y6LioqUl5eniooKpaSkKD09XXV1dfrPf/6jZ555xqyI8WkqSMvMzNTcuXM1cOBAvfDCCwoJCdH69euVlpamYcOGtXsjc7SO22+/Xdu2bdPmzZt14MABpaSkaMCAAdq1a5fWrl2rlStXHnWNHThwQAcOHNB///tfSd4FV/r27audO3dK0jErulvCnXfeqeXLl+vZZ581q+Ak72IGiYmJiomJkdPplMvlUnh4uKKioszp+IsXL9aSJUtUWVnpd8zk5GTdeuutCgsL01dffaWgoCANHjxYiYmJqqysVHZ2trZu3arg4GBdeeWVGjdunOx2u8LDwxUZGSmPx6O5c+fq22+/1aRJk3ThhRdq06ZNKi0tVXh4uIYOHdpqr8d1112nDz/8UG+//bbuvPNOrVq1SrGxserUqZPCwsJks9m0b98+7du3T6mpqerfv79WrVqlTz75RHFxcZowYYK6du2qyspK82Pv3r3auHGjNmzYoI0bN5r9ISXp5Zdf9nv+s846S99++60k77+bPr4+aafrggMEaQB+lMrLyzVv3jxJ3p48rc0X1F1//fX61a9+pV27dum///2vLr74Yv8dzSDNG9S0e0VahPevbyVVHTtIK6txKftQjaTAgjRfcBWb4P3Ff9euXTI0xBtfHhH6lJUeUpIkw+JdEEBSi/dIs3fQijTfip3RAfRHk34I0ipq3bpwzBjp7YclSaWlhxST+EMzbMMwdPDgQVVWVsrtdqtbt26KiPBfIKK6ulohISHNalSdk5OjxYsXq6CgQMOGDVNISIjeffdd7d69W+Hh4ZJadzW9w02ePFlz5szRggULVFZWpujoaLndbpWUlCg4OFghISEKCQkJKHQxDEP19fWqq6uTy+VSUFCQwsLCZLFYZBiGDMMwX6f6+nplZ2dr48aN2rFjh0pKSlRRUSGbzSaHw6EePXqod+/eqqysVG5urnbu3KkdO3bo4MGD2rdvn3JzcyVJCxYs0OrVq2Wz2bRw4UK/8TzwwAOtHhZYLBZddNFF+te//qVPP/20Qwdp1dXV5rTOq6++up1Hg0DEx8drxowZJ3WMmJgY3X777X73tVWIJnl/r7j88sv1yiuv6PLLL5fVajX/PYiIiFDXrl1VVFRkVsT79OjRQxUVFSosLDTHfP7556u+vl5JSUkaO3Zso8/3i1/8Quedd54GDRpknme7rnKONpGQkKA333yz0W35+fmaO3eu5s2bp/DwcP3sZz/TyJEjtXfvXq1atUqff/65li5dqkOHDum7776T5P2+be2VXK+66iqzh+CHH354UseyWq0KCgpSfn6+2R/UZ8GCBY0+5uOPP/a7nZGRIcMwtGnTJknSM888o549e6qmxvt761lnndWqf+C79NJLFRYWpl27dik5OVnGkYtqHccDDzxw3H1sNpvGjBmjjIwMrVy5Utu3b1dISIjy8/P1zTffSPKusHr4YhH9+vWT5P3DxumIIA1Ah+N7o3msN6h///vfVVlZqf79+7fpFITw8HBNnz5dzzzzjJ5//vmjgrSammqF6vCpne3boD0h3FuRVlzZsad27i3yrnKaHOVQdNjx38iENgRpoRFRZgNll8stb2zo/wtGeUPDaYvFIrenIUhr4a+Lo2E8zg5WkdbcqZ2+HmkVTrd69Ohp3j982DD99NafKzIyUnv37tWCBQvMhsM+Xbt2VefOnRUTE6Nt27Zp9+7dCg0NVf/+/dWtWzclJyfLZrOpurpasbGxGjhwoNLT0xUbG6sVK1bolVdeMaeNHEtbBWlnnnmmunXrpn379ik2NlZJSUkqKioyq1N9fIFaaGioQkNDFRISouTkZKWlpSk3N1dr165t9JdMq9Wq4OBgOZ1OWSwWxcTEKDg4WEVFRfJ4Tvz7KDo6WldddZXeeustLVq0SJK3+nbChAnatWuX0tPT26wHmC9IW7hwoWbOnHnCCxi0tvnz56u6ulrdu3dv1SlLwJFmzpyprKwss7LOp7KyUt9//70k78+uuLg4RUZGKjs7W7t375YkpaSk6KabbtIdd9wR0LVls9n4/oaf5ORkzZw586gVPVNTU5WZmak777xT9fX12rBhg/bv36/ExET17Nmz1asXg4ODtWTJEn3xxRd+99fV1amwsFClpaUKCQlRcHCwqqqqVF5ervLyctXU1GjUqFGaOHGi0tPTFRYWJrvdLrfbrXnz5un111+X3W7X+eefL6vVqnXr1qmiokKRkZFKSkpSv379lJ+fr7ffflsbN26U5P1DmO9zXwXrRx99ZPZslaRrr722VV+P8PBwXXXVVfrnP/8pwzDUr18/1dTUKDc3V3V13j+UJyQkqHv37tq/f78KCgqUkJCgSy65RCUlJVq8eLGqqqoUEhKiiIgIRUREKCUlRRkZGRo0aJAyMjI0ePDgRldGzs7O1kMPPaQlS5YcFURedtllqqioOOoPqacLgjQAHUJ9fb2effZZzZ8/X99++60mTJigd99916xCOZzb7dbs2bMlSbNmzWrzaTi33367nnnmGX388cdatmyZzjnnHHNbQX6+ukkKDff+UOkIiw1IUnEHr0jzVaOlxQa2smhIw9TOWrd3ddVNmzZpf3a2ekVKq1at1Mj0n5j7lpWVej+xWOWu975RCW7hirSQIN94OlhFmjm1s7kVaS7J8kPYWFiQrwcffNBvX5vNpoiICFksFpWWlmr//v1ms2ufmpoarVmzxm9qxrFYLBaNGDFCXbp00erVq3Xo0CFdeuml6tatm1566SWVlJRo5MiRAR3rZFksFj322GOaNWuWiouLlZ+f3+h+tbW1qq2tVWlpqXmf7w3wsXg8Hjmd3oDbMAy/RunBwcHq16+f+vXrp8TEREVFRckwDFVVVWn79u3avXu3oqKilJycrB49eqhPnz5KS0tTcnKyhgwZovDwcN1666265pprFBsbq9dff12DBg06uRfkBPh6U23atElpaWnq2bOn+vTpoy5duigiIkKVlZVau3atioqKzL44/fr1U8+ePRUVFaWwsDCFh4fLMAxt3rxZ69evV3FxsSorK5WUlKRevXrJbrerurpaNTU1qqmpkd1uV3x8vOLi4hQXF6f4+HiFhoYe9XOipKREs2fP1ptvvmm+IWJaJ9raoEGDtHv3brlcLhUWFspqtcrhcKiwsNAM8fv372/+LlRRUaGvvvpKwcHBuuCCC1p9mjtgs9k0dOjQVp262JhevXqpV69eLXKs4OBgXXvttQEHXvfdd5/5eX5+vpYtW6bCwkJdffXVio+P16FDh7Rs2TLFxcUpPT1dnTp1apFxHsucOXN0ySWXaNSoUerevbt5v8fjkcvl8utDWlJSoqioKPPfh/r6ehmGcUL/XqSlpem1115rdFtoaGij958u+NcXQIcwe/Zs/fKXvzRv/+c//9HYsWP1yCOPaP/+/UpNTdWYMWMUFhamefPmad++fUpMTNSUKVPafKy9evXStGnT9I9//EPXXXed1q1bp/j4eEnewKGbpKho72pGLT2FsLl+LD3SckqrJUmdYwL7oRxiVoDVa9CgQdq0aZMqq6qlSJseuP8+nfFNts4880zt3btXX819URdeIsliNad2tnTA2dEr0gKd2hnhaOiRVuv2C9KefeZpLVrytYKCghQTE6Nx48Zp/Pjx5pu7oqIibdu2Tfn5+SouLlZ6erqGDBmikpISbd68WQcPHjSDqNDQUOXn52vTpk3KyclRcXGxkpOTNX36dN14441KTExsdGz333+/tm3b1qaN4KdOnaobb7xRhYWFys7OVkpKijmtwhfc1NbWmp/X1NSourpaeXl5OnDggOLi4jR8+HB17txZdrtddrtdwcHBcrvdKi8vV11dnUJDQ+XxeFRaWiqn06nk5GQlJCSc9BvkkSNHavfu3e0aDKWmpuqVV17R3/72N61atUq7du3y+yv+4fbs2aO5c+c2us03BfZEORwOORwO1dTUyGq1Ki0tTfn5+WYPHZvNprPOOuuoqgygrQQHB/u9GY+NjVWfPn2O2i8yMlITJ05sy6EBp7Xk5GRdddVVfvfFxsYetwdjS4uJidE111xz1P2+8P1wR65UHGgfaTQPQRoCYhiGXC5Xi692crryeDzN6ht0qju8wuyXv/ylzjvvPE2bNk3ffvut39RJ3zSxAwcOSPJWhrXXX0Oee+45LV++XNu3b9fEiRPVq1cvlZeX65KQ7zTiDCkyOkZyd4CKNN+qnR18ameOWZHWvCCt1u3RU089pWHDhimt9m+S66CsFm8w6/ueGpJilRShsLBwuT2+irQWntpp9khruSBtzuIdWnegVC9cP0yh9hP7Jai8xi1JigoJ7Me9bwpoRa1b0g/fuz+9aZp+evvdTT4uISFBCQkJjd7f2JvBExEREdEu05IsFouSkpKUlJTkd39kZKTZ3Li5HA7HUdW2KSkpJzzGpnSE6qpbbrlFt9xyi4qLi7Vhwwbt2LFDeXl5qqqqUlBQkIYMGaL4+HitXLlS3333nXbs2KG9e/eqsrLSnOpmGIaio6M1bNgws7nywYMHvb0RDUNhYWHm1Fqn06ni4mKVlJSouLhYbrdbTqfTrP6TZIZ5gwcP1n333adJkyad8NcSAACcfgjScExffPGFrr76ahUVFUnyrs4xfvx49erVS9HR0UpMTFRqaqo6deqk+Pj4DvFLe0fm8Xh099136+WXX1ZsbKx69uypoUOHasSIEerbt6969Oih6Oho2e121dTUqLS0VHFxcQoJCWnvobeqefPmmb0XHn/8cYWEhOirr77SjBkzVFJSou7du2vz5s3av3+/OUUsPDxct912W7uNOTIyUu+9955Gjx7tt+LROWMdkhyKjYuXCqTg9u6R1lCRVlJVJ4/HkLWdg72m+KZ2dm5ukOaqV5cuXXTPPfdIL/9byj2o3zzyG7lf/1yGYSgqKko3jh0kFc1WaFi43FWts9jA4eNpCYZh6MUvdqnGVa8lWws0aVDqCR2nuVM7IxoCtxpXvdwGvySg5cTHx+uCCy7QBRdc0Oj2Cy+80O+2YRiqq6tTVVWVWanX3D9AGYahyspKlZSUyOl0KiwsTC6XS9nZ2bLZbMrMzOT3FgAA0Gz8joxjevDBB80QTfIub9vUErfBwcFKSkpSYmKi+f+IiAh5PB45HA4lJCSYfV58UzTi4+N14403nhYlp4Zh6I477tCLL74oScrLy1NeXp6+/vrro/Y9fBqLxWJRp06dFBsba05PCQkJUVxcnNLS0pSWlqYuXbooNDRUFRUV6t69u84666w2PbeT9fTTT0uSbrvtNjM0HDBggJYtW2bu42v2mZ2dLafTqQEDBrT7Uu2DBw/WwoULlZWVpeTkZMXExGhY0Xyp5iuldOosFbR/RVpcQ0Wa22OovNalmLCmq0oNw9Dc1dnqEhemM3vEt9UQJUk5pQ1BWsBTO71vqGvqDguuGqYiZo4epcU3HtbPK2eN9NfZDVM7vRUuLb7YQAtXpBVWOFXTEMp9vu3EgzRzsYFmTu2UpEpnvWJ8N4yONWUVpz6LxWL+zDuZYzRWOZienn6ywwMAAKcxgjQ0acWKFfr6668VHBys9evXKy4uTl999ZWWLFmigoIClZaWqqCgQLm5uSoqKpLL5VJOTo5ycnKa9TyVlZW64447WuksOo7f/va3evHFF2WxWPTXv/5VQ4YM0datW/Xdd99p7dq12rVrl7KzsyXJL0QzDKNZr6vFYtHq1avbvCnoiVq+fLlWrFghu91+1DL0h7NYLBo0aFC7NMw+losuushsqC1J+u9uaflX8sgbrLR3jzR7kFVRIUEqr3WrqLLumEHau6sO6L55GxXhCNKKB8Yo3NE2PyIMw2j+1M6gH6Z2mnw9vY4MfXy9lQ5btbPFFxtoCPYqGirATtbe4mrz8y+2FZ5wNWF5M1fttAdZ5Qiyyun2qMJZrxhZJBkEaQAAAEADgjQ06ZlnnpEkXX/99erfv78k6X//93/1v//7v0ftW1dXp/z8fOXn56uwsFAFBQUqLCxUVVWVbDabamtrVVRUpIqKClksFlksFuXl5emzzz7T73//e82YMeOU7r/m8Xj03HPPSfL21rr55pslScOHD/drlu9yuVRVVaWamhpFRkYqPDxcRUVF2rNnjyoqKlRbWyun02m+ntnZ2eZHTU2NiouLtWfPHj3xxBP64IMP2uVcm+svf/mLJOm6665r9wqzFtEQ2vhih/auSJO80zvLa90qrnSqV1LjS1QfKKnW4x95VxqsdLr1n425umZElyaPWef2qMrpVmz4yV+35TVuVTi9vbw6xwS6amdjUyl9r/WRTckNc7u73je1s2Ur0gakRkuSVu4pOeZ+h6rqdO0ry3VOr0Q9fOmAJvfbW1xlfl5U6dTmg+XKSItu9rjKa72va6CLDUhSZEiwnJVOb580i8X7PX0Sjd4BAACAUwlBGhq1b98+c/Wsu+9uusG0j91uV5cuXdSlS9NvvI9UW1urHj16KDs7W2+88YZ+9rOfnfB4O7rVq1eroKBAkZGRuuWWW5rcLzg4WDExMYqJiTHvS0xMbHIVuyNt3rxZGRkZmjdvnjZu3KiMjIyTHXqrKi4u1vvvvy9Jx6xG+1FpqNzxGN5QJ6gDBGnxEXbtLqpScVXjK3d6PIZ+NXeDqurqzWqk97870GSQZhiGfvr6Kq3aW6K3bhmt4d3iGt0vUNkNK3bGh9sDbqofavcGYX5BWpMVaR5zu8u32EALf11+0sfbaH/tgVKV17qarABbvLVA2/MrtauwSred31OJkY1PW9t3WJAmead3nkiQ9sPUzsB/3EeGBKmo0qlKZ8PKnYaHijQAAACgAcsGolHPPfec6uvrNWbMGA0ePLhVniMkJET33nuvJOnJJ5+U2+1ulefpCD766CNJ0rhx41q18u6MM84wl2j+7W9/22rP01LeeOMNOZ1ODRkyRCNHjmzv4bSMhsChXr4grf3/mY0P94Y1B0qqtWpvyVEN8ZftLNLy3cUKDbbpnzePltUirdp7SLsKKxs93uItBVq2s0hOt0e/mrvhpBvs5zRzoQHpsKmdzQjSDIvVLKxq6Yq0tNgw9UgMV73H0Dc7i5rcb/W+Q5Kkeo+hjzYcbHI/39TOnonelR0/31ZwQuNq7tROyRukSQ3TVJt6TQEAAIDTVPu/w0OHU15err/+9a+SpFmzZrXqc82YMUMJCQnavXu3/vnPf7bqc7UnX5B2ySWXtPpzPfigt8n6e++9Z65w2REZhqGXX35ZkvT//t//O3VWTjMr0jpGjzTJW5EmSU9+vFVXv7RcD8zf6Ld90cZcSdKVwzprVHqcLuibJEl6/7vso47l8Rj606fbzNu7Cqv0/JKdxx1DSVWd7n53nRkkHS67mf3RpMOndh7eI63htW4qSNMPX4vW+Lqc18dbObp0e2GT+6ze98PUzwXrmg7S9jcEaTed1V2StO5AqUqaqChsSq2r3lz8IDos8CDNt+BApdMtc7osQRoAAAAgiSANjXj11VdVXl6uvn37asKECa36XOHh4fr1r38tSbr//vtVVlbWqs/XHg4ePKg1a9bIYrHo4osvbvXnGzx4sCZPnizDMDR16lQ5nc5Wf87mWLx4sa688kqNHTtW27ZtU3h4uK6//vr2HlYL6ng90nok+vdFm782x6w2c9d79N/NeZKkSRnelSGvbpjSOW9Ntrnwhc/CDQe1Na9CkSFB+v1V3qnDLy7d1WT1ms9bK/Zp/tocPfXx0av+NnfFTklyBB9raucR/bwOW2zAJ7gVKgV/0hCkfbm96KjXTfJOs9ye732drBZp/YFS7SmqOmo/wzDMHmmje8SrX0qkDENavCX/uGPYVVipP3yyVf/z/DK9vXK/JO9pR9ibN7VTauiv5ntNj+o7BwAAAJye6JEGP263W7Nnz5bk7Y1mbYNpaXfeeaf++te/avv27Xr00Uf19NNPt9pzlZaWKicnR2VlZSovL1dZWZmcTqfi4+OVnJyslJQUJSUltej0y0WLFkmSRo0a1WbN9OfMmaPFixdr8+bNeuCBB3TbbbfJ5XKprq7uuP+vq6tTWVmZysrKlJaWpiFDhiguLk4ej8f8qKioUFGRd/pafHy8EhISlJCQILvdbr6mkZGRcjgcqq6uVlVVlaqqqvTPf/5TTz31lF/IMHXqVEVFRbXJ69ImfFM7O1CPtCmjuyop0qGeiRF6Omu7PtuSrxc+36mnrxmiFXtKdKjapbhwu0ale3udXdgvSXabVQUVTh0oqVHXeO8CAIZhaPbiHZKkW8/rqWtHdtW8NTlasadEy3cVq2di4wsZSNK6A96QfO3+UlU63WbVk3TY1M5mBGmhDRVpNc2Z2nnY345aoyLtzPR42YOsyimt0a7CqqMWdliz31uNl54Qrq5xYVq6vVAL1ubo7ov6+O13qNrlbfQvqWtcmC4emKqteRVatDHXDDkb8/HGXN325g9VqJtyvK95pCOoWSt+Rji81WuVhwdpVKQBAAAAkgjSTgm7du3SbbfdpoSEBMXHx6uurk4FBQWqra2VzWaTy+VSZWWl6uvrFRoaKo/Ho8LCQtXW1io2NlbR0dEKDg5WcHCwqqurtXfvXsXHx+vGG29sk/Hb7XbNnj1bF198sebMmaNdu3bJ7Xb7ffjGHhkZKcMw5HK5/AKgxm7X1/v3bSovL1dpaWlAY7JarXI4HLLb7bLb7ebnvv+7XC5VV1crKChIkZGRkqTq6mp5PB45HA7V19errKxMNTU1qq72TtFqi2mdPomJiXrllVd0xRVX6Omnn27VcPJE3HzzzTr77LNlt9t12WWXtfdwWtZRiw20f+FvSLBNlw7uJEm6c0wvfbYlX/9ed1B3XtjbnNY5/oxks2+YPciqfqmR2pBdpo05ZWaQtvZAqXYXVinMbtO0himHg9KitWJPyXEr0jbmlEqS3B5DK3YXa0z/H0Jl32IDabGBrdjpOyepmVM7LYcFaa0QcIbabRrVPU7Ldhbpv5vz1Cupl9/2NQ3TWod1jdW5vRO0dHuhFm44eFSQ5qtGS40OUUiwTRMzUvTMZ9u1bGeRympcTa7AuWBdTsPxY1RdV6+teRWSmjetU6JHGgAAAHAsBGmngOzsbGVlZZ3QY/fs2dPo/TNnzlRYWOBvak/WhAkTdNlll+nf//63Pvzww1Z9rri4OMXExCgqKkrR0dGy2+0qKipSfn6+CgoK5Ha75fF4VFNTo5qamhZ5TrvdrmuuuaZFjhWoyy+/3Kz2CwoKUnBwsOx2e0D/j46OVmRkpHbv3q3169erurpaVqtVVqtVFotFERER5kqixcXFZjDrExQU5Ld4hN1uV3h4uJKTk/Xoo4+2+WvRpowjpnZ2gB5phxuUFqML+ibq822Fmv76KrPv1oSBqX77DewcbQZpkwZ5ty1Y6w1qxp+RYlaU+arQdhUePUXRJ7+8VvnlP0wx/mpHkV+QdkKLDTQEaU6/hQ6aeq2Nw/7rnW7bWj35JmakatnOIs1evENn9YxXYYVTz3y2Q5cMStV3e71B2ojusTq/r/f62V1YZYZjhRVOGTLMFTu7NQSYvZMj1TspQjsKKvXZ9/m6anjaUc/rrvfom53FkqRHLj1DFot02QtfyzCat9CAJEWFHNYjzQwnmdoJAAAASARpp4S+ffvqjTfeUFFRkQoLC+VwOJScnKywsDDV19fLZrMpIiJCNptNtbW1slgsSkhIUGhoqA4dOqSysjK5XC653W65XC6Fhobq2muvbfPzeP311/XBBx/I4/EoKCjI78NqtaqmpkYVFRWyWCxm4HN4+HPkbZvN5vdmOTQ0VN26dVNERNPTzzwej8rKylRbWyun02lOdTzy8+DgYIWGhsrtdptjCg0NldVqldPplNVqVWxsrEJDQ2WxWJSYmKi4uLi2eBn9zJ4925yq29qqq6tVV1enyMhI2Ww28zULDQ1VUNBp9E9NB5zaeaRfTeinNft/6M8VHRqss3rG++2T0Tla0g+VZHVujxau9zbHv2JoZ3M/3/TFXQXeijSPx9Ch6jrFRzjMfdYf8B7DYvHmMV8ftqplldOtQ9XelSWbF6Q19EhzN39qZ2t+TSaP7KIvthXo0+/zdcPfVqiqzju+Lbnl5j7Du8UqJsyuTtEhOlhWq6255RrcJUYT53wlp6te485IkSR1jw83HzMxI1WzF+/Qoo25jQZp67PLVOF0KyYsWAM7R8tmtej6UV315or9SjjsaxGICLMijSANAAAAONJp9O721JWSktJm0zBbU0xMjG6++eZ2HYMvAEPzhYWF+VUxOhwOORzNewN/SmgIHHxBWkdYbOBI/VOjtOzXF+i977L10YaDunJYmoJt/lNQfUHappxyGYahL7cX6lC1S4mRDr/QzVeRllNao+o6t95Yvk9PfbxVf582wqw629jQq+ui/snK2pKvHQWVyiurVUp0iHY2BHBRIUHNqpzy9Uhz1Rty13u801KPF6Q1hEJHnmtLsloteubaIbrqxW/MqZVj+yfrs4aFAqJCgtSr4TUb0ClKB8tqtSW3XEE2iworvFV7c1d7V0v1TamVpEmDvEHaVzuKVF7rOuq1+mqHd6XQs3smmN9z90/sr/hwu8YOaF5vxsiGY1fQIw0AAAA4Svs37wGAU8mPoCJN8oYlN5+Trvm3n60bz+x21PY+yZGy26wqq3HpQEmN5jdM67xscCezl5okxYbbFRfuXZxjd2GVOf3zi22F5j7rs71B2rl9EjWoIaBbtrNIHo+h3y7yruJ5Zg//irjj8U3tlKRad0PI02SPtIapnb6vSStPtw13BOnVm0ZqWmY3/fPmUfrbtBF65trBsgdZNTEj1Wz8PyDVu8jG97nlWrOv9KjjHF6R1ic5Ur2SIlRX79G/1x08at+vdnir/M7tnWDeF+EI0qxxfTUoLaZZ4/dN262odcmcLkuQBgAAAEiiIg0AWph/j7SgVqx+ak32IKv6pkRqY06Z/rMxV59+nydJuvywaZ0+PRPDVVJVpzX7D2lbvrcKa3vD/w3D0IbsUknS4LRo5ZbWaH12mf721W7tLKjUyj0lCrPb9NAlA5o1PkfQD69rraveG/6Y1VNHTEP0BWmWtlsAolNMqB69bKB5+4qhaRp/RopZSSd5KwMlaUtuhblKZ4/EcO0u9O+R5nPjmd30yIeb9fLSXbpuZBfze6usxqV1DdNnzzksSDtRkX490nyvFVM7AQAAAImKNABoWT+SirRADGyoHvvzp9vkqjd0bu8E877D+fqkvbViv5lh+aZsZh+qUWm1S3abN5i7cliaQoNt2ppXoZeW7pIk/XJcX3WJa97iJhaLxQzTan0LDgTYIy24nRaACLMH+fVtHNDJG6Rty6/QqoaFCH57eYYmZqQos0e8+iRH+j3+2pFdlBBhV/ahGn24/oeqtC+3F6reY6hHYnizVj5tSqRfjzSmdgIAAACHoyINAFrSEUFaR+yRFqiMztF6W5LbY8hqkf5vUv9G9/P1SfP1BJOk4qo6FVc6tb6hGq1faqQcQTb1SopQ1qyf6KmPt+qjDbkalR6naWd1P6Hxhdptcro9AQdpHrXN1M5AdYkNU7jdpqq6ehVVOhVktWho1xhl9hze6P4hwTb99Jx0/eGTbfrLF7sUG27X51sL9NaK/ZKk8/oktsi4fD3SKp1uKZQgDQAAADgcQRoAtKSGkixPQ2XWj7kibVDaD9Vn147son4pUY3u5wvSjrSjoFKr93krrQYf1qcrLTZMz18/TA9OqlVsePAJh40hQTZJLtW6fCGP7zhHTkNsmNrZsD24DaZ2BsJqtah/apS+a3iNzugU5df7rTE3nNlNL36xSzsLKjX9tVXm/ef3TdSdF/ZukXEd3iPNCLN4XzVW7QQAAAAkEaQBQMtqqNxxm43tO0ZocyL6JEcqJSpETne97r6oT5P7HRmkDewcpU055dpRUKlvdhZLkjJ7Hr2YQEp0yEmNLyT4yKmdTS024Jva2bEq0iTv9E5fkDas2/FXDI4KCdavJ/TTc0t2KC7coa5xoZqW2V1n9Tr53mg+vqmdrnpDLDYAAAAA+CNIA4CW5JtGeAr0SLMHWfXxL86VxzAUH+Focr/OsaGyB1lV5/YoPSFcZ/dK0Kaccn27q9hcfKC5q3IGwle9ZVakHXdqp3d7Wyw2ECjfggOSNKzr8YM0yVuVdkMjK622lHB7kCwWbxGaR1bZJII0AAAAoEHHeTcBAKcCX0Vaw80fc480SYoNtx8zRJO859gjIVySNDo9Tr2TvE3yP9nsXemzf2qU4sLtLT42X5BWc1SPtCNX7fSvSGuvxQYaM+CwIG14ABVpbcFqtSjC7v07m8esSGNqJwAAACBRkQYALezU6ZHWHGf1TNDWvApNGJhihmb1DS/C2Y1M62wJYXZvkFZS5fTeYWki9PH1rWu42ZGm2/ZPjdKo7nGKC7erU0xoew/HFBUarAqnW/WG5F16gCANAAAAkAjSAKBlNYQ2bo9v1c6OE9q0pl9N6Kupmd3UPSFcVU6337azerVOkDaie5y+2VWshetzde3Irsed2lnbMKyY0OBWGc+JsAdZ9d6tme09jKOM6B6rnHU1qnZ5FCIxtRMAAABocHq8wwOAttIQONQbHW8aYWsKCbape8P0znBHkNJivdVVNqtFo9JbJ0i7eniaJGnZziIdKKk+RpDmDTdLa7xJ2ugeca0ynlPJZUM6SZIqnA2vJUEaAAAAIIkgDQBaVkNoU98wE+7H3iPtRPVO8q7kOTgtWhGO1il+7hIXprMbqt3mrs5WkytMNtwuawjSzurZcitcnqrO7Z2o2LBguX0vJUEaAAAAIIkgDQBaVkPgkFvu7dsVFdJxphG2pZHp3qqvcWektOrzXDOiiyRvkObx9Ug7qp+X97bLI0WHBvutlInGBdusmpiRai7QwGIDAAAAgFebBGkvvPCCunfvrpCQEI0ePVorV6485v7vv/+++vXrp5CQEGVkZGjRokV+2w3D0MMPP6zU1FSFhoZq7Nix2rFjR2ueAgAExgzS6hRmt+miM5LbeUDt42fn9NDbt5ypW87t0arPM/6MFEWFBCmntEa5ZXXeO5uoSPPIojN7xJ22VYLNddmQzuaqnXVu93H2BgAAAE4PrR6kvfvuu5o1a5YeeeQRrVmzRoMHD9b48eNVUFDQ6P7ffPONrrvuOt18881au3atLr/8cl1++eXatGmTuc8f/vAHzZkzRy+99JJWrFih8PBwjR8/XrW1ta19OgBwbL7QxrDo8qGdT9uKNHuQVZk941s9tAoJtmnyqK6SpO/2l3rvbDJIszKtsxlGdIuVzeZdGfXfaw+082gAAACAjqHVg7Snn35at9xyi6ZPn64BAwbopZdeUlhYmF599dVG9589e7YmTJige++9V/3799fjjz+uYcOG6fnnn5fkrUZ79tln9eCDD+qyyy7ToEGD9MYbb+jgwYNasGBBa58OAByT010vSTJk0Y1ndmvn0ZweZl3URwNSo1Tr8k4/rK2r99vuqvd9TaSzerbOwgenIqvVooTIEEnSgrU5+nJ7YTuPCAAAAGh/rdMBukFdXZ1Wr16t+++/37zParVq7NixWr58eaOPWb58uWbNmuV33/jx482QbM+ePcrLy9PYsWPN7dHR0Ro9erSWL1+uyZMnH3VMp9Mpp9Np3i4vLz+Z00IjCnL2qPj169t7GEC76+LaJ4ek7okR9OJqIyHBNr14wzCtes77t6HipS+q9Ou5sjT0TIuuL1FnScFBQerVsAgCAhMd6pDKpUeC/qHKtz7QlmBbew8JAAAAksq6jdeZN/ymvYdxWmrVIK2oqEj19fVKTvbvEZScnKytW7c2+pi8vLxG98/LyzO3++5rap8jPfnkk3r00UdP6BwQGJezWv1d37f3MIAO48zBGe09hNNKt/hwWQYNkjYsUWdLkTrXFx21jz0uzQzXEKDozlL+RvWx5nhvu9p3OAAAAPBaUdqvvYdw2mrVIK2juP/++/2q3MrLy9WlS5d2HNGpJyaxs9ZmzmnvYQAdgj2mk0aNHNPewzjtdP2f/5MyzlVpeZn2FVdJkqwWbyW0LdiuYSMvbucR/ghd+Yq0d5nqXC5tyS2Xh9U7AQAAOoSE1N7tPYTTVqsGaQkJCbLZbMrPz/e7Pz8/XykpKY0+JiUl5Zj7+/6fn5+v1NRUv32GDBnS6DEdDoccDseJngYCEB4Zo6Hjp7X3MACczoLsUu+xipEU085DOWWEREv9JskuiSJLAAAAoJUXG7Db7Ro+fLgWL15s3ufxeLR48WJlZmY2+pjMzEy//SUpKyvL3D89PV0pKSl++5SXl2vFihVNHhMAAAAAAAA4Wa0+tXPWrFmaNm2aRowYoVGjRunZZ59VVVWVpk+fLkmaOnWqOnfurCeffFKS9Itf/ELnnXee/vznP2vSpEl655139N133+mVV16RJFksFt1111164okn1Lt3b6Wnp+uhhx5Sp06ddPnll7f26QAAAAAAAOA01epB2rXXXqvCwkI9/PDDysvL05AhQ/TJJ5+YiwXs379fVusPhXFnnXWW3nrrLT344IN64IEH1Lt3by1YsEADBw409/nVr36lqqoqzZgxQ6WlpTrnnHP0ySefKCQkpLVPBwAAAAAAAKcpi2Gcfp2Dy8vLFR0drbKyMkVFRbX3cAAAAAAAANBOmpMTtWqPNAAAAAAAAOBUQZAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABKDVgrSSkhJNmTJFUVFRiomJ0c0336zKyspj7v/zn/9cffv2VWhoqLp27ao777xTZWVlfvtZLJajPt55553WOg0AAAAAAABAkhTUWgeeMmWKcnNzlZWVJZfLpenTp2vGjBl66623Gt3/4MGDOnjwoP70pz9pwIAB2rdvn2699VYdPHhQc+fO9dv3tdde04QJE8zbMTExrXUaAAAAAAAAgCTJYhiG0dIH3bJliwYMGKBVq1ZpxIgRkqRPPvlEEydOVHZ2tjp16hTQcd5//33dcMMNqqqqUlCQN/OzWCyaP3++Lr/88hMeX3l5uaKjo1VWVqaoqKgTPg4AAAAAAAB+3JqTE7XK1M7ly5crJibGDNEkaezYsbJarVqxYkXAx/GdgC9E85k5c6YSEhI0atQovfrqqzpeFuh0OlVeXu73AQAAAAAAADRHq0ztzMvLU1JSkv8TBQUpLi5OeXl5AR2jqKhIjz/+uGbMmOF3/2OPPaYLL7xQYWFh+vTTT3X77bersrJSd955Z5PHevLJJ/Xoo482/0QAAAAAAACABs2qSLvvvvsabfZ/+MfWrVtPelDl5eWaNGmSBgwYoN/85jd+2x566CGdffbZGjp0qH7961/rV7/6lf74xz8e83j333+/ysrKzI8DBw6c9BgBAAAAAABwemlWRdo999yjm2666Zj79OjRQykpKSooKPC73+12q6SkRCkpKcd8fEVFhSZMmKDIyEjNnz9fwcHBx9x/9OjRevzxx+V0OuVwOBrdx+FwNLkNAAAAAAAACESzgrTExEQlJiYed7/MzEyVlpZq9erVGj58uCRpyZIl8ng8Gj16dJOPKy8v1/jx4+VwOPThhx8qJCTkuM+1bt06xcbGEpQBAAAAAACgVbVKj7T+/ftrwoQJuuWWW/TSSy/J5XLpjjvu0OTJk80VO3NycjRmzBi98cYbGjVqlMrLyzVu3DhVV1frX//6l9+iAImJibLZbFq4cKHy8/N15plnKiQkRFlZWfrd736nX/7yl61xGgAAAAAAAICpVYI0SXrzzTd1xx13aMyYMbJarbrqqqs0Z84cc7vL5dK2bdtUXV0tSVqzZo25omevXr38jrVnzx51795dwcHBeuGFF3T33XfLMAz16tVLTz/9tG655ZbWOg0AAAAAAABAkmQxDMNo70G0tfLyckVHR6usrExRUVHtPRwAAAAAAAC0k+bkRM1atRMAAAAAAAA4XRGkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAFotSCtpKREU6ZMUVRUlGJiYnTzzTersrLymI85//zzZbFY/D5uvfVWv33279+vSZMmKSwsTElJSbr33nvldrtb6zQAAAAAAAAASVJQax14ypQpys3NVVZWllwul6ZPn64ZM2borbfeOubjbrnlFj322GPm7bCwMPPz+vp6TZo0SSkpKfrmm2+Um5urqVOnKjg4WL/73e9a61QAAAAAAAAAWQzDMFr6oFu2bNGAAQO0atUqjRgxQpL0ySefaOLEicrOzlanTp0afdz555+vIUOG6Nlnn210+8cff6xLLrlEBw8eVHJysiTppZde0q9//WsVFhbKbrcHNL7y8nJFR0errKxMUVFRzT9BAAAAAAAAnBKakxO1SkXa8uXLFRMTY4ZokjR27FhZrVatWLFCV1xxRZOPffPNN/Wvf/1LKSkpuvTSS/XQQw+ZVWnLly9XRkaGGaJJ0vjx43Xbbbdp8+bNGjp0aKPHdDqdcjqd5u2ysjJJ3hcKAAAAAAAApy9fPhRIrVmrBGl5eXlKSkryf6KgIMXFxSkvL6/Jx11//fXq1q2bOnXqpA0bNujXv/61tm3bpnnz5pnHPTxEk2TePtZxn3zyST366KNH3d+lS5eAzwkAAAAAAACnroqKCkVHRx9zn2YFaffdd59+//vfH3OfLVu2NOeQfmbMmGF+npGRodTUVI0ZM0a7du1Sz549T/i4999/v2bNmmXe9ng8KikpUXx8vCwWywkfF/7Ky8vVpUsXHThwgCmzQDNw7QAnhmsHaD6uG+DEcO0AJ+bHcu0YhqGKioomW5EdrllB2j333KObbrrpmPv06NFDKSkpKigo8Lvf7XarpKREKSkpAT/f6NGjJUk7d+5Uz549lZKSopUrV/rtk5+fL0nHPK7D4ZDD4fC7LyYmJuBxoHmioqI69AUCdFRcO8CJ4doBmo/rBjgxXDvAifkxXDvHq0TzaVaQlpiYqMTExOPul5mZqdLSUq1evVrDhw+XJC1ZskQej8cMxwKxbt06SVJqaqp53N/+9rcqKCgwp45mZWUpKipKAwYMaM6pAAAAAAAAAM1ibY2D9u/fXxMmTNAtt9yilStX6uuvv9Ydd9yhyZMnm2VyOTk56tevn1lhtmvXLj3++ONavXq19u7dqw8//FBTp07VT37yEw0aNEiSNG7cOA0YMEA33nij1q9fr//+97968MEHNXPmzKMqzgAAAAAAAICW1CpBmuRdfbNfv34aM2aMJk6cqHPOOUevvPKKud3lcmnbtm2qrq6WJNntdn322WcaN26c+vXrp3vuuUdXXXWVFi5caD7GZrPpo48+ks1mU2Zmpm644QZNnTpVjz32WGudBprB4XDokUceIdQEmolrBzgxXDtA83HdACeGawc4MafitWMxAlnbEwAAAAAAADjNtVpFGgAAAAAAAHAqIUgDAAAAAAAAAkCQBgAAAAAAAASAIA0AAAAAAAAIAEEaWsQLL7yg7t27KyQkRKNHj9bKlSvbe0hAu/ryyy916aWXqlOnTrJYLFqwYIHfdsMw9PDDDys1NVWhoaEaO3asduzY4bdPSUmJpkyZoqioKMXExOjmm29WZWVlG54F0LaefPJJjRw5UpGRkUpKStLll1+ubdu2+e1TW1urmTNnKj4+XhEREbrqqquUn5/vt8/+/fs1adIkhYWFKSkpSffee6/cbndbngrQpl588UUNGjRIUVFRioqKUmZmpj7++GNzO9cNEJinnnpKFotFd911l3kf1w9wtN/85jeyWCx+H/369TO3n+rXDUEaTtq7776rWbNm6ZFHHtGaNWs0ePBgjR8/XgUFBe09NKDdVFVVafDgwXrhhRca3f6HP/xBc+bM0UsvvaQVK1YoPDxc48ePV21trbnPlClTtHnzZmVlZemjjz7Sl19+qRkzZrTVKQBtbunSpZo5c6a+/fZbZWVlyeVyady4caqqqjL3ufvuu7Vw4UK9//77Wrp0qQ4ePKgrr7zS3F5fX69Jkyaprq5O33zzjf7xj3/o9ddf18MPP9wepwS0ibS0ND311FNavXq1vvvuO1144YW67LLLtHnzZklcN0AgVq1apZdfflmDBg3yu5/rB2jcGWecodzcXPNj2bJl5rZT/roxgJM0atQoY+bMmebt+vp6o1OnTsaTTz7ZjqMCOg5Jxvz5883bHo/HSElJMf74xz+a95WWlhoOh8N4++23DcMwjO+//96QZKxatcrc5+OPPzYsFouRk5PTZmMH2lNBQYEhyVi6dKlhGN7rJDg42Hj//ffNfbZs2WJIMpYvX24YhmEsWrTIsFqtRl5enrnPiy++aERFRRlOp7NtTwBoR7Gxscbf/vY3rhsgABUVFUbv3r2NrKws47zzzjN+8YtfGIbBzx2gKY888ogxePDgRredDtcNFWk4KXV1dVq9erXGjh1r3me1WjV27FgtX768HUcGdFx79uxRXl6e33UTHR2t0aNHm9fN8uXLFRMToxEjRpj7jB07VlarVStWrGjzMQPtoaysTJIUFxcnSVq9erVcLpfftdOvXz917drV79rJyMhQcnKyuc/48eNVXl5uVucAp7L6+nq98847qqqqUmZmJtcNEICZM2dq0qRJfteJxM8d4Fh27NihTp06qUePHpoyZYr2798v6fS4boLaewD4cSsqKlJ9fb3fBSBJycnJ2rp1azuNCujY8vLyJKnR68a3LS8vT0lJSX7bg4KCFBcXZ+4DnMo8Ho/uuusunX322Ro4cKAk73Vht9sVExPjt++R105j15ZvG3Cq2rhxozIzM1VbW6uIiAjNnz9fAwYM0Lp167hugGN45513tGbNGq1ateqobfzcARo3evRovf766+rbt69yc3P16KOP6txzz9WmTZtOi+uGIA0AAHQ4M2fO1KZNm/z6bQBoWt++fbVu3TqVlZVp7ty5mjZtmpYuXdrewwI6tAMHDugXv/iFsrKyFBIS0t7DAX40Lr74YvPzQYMGafTo0erWrZvee+89hYaGtuPI2gZTO3FSEhISZLPZjlqBIz8/XykpKe00KqBj810bx7puUlJSjlqww+12q6SkhGsLp7w77rhDH330kT7//HOlpaWZ96ekpKiurk6lpaV++x957TR2bfm2Aacqu92uXr16afjw4XryySc1ePBgzZ49m+sGOIbVq1eroKBAw4YNU1BQkIKCgrR06VLNmTNHQUFBSk5O5voBAhATE6M+ffpo586dp8XPHYI0nBS73a7hw4dr8eLF5n0ej0eLFy9WZmZmO44M6LjS09OVkpLid92Ul5drxYoV5nWTmZmp0tJSrV692txnyZIl8ng8Gj16dJuPGWgLhmHojjvu0Pz587VkyRKlp6f7bR8+fLiCg4P9rp1t27Zp//79ftfOxo0b/YLorKwsRUVFacCAAW1zIkAH4PF45HQ6uW6AYxgzZow2btyodevWmR8jRozQlClTzM+5foDjq6ys1K5du5Samnp6/Nxp79UO8OP3zjvvGA6Hw3j99deN77//3pgxY4YRExPjtwIHcLqpqKgw1q5da6xdu9aQZDz99NPG2rVrjX379hmGYRhPPfWUERMTY/z73/82NmzYYFx22WVGenq6UVNTYx5jwoQJxtChQ40VK1YYy5YtM3r37m1cd9117XVKQKu77bbbjOjoaOOLL74wcnNzzY/q6mpzn1tvvdXo2rWrsWTJEuO7774zMjMzjczMTHO72+02Bg4caIwbN85Yt26d8cknnxiJiYnG/fff3x6nBLSJ++67z1i6dKmxZ88eY8OGDcZ9991nWCwW49NPPzUMg+sGaI7DV+00DK4foDH33HOP8cUXXxh79uwxvv76a2Ps2LFGQkKCUVBQYBjGqX/dEKShRTz33HNG165dDbvdbowaNcr49ttv23tIQLv6/PPPDUlHfUybNs0wDMPweDzGQw89ZCQnJxsOh8MYM2aMsW3bNr9jFBcXG9ddd50RERFhREVFGdOnTzcqKira4WyAttHYNSPJeO2118x9ampqjNtvv92IjY01wsLCjCuuuMLIzc31O87evXuNiy++2AgNDTUSEhKMe+65x3C5XG18NkDb+elPf2p069bNsNvtRmJiojFmzBgzRDMMrhugOY4M0rh+gKNde+21RmpqqmG3243OnTsb1157rbFz505z+6l+3VgMwzDapxYOAAAAAAAA+PGgRxoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAABCkAQAAAAAAAAEgSAMAAAAAAAACQJAGAAAAAAAABIAgDQAAAAAAAAgAQRoAAAAAAAAQAII0AAAAAAAAIAAEaQAAAAAAAEAACNIAAAAAAACAAPx/mPUKAAdULAgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Assuming dataiter is a DataLoader object\n",
    "break_index = None\n",
    "for idx, (x, y, s) in enumerate(dataiter):\n",
    "    if y[0, :, 2].sum() > 0:\n",
    "        break_index = idx\n",
    "        break\n",
    "    if s[0, :, 2].sum() > 0:\n",
    "        break_index = idx\n",
    "        break\n",
    "\n",
    "print(\"Break index:\", break_index)\n",
    "\n",
    "plt.plot(np.arange(-BORDER, SEQ_LEN + BORDER), x[0, :].detach().numpy(), 'k-')\n",
    "plt.plot(y[0, :, 2].detach().numpy())\n",
    "plt.plot(s[0, :, 2].detach().numpy())\n",
    "plt.ylim([-0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/300] train_loss: 0.67188 valid_loss: 0.59806 test_loss: 0.60248 \n",
      "Validation loss decreased (inf --> 0.598058).  Saving model ...\n",
      "[  2/300] train_loss: 0.52692 valid_loss: 0.45704 test_loss: 0.46297 \n",
      "Validation loss decreased (0.598058 --> 0.457042).  Saving model ...\n",
      "[  3/300] train_loss: 0.39110 valid_loss: 0.36267 test_loss: 0.37084 \n",
      "Validation loss decreased (0.457042 --> 0.362668).  Saving model ...\n",
      "[  4/300] train_loss: 0.30494 valid_loss: 0.29602 test_loss: 0.30830 \n",
      "Validation loss decreased (0.362668 --> 0.296016).  Saving model ...\n",
      "[  5/300] train_loss: 0.25450 valid_loss: 0.25815 test_loss: 0.27125 \n",
      "Validation loss decreased (0.296016 --> 0.258150).  Saving model ...\n",
      "[  6/300] train_loss: 0.22809 valid_loss: 0.23795 test_loss: 0.25236 \n",
      "Validation loss decreased (0.258150 --> 0.237950).  Saving model ...\n",
      "[  7/300] train_loss: 0.21048 valid_loss: 0.21962 test_loss: 0.23259 \n",
      "Validation loss decreased (0.237950 --> 0.219623).  Saving model ...\n",
      "[  8/300] train_loss: 0.20139 valid_loss: 0.20542 test_loss: 0.21756 \n",
      "Validation loss decreased (0.219623 --> 0.205418).  Saving model ...\n",
      "[  9/300] train_loss: 0.19049 valid_loss: 0.19327 test_loss: 0.20656 \n",
      "Validation loss decreased (0.205418 --> 0.193273).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17863 valid_loss: 0.18636 test_loss: 0.20110 \n",
      "Validation loss decreased (0.193273 --> 0.186364).  Saving model ...\n",
      "[ 11/300] train_loss: 0.17719 valid_loss: 0.17642 test_loss: 0.19075 \n",
      "Validation loss decreased (0.186364 --> 0.176421).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16784 valid_loss: 0.17291 test_loss: 0.18779 \n",
      "Validation loss decreased (0.176421 --> 0.172907).  Saving model ...\n",
      "[ 13/300] train_loss: 0.16298 valid_loss: 0.16850 test_loss: 0.18436 \n",
      "Validation loss decreased (0.172907 --> 0.168502).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15819 valid_loss: 0.16357 test_loss: 0.17473 \n",
      "Validation loss decreased (0.168502 --> 0.163567).  Saving model ...\n",
      "[ 15/300] train_loss: 0.15360 valid_loss: 0.16154 test_loss: 0.17150 \n",
      "Validation loss decreased (0.163567 --> 0.161539).  Saving model ...\n",
      "[ 16/300] train_loss: 0.15198 valid_loss: 0.15805 test_loss: 0.16763 \n",
      "Validation loss decreased (0.161539 --> 0.158046).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14673 valid_loss: 0.15104 test_loss: 0.16160 \n",
      "Validation loss decreased (0.158046 --> 0.151044).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14329 valid_loss: 0.15266 test_loss: 0.16033 \n",
      "[ 19/300] train_loss: 0.14210 valid_loss: 0.15110 test_loss: 0.16331 \n",
      "[ 20/300] train_loss: 0.14345 valid_loss: 0.15279 test_loss: 0.15889 \n",
      "[ 21/300] train_loss: 0.14079 valid_loss: 0.15247 test_loss: 0.15886 \n",
      "[ 22/300] train_loss: 0.13445 valid_loss: 0.14873 test_loss: 0.15721 \n",
      "Validation loss decreased (0.151044 --> 0.148730).  Saving model ...\n",
      "[ 23/300] train_loss: 0.13462 valid_loss: 0.15081 test_loss: 0.15748 \n",
      "[ 24/300] train_loss: 0.13613 valid_loss: 0.14525 test_loss: 0.15478 \n",
      "Validation loss decreased (0.148730 --> 0.145251).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12698 valid_loss: 0.14726 test_loss: 0.15327 \n",
      "[ 26/300] train_loss: 0.12747 valid_loss: 0.14278 test_loss: 0.15079 \n",
      "Validation loss decreased (0.145251 --> 0.142778).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12818 valid_loss: 0.15113 test_loss: 0.15767 \n",
      "[ 28/300] train_loss: 0.12839 valid_loss: 0.14355 test_loss: 0.15140 \n",
      "[ 29/300] train_loss: 0.12615 valid_loss: 0.14352 test_loss: 0.15290 \n",
      "[ 30/300] train_loss: 0.12283 valid_loss: 0.13900 test_loss: 0.14631 \n",
      "Validation loss decreased (0.142778 --> 0.138996).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11945 valid_loss: 0.14017 test_loss: 0.15001 \n",
      "[ 32/300] train_loss: 0.12137 valid_loss: 0.14132 test_loss: 0.15161 \n",
      "[ 33/300] train_loss: 0.12027 valid_loss: 0.13457 test_loss: 0.14550 \n",
      "Validation loss decreased (0.138996 --> 0.134573).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11750 valid_loss: 0.13272 test_loss: 0.14210 \n",
      "Validation loss decreased (0.134573 --> 0.132716).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11561 valid_loss: 0.13444 test_loss: 0.14559 \n",
      "[ 36/300] train_loss: 0.11448 valid_loss: 0.12893 test_loss: 0.14055 \n",
      "Validation loss decreased (0.132716 --> 0.128933).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11534 valid_loss: 0.13295 test_loss: 0.14260 \n",
      "[ 38/300] train_loss: 0.11452 valid_loss: 0.13161 test_loss: 0.14213 \n",
      "[ 39/300] train_loss: 0.11442 valid_loss: 0.12526 test_loss: 0.13799 \n",
      "Validation loss decreased (0.128933 --> 0.125256).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11302 valid_loss: 0.12415 test_loss: 0.13727 \n",
      "Validation loss decreased (0.125256 --> 0.124146).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11296 valid_loss: 0.12477 test_loss: 0.13831 \n",
      "[ 42/300] train_loss: 0.11243 valid_loss: 0.12137 test_loss: 0.13522 \n",
      "Validation loss decreased (0.124146 --> 0.121371).  Saving model ...\n",
      "[ 43/300] train_loss: 0.11017 valid_loss: 0.12272 test_loss: 0.13689 \n",
      "[ 44/300] train_loss: 0.10952 valid_loss: 0.12722 test_loss: 0.14451 \n",
      "[ 45/300] train_loss: 0.10925 valid_loss: 0.12256 test_loss: 0.13694 \n",
      "[ 46/300] train_loss: 0.11160 valid_loss: 0.12422 test_loss: 0.13770 \n",
      "[ 47/300] train_loss: 0.10841 valid_loss: 0.11940 test_loss: 0.13315 \n",
      "Validation loss decreased (0.121371 --> 0.119396).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10796 valid_loss: 0.11935 test_loss: 0.13565 \n",
      "Validation loss decreased (0.119396 --> 0.119348).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10684 valid_loss: 0.11940 test_loss: 0.13579 \n",
      "[ 50/300] train_loss: 0.10696 valid_loss: 0.11465 test_loss: 0.12919 \n",
      "Validation loss decreased (0.119348 --> 0.114651).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10637 valid_loss: 0.11857 test_loss: 0.13134 \n",
      "[ 52/300] train_loss: 0.10260 valid_loss: 0.11867 test_loss: 0.13170 \n",
      "[ 53/300] train_loss: 0.10195 valid_loss: 0.11352 test_loss: 0.12753 \n",
      "Validation loss decreased (0.114651 --> 0.113519).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10173 valid_loss: 0.11609 test_loss: 0.12955 \n",
      "[ 55/300] train_loss: 0.10361 valid_loss: 0.11644 test_loss: 0.12928 \n",
      "[ 56/300] train_loss: 0.10230 valid_loss: 0.11582 test_loss: 0.12920 \n",
      "[ 57/300] train_loss: 0.10377 valid_loss: 0.11123 test_loss: 0.12682 \n",
      "Validation loss decreased (0.113519 --> 0.111232).  Saving model ...\n",
      "[ 58/300] train_loss: 0.10106 valid_loss: 0.11197 test_loss: 0.12620 \n",
      "[ 59/300] train_loss: 0.10538 valid_loss: 0.11183 test_loss: 0.12618 \n",
      "[ 60/300] train_loss: 0.10299 valid_loss: 0.11373 test_loss: 0.13067 \n",
      "[ 61/300] train_loss: 0.09852 valid_loss: 0.10842 test_loss: 0.12463 \n",
      "Validation loss decreased (0.111232 --> 0.108423).  Saving model ...\n",
      "[ 62/300] train_loss: 0.10095 valid_loss: 0.11269 test_loss: 0.12658 \n",
      "[ 63/300] train_loss: 0.10304 valid_loss: 0.10892 test_loss: 0.12225 \n",
      "[ 64/300] train_loss: 0.09908 valid_loss: 0.10829 test_loss: 0.12112 \n",
      "Validation loss decreased (0.108423 --> 0.108289).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09637 valid_loss: 0.10632 test_loss: 0.12182 \n",
      "Validation loss decreased (0.108289 --> 0.106318).  Saving model ...\n",
      "[ 66/300] train_loss: 0.10002 valid_loss: 0.11000 test_loss: 0.12363 \n",
      "[ 67/300] train_loss: 0.09743 valid_loss: 0.10760 test_loss: 0.12306 \n",
      "[ 68/300] train_loss: 0.09877 valid_loss: 0.10530 test_loss: 0.12146 \n",
      "Validation loss decreased (0.106318 --> 0.105301).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09644 valid_loss: 0.10577 test_loss: 0.12273 \n",
      "[ 70/300] train_loss: 0.09587 valid_loss: 0.10715 test_loss: 0.12194 \n",
      "[ 71/300] train_loss: 0.09605 valid_loss: 0.10734 test_loss: 0.12184 \n",
      "[ 72/300] train_loss: 0.09433 valid_loss: 0.10504 test_loss: 0.12112 \n",
      "Validation loss decreased (0.105301 --> 0.105040).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09874 valid_loss: 0.10914 test_loss: 0.12299 \n",
      "[ 74/300] train_loss: 0.09378 valid_loss: 0.10546 test_loss: 0.12045 \n",
      "[ 75/300] train_loss: 0.09552 valid_loss: 0.10296 test_loss: 0.11830 \n",
      "Validation loss decreased (0.105040 --> 0.102964).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09589 valid_loss: 0.10505 test_loss: 0.11894 \n",
      "[ 77/300] train_loss: 0.09401 valid_loss: 0.10375 test_loss: 0.11865 \n",
      "[ 78/300] train_loss: 0.09295 valid_loss: 0.10422 test_loss: 0.11879 \n",
      "[ 79/300] train_loss: 0.09324 valid_loss: 0.10613 test_loss: 0.11983 \n",
      "[ 80/300] train_loss: 0.09439 valid_loss: 0.10321 test_loss: 0.11813 \n",
      "[ 81/300] train_loss: 0.08985 valid_loss: 0.10203 test_loss: 0.11624 \n",
      "Validation loss decreased (0.102964 --> 0.102025).  Saving model ...\n",
      "[ 82/300] train_loss: 0.09281 valid_loss: 0.10331 test_loss: 0.11903 \n",
      "[ 83/300] train_loss: 0.09303 valid_loss: 0.10163 test_loss: 0.11568 \n",
      "Validation loss decreased (0.102025 --> 0.101632).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08927 valid_loss: 0.10042 test_loss: 0.11568 \n",
      "Validation loss decreased (0.101632 --> 0.100423).  Saving model ...\n",
      "[ 85/300] train_loss: 0.08973 valid_loss: 0.10172 test_loss: 0.11543 \n",
      "[ 86/300] train_loss: 0.09047 valid_loss: 0.10164 test_loss: 0.11590 \n",
      "[ 87/300] train_loss: 0.09365 valid_loss: 0.09976 test_loss: 0.11531 \n",
      "Validation loss decreased (0.100423 --> 0.099763).  Saving model ...\n",
      "[ 88/300] train_loss: 0.09165 valid_loss: 0.10447 test_loss: 0.11693 \n",
      "[ 89/300] train_loss: 0.09154 valid_loss: 0.09850 test_loss: 0.11275 \n",
      "Validation loss decreased (0.099763 --> 0.098500).  Saving model ...\n",
      "[ 90/300] train_loss: 0.09225 valid_loss: 0.09823 test_loss: 0.11174 \n",
      "Validation loss decreased (0.098500 --> 0.098229).  Saving model ...\n",
      "[ 91/300] train_loss: 0.09108 valid_loss: 0.10141 test_loss: 0.11407 \n",
      "[ 92/300] train_loss: 0.08836 valid_loss: 0.09934 test_loss: 0.11328 \n",
      "[ 93/300] train_loss: 0.08776 valid_loss: 0.09905 test_loss: 0.11133 \n",
      "[ 94/300] train_loss: 0.09019 valid_loss: 0.09988 test_loss: 0.11438 \n",
      "[ 95/300] train_loss: 0.08971 valid_loss: 0.09786 test_loss: 0.11186 \n",
      "Validation loss decreased (0.098229 --> 0.097865).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08572 valid_loss: 0.09648 test_loss: 0.11108 \n",
      "Validation loss decreased (0.097865 --> 0.096484).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08636 valid_loss: 0.09815 test_loss: 0.11123 \n",
      "[ 98/300] train_loss: 0.08482 valid_loss: 0.09568 test_loss: 0.10903 \n",
      "Validation loss decreased (0.096484 --> 0.095684).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08633 valid_loss: 0.09674 test_loss: 0.10956 \n",
      "[100/300] train_loss: 0.08816 valid_loss: 0.09772 test_loss: 0.11031 \n",
      "[101/300] train_loss: 0.08728 valid_loss: 0.10054 test_loss: 0.11282 \n",
      "[102/300] train_loss: 0.08565 valid_loss: 0.09689 test_loss: 0.11043 \n",
      "[103/300] train_loss: 0.08568 valid_loss: 0.09468 test_loss: 0.10851 \n",
      "Validation loss decreased (0.095684 --> 0.094681).  Saving model ...\n",
      "[104/300] train_loss: 0.08491 valid_loss: 0.09404 test_loss: 0.10894 \n",
      "Validation loss decreased (0.094681 --> 0.094040).  Saving model ...\n",
      "[105/300] train_loss: 0.08713 valid_loss: 0.09607 test_loss: 0.10846 \n",
      "[106/300] train_loss: 0.08455 valid_loss: 0.09696 test_loss: 0.10971 \n",
      "[107/300] train_loss: 0.08466 valid_loss: 0.09566 test_loss: 0.10804 \n",
      "[108/300] train_loss: 0.08749 valid_loss: 0.09375 test_loss: 0.10768 \n",
      "Validation loss decreased (0.094040 --> 0.093752).  Saving model ...\n",
      "[109/300] train_loss: 0.08483 valid_loss: 0.09399 test_loss: 0.10683 \n",
      "[110/300] train_loss: 0.08555 valid_loss: 0.09754 test_loss: 0.11053 \n",
      "[111/300] train_loss: 0.08426 valid_loss: 0.09439 test_loss: 0.10665 \n",
      "[112/300] train_loss: 0.08522 valid_loss: 0.09508 test_loss: 0.10649 \n",
      "[113/300] train_loss: 0.08530 valid_loss: 0.09340 test_loss: 0.10596 \n",
      "Validation loss decreased (0.093752 --> 0.093399).  Saving model ...\n",
      "[114/300] train_loss: 0.08263 valid_loss: 0.09397 test_loss: 0.10699 \n",
      "[115/300] train_loss: 0.08586 valid_loss: 0.09399 test_loss: 0.10598 \n",
      "[116/300] train_loss: 0.08466 valid_loss: 0.09345 test_loss: 0.10593 \n",
      "[117/300] train_loss: 0.08329 valid_loss: 0.09232 test_loss: 0.10534 \n",
      "Validation loss decreased (0.093399 --> 0.092319).  Saving model ...\n",
      "[118/300] train_loss: 0.08293 valid_loss: 0.09313 test_loss: 0.10451 \n",
      "[119/300] train_loss: 0.08425 valid_loss: 0.09257 test_loss: 0.10540 \n",
      "[120/300] train_loss: 0.08168 valid_loss: 0.09240 test_loss: 0.10514 \n",
      "[121/300] train_loss: 0.08365 valid_loss: 0.09286 test_loss: 0.10543 \n",
      "[122/300] train_loss: 0.08350 valid_loss: 0.09151 test_loss: 0.10376 \n",
      "Validation loss decreased (0.092319 --> 0.091510).  Saving model ...\n",
      "[123/300] train_loss: 0.07900 valid_loss: 0.09008 test_loss: 0.10263 \n",
      "Validation loss decreased (0.091510 --> 0.090083).  Saving model ...\n",
      "[124/300] train_loss: 0.08246 valid_loss: 0.09094 test_loss: 0.10401 \n",
      "[125/300] train_loss: 0.08284 valid_loss: 0.09067 test_loss: 0.10342 \n",
      "[126/300] train_loss: 0.08067 valid_loss: 0.09036 test_loss: 0.10337 \n",
      "[127/300] train_loss: 0.08061 valid_loss: 0.08934 test_loss: 0.10223 \n",
      "Validation loss decreased (0.090083 --> 0.089341).  Saving model ...\n",
      "[128/300] train_loss: 0.08169 valid_loss: 0.09065 test_loss: 0.10301 \n",
      "[129/300] train_loss: 0.08152 valid_loss: 0.09109 test_loss: 0.10451 \n",
      "[130/300] train_loss: 0.08375 valid_loss: 0.09181 test_loss: 0.10570 \n",
      "[131/300] train_loss: 0.08294 valid_loss: 0.08996 test_loss: 0.10241 \n",
      "[132/300] train_loss: 0.07841 valid_loss: 0.09038 test_loss: 0.10423 \n",
      "[133/300] train_loss: 0.08138 valid_loss: 0.08960 test_loss: 0.10283 \n",
      "[134/300] train_loss: 0.07969 valid_loss: 0.09213 test_loss: 0.10491 \n",
      "[135/300] train_loss: 0.07969 valid_loss: 0.08839 test_loss: 0.10095 \n",
      "Validation loss decreased (0.089341 --> 0.088387).  Saving model ...\n",
      "[136/300] train_loss: 0.08008 valid_loss: 0.08836 test_loss: 0.10092 \n",
      "Validation loss decreased (0.088387 --> 0.088363).  Saving model ...\n",
      "[137/300] train_loss: 0.08147 valid_loss: 0.08855 test_loss: 0.10176 \n",
      "[138/300] train_loss: 0.07965 valid_loss: 0.08977 test_loss: 0.10141 \n",
      "[139/300] train_loss: 0.07823 valid_loss: 0.09010 test_loss: 0.10274 \n",
      "[140/300] train_loss: 0.07851 valid_loss: 0.08899 test_loss: 0.10125 \n",
      "[141/300] train_loss: 0.07773 valid_loss: 0.09002 test_loss: 0.10332 \n",
      "[142/300] train_loss: 0.08013 valid_loss: 0.09134 test_loss: 0.10447 \n",
      "[143/300] train_loss: 0.07987 valid_loss: 0.08895 test_loss: 0.10180 \n",
      "[144/300] train_loss: 0.07970 valid_loss: 0.08665 test_loss: 0.09942 \n",
      "Validation loss decreased (0.088363 --> 0.086654).  Saving model ...\n",
      "[145/300] train_loss: 0.08252 valid_loss: 0.08832 test_loss: 0.09837 \n",
      "[146/300] train_loss: 0.07849 valid_loss: 0.08764 test_loss: 0.10080 \n",
      "[147/300] train_loss: 0.07839 valid_loss: 0.08744 test_loss: 0.09941 \n",
      "[148/300] train_loss: 0.07833 valid_loss: 0.08738 test_loss: 0.10001 \n",
      "[149/300] train_loss: 0.07689 valid_loss: 0.08715 test_loss: 0.09917 \n",
      "[150/300] train_loss: 0.07759 valid_loss: 0.08673 test_loss: 0.09969 \n",
      "[151/300] train_loss: 0.07521 valid_loss: 0.08949 test_loss: 0.10132 \n",
      "[152/300] train_loss: 0.07457 valid_loss: 0.08772 test_loss: 0.09924 \n",
      "[153/300] train_loss: 0.07927 valid_loss: 0.08663 test_loss: 0.09902 \n",
      "Validation loss decreased (0.086654 --> 0.086632).  Saving model ...\n",
      "[154/300] train_loss: 0.07900 valid_loss: 0.08778 test_loss: 0.09985 \n",
      "[155/300] train_loss: 0.07367 valid_loss: 0.08665 test_loss: 0.09807 \n",
      "[156/300] train_loss: 0.07778 valid_loss: 0.08835 test_loss: 0.10046 \n",
      "[157/300] train_loss: 0.07743 valid_loss: 0.08790 test_loss: 0.10192 \n",
      "[158/300] train_loss: 0.07436 valid_loss: 0.08613 test_loss: 0.09669 \n",
      "Validation loss decreased (0.086632 --> 0.086131).  Saving model ...\n",
      "[159/300] train_loss: 0.07425 valid_loss: 0.08829 test_loss: 0.09945 \n",
      "[160/300] train_loss: 0.07780 valid_loss: 0.08656 test_loss: 0.09595 \n",
      "[161/300] train_loss: 0.07760 valid_loss: 0.08561 test_loss: 0.09649 \n",
      "Validation loss decreased (0.086131 --> 0.085614).  Saving model ...\n",
      "[162/300] train_loss: 0.07494 valid_loss: 0.08578 test_loss: 0.09690 \n",
      "[163/300] train_loss: 0.07640 valid_loss: 0.08679 test_loss: 0.09855 \n",
      "[164/300] train_loss: 0.07522 valid_loss: 0.08470 test_loss: 0.09679 \n",
      "Validation loss decreased (0.085614 --> 0.084698).  Saving model ...\n",
      "[165/300] train_loss: 0.07453 valid_loss: 0.08593 test_loss: 0.09783 \n",
      "[166/300] train_loss: 0.07537 valid_loss: 0.08608 test_loss: 0.09722 \n",
      "[167/300] train_loss: 0.07416 valid_loss: 0.08548 test_loss: 0.09724 \n",
      "[168/300] train_loss: 0.07384 valid_loss: 0.08635 test_loss: 0.09669 \n",
      "[169/300] train_loss: 0.07502 valid_loss: 0.08610 test_loss: 0.09910 \n",
      "[170/300] train_loss: 0.07378 valid_loss: 0.08586 test_loss: 0.09616 \n",
      "[171/300] train_loss: 0.07612 valid_loss: 0.08535 test_loss: 0.09670 \n",
      "[172/300] train_loss: 0.07628 valid_loss: 0.08455 test_loss: 0.09554 \n",
      "Validation loss decreased (0.084698 --> 0.084553).  Saving model ...\n",
      "[173/300] train_loss: 0.07182 valid_loss: 0.08691 test_loss: 0.09773 \n",
      "[174/300] train_loss: 0.07428 valid_loss: 0.08496 test_loss: 0.09523 \n",
      "[175/300] train_loss: 0.07320 valid_loss: 0.08463 test_loss: 0.09660 \n",
      "[176/300] train_loss: 0.07251 valid_loss: 0.08431 test_loss: 0.09634 \n",
      "Validation loss decreased (0.084553 --> 0.084311).  Saving model ...\n",
      "[177/300] train_loss: 0.07058 valid_loss: 0.08588 test_loss: 0.09730 \n",
      "[178/300] train_loss: 0.07479 valid_loss: 0.08406 test_loss: 0.09500 \n",
      "Validation loss decreased (0.084311 --> 0.084057).  Saving model ...\n",
      "[179/300] train_loss: 0.07291 valid_loss: 0.08573 test_loss: 0.09579 \n",
      "[180/300] train_loss: 0.07324 valid_loss: 0.08423 test_loss: 0.09534 \n",
      "[181/300] train_loss: 0.07264 valid_loss: 0.08414 test_loss: 0.09549 \n",
      "[182/300] train_loss: 0.07164 valid_loss: 0.08496 test_loss: 0.09681 \n",
      "[183/300] train_loss: 0.07347 valid_loss: 0.08324 test_loss: 0.09575 \n",
      "Validation loss decreased (0.084057 --> 0.083242).  Saving model ...\n",
      "[184/300] train_loss: 0.07374 valid_loss: 0.08355 test_loss: 0.09511 \n",
      "[185/300] train_loss: 0.07197 valid_loss: 0.08325 test_loss: 0.09430 \n",
      "[186/300] train_loss: 0.07230 valid_loss: 0.08434 test_loss: 0.09733 \n",
      "[187/300] train_loss: 0.07074 valid_loss: 0.08362 test_loss: 0.09477 \n",
      "[188/300] train_loss: 0.07345 valid_loss: 0.08181 test_loss: 0.09439 \n",
      "Validation loss decreased (0.083242 --> 0.081813).  Saving model ...\n",
      "[189/300] train_loss: 0.07371 valid_loss: 0.08441 test_loss: 0.09448 \n",
      "[190/300] train_loss: 0.07141 valid_loss: 0.08383 test_loss: 0.09471 \n",
      "[191/300] train_loss: 0.07079 valid_loss: 0.08416 test_loss: 0.09453 \n",
      "[192/300] train_loss: 0.07377 valid_loss: 0.08371 test_loss: 0.09424 \n",
      "[193/300] train_loss: 0.07344 valid_loss: 0.08325 test_loss: 0.09488 \n",
      "[194/300] train_loss: 0.07125 valid_loss: 0.08343 test_loss: 0.09473 \n",
      "[195/300] train_loss: 0.07091 valid_loss: 0.08418 test_loss: 0.09390 \n",
      "[196/300] train_loss: 0.07163 valid_loss: 0.08595 test_loss: 0.09840 \n",
      "[197/300] train_loss: 0.07158 valid_loss: 0.08275 test_loss: 0.09399 \n",
      "[198/300] train_loss: 0.07353 valid_loss: 0.08407 test_loss: 0.09414 \n",
      "[199/300] train_loss: 0.07003 valid_loss: 0.08421 test_loss: 0.09386 \n",
      "[200/300] train_loss: 0.06997 valid_loss: 0.08278 test_loss: 0.09376 \n",
      "[201/300] train_loss: 0.07015 valid_loss: 0.08354 test_loss: 0.09330 \n",
      "[202/300] train_loss: 0.07041 valid_loss: 0.08291 test_loss: 0.09390 \n",
      "[203/300] train_loss: 0.06836 valid_loss: 0.08147 test_loss: 0.09258 \n",
      "Validation loss decreased (0.081813 --> 0.081466).  Saving model ...\n",
      "[204/300] train_loss: 0.06869 valid_loss: 0.08434 test_loss: 0.09416 \n",
      "[205/300] train_loss: 0.07159 valid_loss: 0.08389 test_loss: 0.09384 \n",
      "[206/300] train_loss: 0.06984 valid_loss: 0.08458 test_loss: 0.09470 \n",
      "[207/300] train_loss: 0.06987 valid_loss: 0.08268 test_loss: 0.09383 \n",
      "[208/300] train_loss: 0.06689 valid_loss: 0.08397 test_loss: 0.09516 \n",
      "[209/300] train_loss: 0.07037 valid_loss: 0.08334 test_loss: 0.09481 \n",
      "[210/300] train_loss: 0.06997 valid_loss: 0.08252 test_loss: 0.09692 \n",
      "[211/300] train_loss: 0.07055 valid_loss: 0.08352 test_loss: 0.09445 \n",
      "[212/300] train_loss: 0.06894 valid_loss: 0.08246 test_loss: 0.09337 \n",
      "[213/300] train_loss: 0.07087 valid_loss: 0.08193 test_loss: 0.09217 \n",
      "[214/300] train_loss: 0.06870 valid_loss: 0.08331 test_loss: 0.09287 \n",
      "[215/300] train_loss: 0.06755 valid_loss: 0.08124 test_loss: 0.09207 \n",
      "Validation loss decreased (0.081466 --> 0.081245).  Saving model ...\n",
      "[216/300] train_loss: 0.06940 valid_loss: 0.08229 test_loss: 0.09271 \n",
      "[217/300] train_loss: 0.06842 valid_loss: 0.08214 test_loss: 0.09231 \n",
      "[218/300] train_loss: 0.06797 valid_loss: 0.08293 test_loss: 0.09198 \n",
      "[219/300] train_loss: 0.06758 valid_loss: 0.08267 test_loss: 0.09570 \n",
      "[220/300] train_loss: 0.07023 valid_loss: 0.08214 test_loss: 0.09177 \n",
      "[221/300] train_loss: 0.06618 valid_loss: 0.08299 test_loss: 0.09305 \n",
      "[222/300] train_loss: 0.06938 valid_loss: 0.08116 test_loss: 0.09130 \n",
      "Validation loss decreased (0.081245 --> 0.081159).  Saving model ...\n",
      "[223/300] train_loss: 0.06892 valid_loss: 0.08147 test_loss: 0.09183 \n",
      "[224/300] train_loss: 0.07038 valid_loss: 0.08125 test_loss: 0.09103 \n",
      "[225/300] train_loss: 0.06673 valid_loss: 0.08253 test_loss: 0.09228 \n",
      "[226/300] train_loss: 0.06847 valid_loss: 0.08086 test_loss: 0.09088 \n",
      "Validation loss decreased (0.081159 --> 0.080863).  Saving model ...\n",
      "[227/300] train_loss: 0.06928 valid_loss: 0.08187 test_loss: 0.09221 \n",
      "[228/300] train_loss: 0.07046 valid_loss: 0.08116 test_loss: 0.09164 \n",
      "[229/300] train_loss: 0.06799 valid_loss: 0.08168 test_loss: 0.09286 \n",
      "[230/300] train_loss: 0.06619 valid_loss: 0.08287 test_loss: 0.09269 \n",
      "[231/300] train_loss: 0.06979 valid_loss: 0.08213 test_loss: 0.09304 \n",
      "[232/300] train_loss: 0.06719 valid_loss: 0.08285 test_loss: 0.09246 \n",
      "[233/300] train_loss: 0.06712 valid_loss: 0.08181 test_loss: 0.09144 \n",
      "[234/300] train_loss: 0.06778 valid_loss: 0.08105 test_loss: 0.09050 \n",
      "[235/300] train_loss: 0.06791 valid_loss: 0.08233 test_loss: 0.09315 \n",
      "[236/300] train_loss: 0.06722 valid_loss: 0.08051 test_loss: 0.08984 \n",
      "Validation loss decreased (0.080863 --> 0.080511).  Saving model ...\n",
      "[237/300] train_loss: 0.06710 valid_loss: 0.08380 test_loss: 0.09341 \n",
      "[238/300] train_loss: 0.06736 valid_loss: 0.08259 test_loss: 0.09092 \n",
      "[239/300] train_loss: 0.06594 valid_loss: 0.08199 test_loss: 0.09173 \n",
      "[240/300] train_loss: 0.06499 valid_loss: 0.08323 test_loss: 0.09192 \n",
      "[241/300] train_loss: 0.06868 valid_loss: 0.08037 test_loss: 0.09048 \n",
      "Validation loss decreased (0.080511 --> 0.080366).  Saving model ...\n",
      "[242/300] train_loss: 0.06644 valid_loss: 0.08025 test_loss: 0.09126 \n",
      "Validation loss decreased (0.080366 --> 0.080251).  Saving model ...\n",
      "[243/300] train_loss: 0.06827 valid_loss: 0.08081 test_loss: 0.09176 \n",
      "[244/300] train_loss: 0.06704 valid_loss: 0.08061 test_loss: 0.09140 \n",
      "[245/300] train_loss: 0.06510 valid_loss: 0.07928 test_loss: 0.09043 \n",
      "Validation loss decreased (0.080251 --> 0.079278).  Saving model ...\n",
      "[246/300] train_loss: 0.06667 valid_loss: 0.08016 test_loss: 0.09111 \n",
      "[247/300] train_loss: 0.06779 valid_loss: 0.07867 test_loss: 0.08956 \n",
      "Validation loss decreased (0.079278 --> 0.078667).  Saving model ...\n",
      "[248/300] train_loss: 0.06553 valid_loss: 0.07980 test_loss: 0.09003 \n",
      "[249/300] train_loss: 0.06690 valid_loss: 0.08058 test_loss: 0.09056 \n",
      "[250/300] train_loss: 0.06581 valid_loss: 0.08002 test_loss: 0.08906 \n",
      "[251/300] train_loss: 0.06347 valid_loss: 0.07955 test_loss: 0.09075 \n",
      "[252/300] train_loss: 0.06657 valid_loss: 0.07924 test_loss: 0.09114 \n",
      "[253/300] train_loss: 0.06541 valid_loss: 0.07911 test_loss: 0.09010 \n",
      "[254/300] train_loss: 0.06632 valid_loss: 0.07959 test_loss: 0.09008 \n",
      "[255/300] train_loss: 0.06520 valid_loss: 0.08044 test_loss: 0.09194 \n",
      "[256/300] train_loss: 0.06651 valid_loss: 0.07934 test_loss: 0.08879 \n",
      "[257/300] train_loss: 0.06610 valid_loss: 0.07908 test_loss: 0.08895 \n",
      "[258/300] train_loss: 0.06429 valid_loss: 0.07831 test_loss: 0.08935 \n",
      "Validation loss decreased (0.078667 --> 0.078313).  Saving model ...\n",
      "[259/300] train_loss: 0.06559 valid_loss: 0.07875 test_loss: 0.09030 \n",
      "[260/300] train_loss: 0.06535 valid_loss: 0.07958 test_loss: 0.09027 \n",
      "[261/300] train_loss: 0.06442 valid_loss: 0.08135 test_loss: 0.09019 \n",
      "[262/300] train_loss: 0.06724 valid_loss: 0.07976 test_loss: 0.09045 \n",
      "[263/300] train_loss: 0.06597 valid_loss: 0.07933 test_loss: 0.08874 \n",
      "[264/300] train_loss: 0.06479 valid_loss: 0.08087 test_loss: 0.09049 \n",
      "[265/300] train_loss: 0.06563 valid_loss: 0.08055 test_loss: 0.09001 \n",
      "[266/300] train_loss: 0.06422 valid_loss: 0.08253 test_loss: 0.09110 \n",
      "[267/300] train_loss: 0.06477 valid_loss: 0.08074 test_loss: 0.09096 \n",
      "[268/300] train_loss: 0.06613 valid_loss: 0.07933 test_loss: 0.08907 \n",
      "[269/300] train_loss: 0.06481 valid_loss: 0.07895 test_loss: 0.08968 \n",
      "[270/300] train_loss: 0.06339 valid_loss: 0.07934 test_loss: 0.08991 \n",
      "[271/300] train_loss: 0.06352 valid_loss: 0.07936 test_loss: 0.08910 \n",
      "[272/300] train_loss: 0.06456 valid_loss: 0.08074 test_loss: 0.08956 \n",
      "[273/300] train_loss: 0.06416 valid_loss: 0.07895 test_loss: 0.08884 \n",
      "[274/300] train_loss: 0.06658 valid_loss: 0.07828 test_loss: 0.08782 \n",
      "Validation loss decreased (0.078313 --> 0.078283).  Saving model ...\n",
      "[275/300] train_loss: 0.06256 valid_loss: 0.07888 test_loss: 0.08796 \n",
      "[276/300] train_loss: 0.06636 valid_loss: 0.07814 test_loss: 0.08944 \n",
      "Validation loss decreased (0.078283 --> 0.078139).  Saving model ...\n",
      "[277/300] train_loss: 0.06370 valid_loss: 0.07807 test_loss: 0.08896 \n",
      "Validation loss decreased (0.078139 --> 0.078067).  Saving model ...\n",
      "[278/300] train_loss: 0.06483 valid_loss: 0.07892 test_loss: 0.08848 \n",
      "[279/300] train_loss: 0.06427 valid_loss: 0.07819 test_loss: 0.08811 \n",
      "[280/300] train_loss: 0.06458 valid_loss: 0.07983 test_loss: 0.08858 \n",
      "[281/300] train_loss: 0.06327 valid_loss: 0.08007 test_loss: 0.08850 \n",
      "[282/300] train_loss: 0.06415 valid_loss: 0.07854 test_loss: 0.08942 \n",
      "[283/300] train_loss: 0.06292 valid_loss: 0.07884 test_loss: 0.08817 \n",
      "[284/300] train_loss: 0.06372 valid_loss: 0.07828 test_loss: 0.08840 \n",
      "[285/300] train_loss: 0.06299 valid_loss: 0.07753 test_loss: 0.08728 \n",
      "Validation loss decreased (0.078067 --> 0.077534).  Saving model ...\n",
      "[286/300] train_loss: 0.06316 valid_loss: 0.07916 test_loss: 0.08936 \n",
      "[287/300] train_loss: 0.06182 valid_loss: 0.07989 test_loss: 0.08996 \n",
      "[288/300] train_loss: 0.06271 valid_loss: 0.07859 test_loss: 0.08897 \n",
      "[289/300] train_loss: 0.06504 valid_loss: 0.07918 test_loss: 0.08856 \n",
      "[290/300] train_loss: 0.06417 valid_loss: 0.07946 test_loss: 0.08787 \n",
      "[291/300] train_loss: 0.06201 valid_loss: 0.07836 test_loss: 0.08856 \n",
      "[292/300] train_loss: 0.06380 valid_loss: 0.07866 test_loss: 0.08761 \n",
      "[293/300] train_loss: 0.06274 valid_loss: 0.07859 test_loss: 0.08748 \n",
      "[294/300] train_loss: 0.06257 valid_loss: 0.07859 test_loss: 0.08764 \n",
      "[295/300] train_loss: 0.06273 valid_loss: 0.07658 test_loss: 0.08672 \n",
      "Validation loss decreased (0.077534 --> 0.076582).  Saving model ...\n",
      "[296/300] train_loss: 0.06428 valid_loss: 0.07938 test_loss: 0.08874 \n",
      "[297/300] train_loss: 0.06357 valid_loss: 0.07785 test_loss: 0.08700 \n",
      "[298/300] train_loss: 0.06443 valid_loss: 0.07796 test_loss: 0.08766 \n",
      "[299/300] train_loss: 0.06097 valid_loss: 0.07810 test_loss: 0.09013 \n",
      "[300/300] train_loss: 0.06166 valid_loss: 0.07768 test_loss: 0.08722 \n",
      "TRAINING MODEL 1\n",
      "[  1/300] train_loss: 0.55629 valid_loss: 0.44800 test_loss: 0.45654 \n",
      "Validation loss decreased (inf --> 0.448003).  Saving model ...\n",
      "[  2/300] train_loss: 0.35943 valid_loss: 0.32939 test_loss: 0.33918 \n",
      "Validation loss decreased (0.448003 --> 0.329394).  Saving model ...\n",
      "[  3/300] train_loss: 0.27239 valid_loss: 0.27106 test_loss: 0.28377 \n",
      "Validation loss decreased (0.329394 --> 0.271057).  Saving model ...\n",
      "[  4/300] train_loss: 0.23400 valid_loss: 0.23485 test_loss: 0.24986 \n",
      "Validation loss decreased (0.271057 --> 0.234848).  Saving model ...\n",
      "[  5/300] train_loss: 0.21059 valid_loss: 0.21831 test_loss: 0.23598 \n",
      "Validation loss decreased (0.234848 --> 0.218313).  Saving model ...\n",
      "[  6/300] train_loss: 0.20009 valid_loss: 0.19946 test_loss: 0.21521 \n",
      "Validation loss decreased (0.218313 --> 0.199465).  Saving model ...\n",
      "[  7/300] train_loss: 0.18606 valid_loss: 0.19020 test_loss: 0.20556 \n",
      "Validation loss decreased (0.199465 --> 0.190197).  Saving model ...\n",
      "[  8/300] train_loss: 0.18118 valid_loss: 0.18177 test_loss: 0.19870 \n",
      "Validation loss decreased (0.190197 --> 0.181771).  Saving model ...\n",
      "[  9/300] train_loss: 0.16976 valid_loss: 0.17645 test_loss: 0.19017 \n",
      "Validation loss decreased (0.181771 --> 0.176454).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17072 valid_loss: 0.17478 test_loss: 0.18713 \n",
      "Validation loss decreased (0.176454 --> 0.174779).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16136 valid_loss: 0.16944 test_loss: 0.18046 \n",
      "Validation loss decreased (0.174779 --> 0.169441).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15872 valid_loss: 0.16281 test_loss: 0.17447 \n",
      "Validation loss decreased (0.169441 --> 0.162812).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15387 valid_loss: 0.16251 test_loss: 0.17407 \n",
      "Validation loss decreased (0.162812 --> 0.162505).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15113 valid_loss: 0.15886 test_loss: 0.16738 \n",
      "Validation loss decreased (0.162505 --> 0.158860).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14613 valid_loss: 0.15636 test_loss: 0.16617 \n",
      "Validation loss decreased (0.158860 --> 0.156355).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14530 valid_loss: 0.15801 test_loss: 0.16666 \n",
      "[ 17/300] train_loss: 0.14473 valid_loss: 0.15338 test_loss: 0.16173 \n",
      "Validation loss decreased (0.156355 --> 0.153375).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13968 valid_loss: 0.15189 test_loss: 0.16068 \n",
      "Validation loss decreased (0.153375 --> 0.151892).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13568 valid_loss: 0.15299 test_loss: 0.16037 \n",
      "[ 20/300] train_loss: 0.13484 valid_loss: 0.15419 test_loss: 0.16301 \n",
      "[ 21/300] train_loss: 0.13380 valid_loss: 0.15037 test_loss: 0.15687 \n",
      "Validation loss decreased (0.151892 --> 0.150368).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13291 valid_loss: 0.14633 test_loss: 0.15533 \n",
      "Validation loss decreased (0.150368 --> 0.146334).  Saving model ...\n",
      "[ 23/300] train_loss: 0.13094 valid_loss: 0.14491 test_loss: 0.15307 \n",
      "Validation loss decreased (0.146334 --> 0.144909).  Saving model ...\n",
      "[ 24/300] train_loss: 0.13148 valid_loss: 0.14748 test_loss: 0.15476 \n",
      "[ 25/300] train_loss: 0.12702 valid_loss: 0.14251 test_loss: 0.15346 \n",
      "Validation loss decreased (0.144909 --> 0.142511).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12845 valid_loss: 0.14190 test_loss: 0.15065 \n",
      "Validation loss decreased (0.142511 --> 0.141901).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12688 valid_loss: 0.14194 test_loss: 0.15096 \n",
      "[ 28/300] train_loss: 0.12395 valid_loss: 0.13507 test_loss: 0.14556 \n",
      "Validation loss decreased (0.141901 --> 0.135073).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12508 valid_loss: 0.13629 test_loss: 0.14499 \n",
      "[ 30/300] train_loss: 0.12538 valid_loss: 0.13518 test_loss: 0.14627 \n",
      "[ 31/300] train_loss: 0.12046 valid_loss: 0.13198 test_loss: 0.14162 \n",
      "Validation loss decreased (0.135073 --> 0.131984).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11971 valid_loss: 0.12951 test_loss: 0.14101 \n",
      "Validation loss decreased (0.131984 --> 0.129507).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11969 valid_loss: 0.13834 test_loss: 0.14771 \n",
      "[ 34/300] train_loss: 0.11865 valid_loss: 0.12850 test_loss: 0.14014 \n",
      "Validation loss decreased (0.129507 --> 0.128500).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11518 valid_loss: 0.12798 test_loss: 0.13845 \n",
      "Validation loss decreased (0.128500 --> 0.127983).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11445 valid_loss: 0.12823 test_loss: 0.14177 \n",
      "[ 37/300] train_loss: 0.11594 valid_loss: 0.13031 test_loss: 0.13991 \n",
      "[ 38/300] train_loss: 0.11338 valid_loss: 0.12500 test_loss: 0.13665 \n",
      "Validation loss decreased (0.127983 --> 0.124997).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11392 valid_loss: 0.12287 test_loss: 0.13434 \n",
      "Validation loss decreased (0.124997 --> 0.122874).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11047 valid_loss: 0.12084 test_loss: 0.13351 \n",
      "Validation loss decreased (0.122874 --> 0.120841).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11378 valid_loss: 0.12539 test_loss: 0.13571 \n",
      "[ 42/300] train_loss: 0.11060 valid_loss: 0.12279 test_loss: 0.13372 \n",
      "[ 43/300] train_loss: 0.10802 valid_loss: 0.11775 test_loss: 0.12993 \n",
      "Validation loss decreased (0.120841 --> 0.117755).  Saving model ...\n",
      "[ 44/300] train_loss: 0.11117 valid_loss: 0.11918 test_loss: 0.13030 \n",
      "[ 45/300] train_loss: 0.10967 valid_loss: 0.12075 test_loss: 0.13217 \n",
      "[ 46/300] train_loss: 0.10891 valid_loss: 0.11737 test_loss: 0.12903 \n",
      "Validation loss decreased (0.117755 --> 0.117366).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10634 valid_loss: 0.11532 test_loss: 0.12769 \n",
      "Validation loss decreased (0.117366 --> 0.115321).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10145 valid_loss: 0.11558 test_loss: 0.12760 \n",
      "[ 49/300] train_loss: 0.10817 valid_loss: 0.11865 test_loss: 0.12806 \n",
      "[ 50/300] train_loss: 0.10496 valid_loss: 0.11568 test_loss: 0.12703 \n",
      "[ 51/300] train_loss: 0.10502 valid_loss: 0.11571 test_loss: 0.12647 \n",
      "[ 52/300] train_loss: 0.10611 valid_loss: 0.11369 test_loss: 0.12545 \n",
      "Validation loss decreased (0.115321 --> 0.113694).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10195 valid_loss: 0.11435 test_loss: 0.12727 \n",
      "[ 54/300] train_loss: 0.10118 valid_loss: 0.11193 test_loss: 0.12463 \n",
      "Validation loss decreased (0.113694 --> 0.111927).  Saving model ...\n",
      "[ 55/300] train_loss: 0.10006 valid_loss: 0.11290 test_loss: 0.12379 \n",
      "[ 56/300] train_loss: 0.10261 valid_loss: 0.11195 test_loss: 0.12332 \n",
      "[ 57/300] train_loss: 0.10085 valid_loss: 0.10996 test_loss: 0.12260 \n",
      "Validation loss decreased (0.111927 --> 0.109956).  Saving model ...\n",
      "[ 58/300] train_loss: 0.10238 valid_loss: 0.11292 test_loss: 0.12363 \n",
      "[ 59/300] train_loss: 0.10102 valid_loss: 0.11225 test_loss: 0.12299 \n",
      "[ 60/300] train_loss: 0.10060 valid_loss: 0.11289 test_loss: 0.12574 \n",
      "[ 61/300] train_loss: 0.09829 valid_loss: 0.11195 test_loss: 0.12221 \n",
      "[ 62/300] train_loss: 0.09916 valid_loss: 0.10823 test_loss: 0.11929 \n",
      "Validation loss decreased (0.109956 --> 0.108229).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09876 valid_loss: 0.11353 test_loss: 0.12600 \n",
      "[ 64/300] train_loss: 0.10106 valid_loss: 0.10829 test_loss: 0.11979 \n",
      "[ 65/300] train_loss: 0.09814 valid_loss: 0.10804 test_loss: 0.11909 \n",
      "Validation loss decreased (0.108229 --> 0.108038).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09454 valid_loss: 0.10947 test_loss: 0.12049 \n",
      "[ 67/300] train_loss: 0.09601 valid_loss: 0.10661 test_loss: 0.12013 \n",
      "Validation loss decreased (0.108038 --> 0.106607).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09506 valid_loss: 0.10631 test_loss: 0.11749 \n",
      "Validation loss decreased (0.106607 --> 0.106308).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09690 valid_loss: 0.10426 test_loss: 0.11659 \n",
      "Validation loss decreased (0.106308 --> 0.104261).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09804 valid_loss: 0.10684 test_loss: 0.11734 \n",
      "[ 71/300] train_loss: 0.09631 valid_loss: 0.10392 test_loss: 0.11578 \n",
      "Validation loss decreased (0.104261 --> 0.103920).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09541 valid_loss: 0.10421 test_loss: 0.11533 \n",
      "[ 73/300] train_loss: 0.09443 valid_loss: 0.10911 test_loss: 0.11771 \n",
      "[ 74/300] train_loss: 0.09204 valid_loss: 0.10340 test_loss: 0.11437 \n",
      "Validation loss decreased (0.103920 --> 0.103395).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09326 valid_loss: 0.10588 test_loss: 0.11473 \n",
      "[ 76/300] train_loss: 0.09375 valid_loss: 0.10340 test_loss: 0.11464 \n",
      "[ 77/300] train_loss: 0.09300 valid_loss: 0.10156 test_loss: 0.11332 \n",
      "Validation loss decreased (0.103395 --> 0.101557).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09073 valid_loss: 0.10472 test_loss: 0.11403 \n",
      "[ 79/300] train_loss: 0.09207 valid_loss: 0.10222 test_loss: 0.11395 \n",
      "[ 80/300] train_loss: 0.09071 valid_loss: 0.10481 test_loss: 0.11703 \n",
      "[ 81/300] train_loss: 0.09265 valid_loss: 0.10346 test_loss: 0.11300 \n",
      "[ 82/300] train_loss: 0.09144 valid_loss: 0.10471 test_loss: 0.11500 \n",
      "[ 83/300] train_loss: 0.09312 valid_loss: 0.10452 test_loss: 0.11450 \n",
      "[ 84/300] train_loss: 0.09120 valid_loss: 0.10421 test_loss: 0.11426 \n",
      "[ 85/300] train_loss: 0.09192 valid_loss: 0.10040 test_loss: 0.11094 \n",
      "Validation loss decreased (0.101557 --> 0.100403).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08901 valid_loss: 0.09959 test_loss: 0.11039 \n",
      "Validation loss decreased (0.100403 --> 0.099594).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08866 valid_loss: 0.10040 test_loss: 0.11029 \n",
      "[ 88/300] train_loss: 0.08976 valid_loss: 0.10220 test_loss: 0.11034 \n",
      "[ 89/300] train_loss: 0.08939 valid_loss: 0.09890 test_loss: 0.11018 \n",
      "Validation loss decreased (0.099594 --> 0.098899).  Saving model ...\n",
      "[ 90/300] train_loss: 0.09038 valid_loss: 0.09950 test_loss: 0.10791 \n",
      "[ 91/300] train_loss: 0.08911 valid_loss: 0.09927 test_loss: 0.10946 \n",
      "[ 92/300] train_loss: 0.08797 valid_loss: 0.09827 test_loss: 0.10871 \n",
      "Validation loss decreased (0.098899 --> 0.098266).  Saving model ...\n",
      "[ 93/300] train_loss: 0.08717 valid_loss: 0.09877 test_loss: 0.10891 \n",
      "[ 94/300] train_loss: 0.08976 valid_loss: 0.09876 test_loss: 0.10991 \n",
      "[ 95/300] train_loss: 0.08704 valid_loss: 0.09856 test_loss: 0.10792 \n",
      "[ 96/300] train_loss: 0.08874 valid_loss: 0.09806 test_loss: 0.10813 \n",
      "Validation loss decreased (0.098266 --> 0.098057).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08883 valid_loss: 0.09734 test_loss: 0.10765 \n",
      "Validation loss decreased (0.098057 --> 0.097342).  Saving model ...\n",
      "[ 98/300] train_loss: 0.08635 valid_loss: 0.09686 test_loss: 0.10740 \n",
      "Validation loss decreased (0.097342 --> 0.096863).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08589 valid_loss: 0.10204 test_loss: 0.11177 \n",
      "[100/300] train_loss: 0.08319 valid_loss: 0.09954 test_loss: 0.10906 \n",
      "[101/300] train_loss: 0.08553 valid_loss: 0.10095 test_loss: 0.11231 \n",
      "[102/300] train_loss: 0.08742 valid_loss: 0.09936 test_loss: 0.10915 \n",
      "[103/300] train_loss: 0.08516 valid_loss: 0.09866 test_loss: 0.10781 \n",
      "[104/300] train_loss: 0.08358 valid_loss: 0.09645 test_loss: 0.10712 \n",
      "Validation loss decreased (0.096863 --> 0.096445).  Saving model ...\n",
      "[105/300] train_loss: 0.08459 valid_loss: 0.09520 test_loss: 0.10574 \n",
      "Validation loss decreased (0.096445 --> 0.095199).  Saving model ...\n",
      "[106/300] train_loss: 0.08324 valid_loss: 0.09666 test_loss: 0.10700 \n",
      "[107/300] train_loss: 0.08426 valid_loss: 0.09870 test_loss: 0.10829 \n",
      "[108/300] train_loss: 0.08456 valid_loss: 0.09707 test_loss: 0.10747 \n",
      "[109/300] train_loss: 0.08464 valid_loss: 0.09519 test_loss: 0.10409 \n",
      "Validation loss decreased (0.095199 --> 0.095195).  Saving model ...\n",
      "[110/300] train_loss: 0.08379 valid_loss: 0.09610 test_loss: 0.10562 \n",
      "[111/300] train_loss: 0.08209 valid_loss: 0.09616 test_loss: 0.10555 \n",
      "[112/300] train_loss: 0.08322 valid_loss: 0.09448 test_loss: 0.10375 \n",
      "Validation loss decreased (0.095195 --> 0.094475).  Saving model ...\n",
      "[113/300] train_loss: 0.08451 valid_loss: 0.09333 test_loss: 0.10341 \n",
      "Validation loss decreased (0.094475 --> 0.093333).  Saving model ...\n",
      "[114/300] train_loss: 0.08439 valid_loss: 0.09543 test_loss: 0.10456 \n",
      "[115/300] train_loss: 0.08330 valid_loss: 0.09391 test_loss: 0.10480 \n",
      "[116/300] train_loss: 0.08228 valid_loss: 0.09428 test_loss: 0.10443 \n",
      "[117/300] train_loss: 0.08250 valid_loss: 0.09463 test_loss: 0.10377 \n",
      "[118/300] train_loss: 0.08068 valid_loss: 0.09317 test_loss: 0.10362 \n",
      "Validation loss decreased (0.093333 --> 0.093168).  Saving model ...\n",
      "[119/300] train_loss: 0.08302 valid_loss: 0.09341 test_loss: 0.10326 \n",
      "[120/300] train_loss: 0.07935 valid_loss: 0.09448 test_loss: 0.10370 \n",
      "[121/300] train_loss: 0.08165 valid_loss: 0.09165 test_loss: 0.10031 \n",
      "Validation loss decreased (0.093168 --> 0.091653).  Saving model ...\n",
      "[122/300] train_loss: 0.08220 valid_loss: 0.09372 test_loss: 0.10158 \n",
      "[123/300] train_loss: 0.07996 valid_loss: 0.09069 test_loss: 0.10010 \n",
      "Validation loss decreased (0.091653 --> 0.090693).  Saving model ...\n",
      "[124/300] train_loss: 0.08002 valid_loss: 0.09124 test_loss: 0.10134 \n",
      "[125/300] train_loss: 0.08156 valid_loss: 0.09251 test_loss: 0.10287 \n",
      "[126/300] train_loss: 0.07812 valid_loss: 0.09173 test_loss: 0.10062 \n",
      "[127/300] train_loss: 0.07852 valid_loss: 0.09278 test_loss: 0.10191 \n",
      "[128/300] train_loss: 0.08082 valid_loss: 0.09239 test_loss: 0.10193 \n",
      "[129/300] train_loss: 0.07842 valid_loss: 0.09214 test_loss: 0.10031 \n",
      "[130/300] train_loss: 0.08043 valid_loss: 0.09371 test_loss: 0.10141 \n",
      "[131/300] train_loss: 0.08001 valid_loss: 0.09103 test_loss: 0.10111 \n",
      "[132/300] train_loss: 0.08045 valid_loss: 0.09090 test_loss: 0.10132 \n",
      "[133/300] train_loss: 0.08066 valid_loss: 0.09005 test_loss: 0.10056 \n",
      "Validation loss decreased (0.090693 --> 0.090054).  Saving model ...\n",
      "[134/300] train_loss: 0.07636 valid_loss: 0.09063 test_loss: 0.10061 \n",
      "[135/300] train_loss: 0.07757 valid_loss: 0.09113 test_loss: 0.10071 \n",
      "[136/300] train_loss: 0.07796 valid_loss: 0.08998 test_loss: 0.10014 \n",
      "Validation loss decreased (0.090054 --> 0.089978).  Saving model ...\n",
      "[137/300] train_loss: 0.07769 valid_loss: 0.09117 test_loss: 0.10090 \n",
      "[138/300] train_loss: 0.07823 valid_loss: 0.09045 test_loss: 0.10028 \n",
      "[139/300] train_loss: 0.07812 valid_loss: 0.08923 test_loss: 0.09932 \n",
      "Validation loss decreased (0.089978 --> 0.089229).  Saving model ...\n",
      "[140/300] train_loss: 0.07689 valid_loss: 0.08841 test_loss: 0.09735 \n",
      "Validation loss decreased (0.089229 --> 0.088413).  Saving model ...\n",
      "[141/300] train_loss: 0.07960 valid_loss: 0.09015 test_loss: 0.09830 \n",
      "[142/300] train_loss: 0.07557 valid_loss: 0.08920 test_loss: 0.09865 \n",
      "[143/300] train_loss: 0.07765 valid_loss: 0.08804 test_loss: 0.09814 \n",
      "Validation loss decreased (0.088413 --> 0.088036).  Saving model ...\n",
      "[144/300] train_loss: 0.07639 valid_loss: 0.08857 test_loss: 0.09919 \n",
      "[145/300] train_loss: 0.07906 valid_loss: 0.08782 test_loss: 0.09743 \n",
      "Validation loss decreased (0.088036 --> 0.087816).  Saving model ...\n",
      "[146/300] train_loss: 0.07622 valid_loss: 0.08901 test_loss: 0.09849 \n",
      "[147/300] train_loss: 0.07617 valid_loss: 0.08740 test_loss: 0.09669 \n",
      "Validation loss decreased (0.087816 --> 0.087397).  Saving model ...\n",
      "[148/300] train_loss: 0.07688 valid_loss: 0.08778 test_loss: 0.09736 \n",
      "[149/300] train_loss: 0.07406 valid_loss: 0.08738 test_loss: 0.09762 \n",
      "Validation loss decreased (0.087397 --> 0.087383).  Saving model ...\n",
      "[150/300] train_loss: 0.07698 valid_loss: 0.08974 test_loss: 0.09875 \n",
      "[151/300] train_loss: 0.07325 valid_loss: 0.08767 test_loss: 0.09740 \n",
      "[152/300] train_loss: 0.07534 valid_loss: 0.08912 test_loss: 0.09883 \n",
      "[153/300] train_loss: 0.07624 valid_loss: 0.08666 test_loss: 0.09712 \n",
      "Validation loss decreased (0.087383 --> 0.086661).  Saving model ...\n",
      "[154/300] train_loss: 0.07478 valid_loss: 0.08713 test_loss: 0.09759 \n",
      "[155/300] train_loss: 0.07833 valid_loss: 0.08736 test_loss: 0.09865 \n",
      "[156/300] train_loss: 0.07465 valid_loss: 0.08951 test_loss: 0.09765 \n",
      "[157/300] train_loss: 0.07653 valid_loss: 0.08653 test_loss: 0.09597 \n",
      "Validation loss decreased (0.086661 --> 0.086529).  Saving model ...\n",
      "[158/300] train_loss: 0.07525 valid_loss: 0.08752 test_loss: 0.09647 \n",
      "[159/300] train_loss: 0.07417 valid_loss: 0.08749 test_loss: 0.09742 \n",
      "[160/300] train_loss: 0.07504 valid_loss: 0.08629 test_loss: 0.09577 \n",
      "Validation loss decreased (0.086529 --> 0.086289).  Saving model ...\n",
      "[161/300] train_loss: 0.07558 valid_loss: 0.08777 test_loss: 0.09856 \n",
      "[162/300] train_loss: 0.07315 valid_loss: 0.08631 test_loss: 0.09627 \n",
      "[163/300] train_loss: 0.07414 valid_loss: 0.08605 test_loss: 0.09532 \n",
      "Validation loss decreased (0.086289 --> 0.086046).  Saving model ...\n",
      "[164/300] train_loss: 0.07585 valid_loss: 0.08836 test_loss: 0.09700 \n",
      "[165/300] train_loss: 0.07277 valid_loss: 0.08792 test_loss: 0.09668 \n",
      "[166/300] train_loss: 0.07575 valid_loss: 0.08673 test_loss: 0.09610 \n",
      "[167/300] train_loss: 0.07371 valid_loss: 0.08675 test_loss: 0.09553 \n",
      "[168/300] train_loss: 0.07587 valid_loss: 0.08835 test_loss: 0.09754 \n",
      "[169/300] train_loss: 0.07354 valid_loss: 0.08649 test_loss: 0.09619 \n",
      "[170/300] train_loss: 0.07351 valid_loss: 0.08545 test_loss: 0.09425 \n",
      "Validation loss decreased (0.086046 --> 0.085448).  Saving model ...\n",
      "[171/300] train_loss: 0.07494 valid_loss: 0.08716 test_loss: 0.09557 \n",
      "[172/300] train_loss: 0.07330 valid_loss: 0.08758 test_loss: 0.09687 \n",
      "[173/300] train_loss: 0.07447 valid_loss: 0.08570 test_loss: 0.09414 \n",
      "[174/300] train_loss: 0.07359 valid_loss: 0.08582 test_loss: 0.09548 \n",
      "[175/300] train_loss: 0.07470 valid_loss: 0.08613 test_loss: 0.09480 \n",
      "[176/300] train_loss: 0.07272 valid_loss: 0.08490 test_loss: 0.09409 \n",
      "Validation loss decreased (0.085448 --> 0.084903).  Saving model ...\n",
      "[177/300] train_loss: 0.07162 valid_loss: 0.08564 test_loss: 0.09486 \n",
      "[178/300] train_loss: 0.07087 valid_loss: 0.08471 test_loss: 0.09412 \n",
      "Validation loss decreased (0.084903 --> 0.084705).  Saving model ...\n",
      "[179/300] train_loss: 0.07283 valid_loss: 0.08390 test_loss: 0.09325 \n",
      "Validation loss decreased (0.084705 --> 0.083896).  Saving model ...\n",
      "[180/300] train_loss: 0.07546 valid_loss: 0.08481 test_loss: 0.09403 \n",
      "[181/300] train_loss: 0.07496 valid_loss: 0.08540 test_loss: 0.09424 \n",
      "[182/300] train_loss: 0.07376 valid_loss: 0.08441 test_loss: 0.09383 \n",
      "[183/300] train_loss: 0.07407 valid_loss: 0.08466 test_loss: 0.09446 \n",
      "[184/300] train_loss: 0.07082 valid_loss: 0.08460 test_loss: 0.09379 \n",
      "[185/300] train_loss: 0.07435 valid_loss: 0.08450 test_loss: 0.09371 \n",
      "[186/300] train_loss: 0.07159 valid_loss: 0.08479 test_loss: 0.09327 \n",
      "[187/300] train_loss: 0.07118 valid_loss: 0.08290 test_loss: 0.09310 \n",
      "Validation loss decreased (0.083896 --> 0.082902).  Saving model ...\n",
      "[188/300] train_loss: 0.07103 valid_loss: 0.08369 test_loss: 0.09416 \n",
      "[189/300] train_loss: 0.07172 valid_loss: 0.08351 test_loss: 0.09370 \n",
      "[190/300] train_loss: 0.07145 valid_loss: 0.08362 test_loss: 0.09266 \n",
      "[191/300] train_loss: 0.07011 valid_loss: 0.08424 test_loss: 0.09385 \n",
      "[192/300] train_loss: 0.07191 valid_loss: 0.08344 test_loss: 0.09374 \n",
      "[193/300] train_loss: 0.06896 valid_loss: 0.08329 test_loss: 0.09300 \n",
      "[194/300] train_loss: 0.07031 valid_loss: 0.08474 test_loss: 0.09304 \n",
      "[195/300] train_loss: 0.07032 valid_loss: 0.08369 test_loss: 0.09230 \n",
      "[196/300] train_loss: 0.06980 valid_loss: 0.08332 test_loss: 0.09229 \n",
      "[197/300] train_loss: 0.07201 valid_loss: 0.08421 test_loss: 0.09366 \n",
      "[198/300] train_loss: 0.07101 valid_loss: 0.08279 test_loss: 0.09199 \n",
      "Validation loss decreased (0.082902 --> 0.082787).  Saving model ...\n",
      "[199/300] train_loss: 0.07054 valid_loss: 0.08258 test_loss: 0.09297 \n",
      "Validation loss decreased (0.082787 --> 0.082579).  Saving model ...\n",
      "[200/300] train_loss: 0.06950 valid_loss: 0.08297 test_loss: 0.09298 \n",
      "[201/300] train_loss: 0.07073 valid_loss: 0.08397 test_loss: 0.09291 \n",
      "[202/300] train_loss: 0.06999 valid_loss: 0.08310 test_loss: 0.09242 \n",
      "[203/300] train_loss: 0.06891 valid_loss: 0.08235 test_loss: 0.09246 \n",
      "Validation loss decreased (0.082579 --> 0.082352).  Saving model ...\n",
      "[204/300] train_loss: 0.06901 valid_loss: 0.08161 test_loss: 0.09194 \n",
      "Validation loss decreased (0.082352 --> 0.081608).  Saving model ...\n",
      "[205/300] train_loss: 0.07104 valid_loss: 0.08306 test_loss: 0.09200 \n",
      "[206/300] train_loss: 0.06826 valid_loss: 0.08215 test_loss: 0.09103 \n",
      "[207/300] train_loss: 0.06884 valid_loss: 0.08373 test_loss: 0.09384 \n",
      "[208/300] train_loss: 0.07023 valid_loss: 0.08182 test_loss: 0.09187 \n",
      "[209/300] train_loss: 0.07188 valid_loss: 0.08230 test_loss: 0.09131 \n",
      "[210/300] train_loss: 0.06793 valid_loss: 0.08299 test_loss: 0.09239 \n",
      "[211/300] train_loss: 0.07067 valid_loss: 0.08389 test_loss: 0.09163 \n",
      "[212/300] train_loss: 0.06637 valid_loss: 0.08166 test_loss: 0.09109 \n",
      "[213/300] train_loss: 0.06849 valid_loss: 0.08144 test_loss: 0.08998 \n",
      "Validation loss decreased (0.081608 --> 0.081444).  Saving model ...\n",
      "[214/300] train_loss: 0.06746 valid_loss: 0.08297 test_loss: 0.09113 \n",
      "[215/300] train_loss: 0.06973 valid_loss: 0.08206 test_loss: 0.09186 \n",
      "[216/300] train_loss: 0.07060 valid_loss: 0.08137 test_loss: 0.09052 \n",
      "Validation loss decreased (0.081444 --> 0.081374).  Saving model ...\n",
      "[217/300] train_loss: 0.06621 valid_loss: 0.08148 test_loss: 0.09095 \n",
      "[218/300] train_loss: 0.06803 valid_loss: 0.08028 test_loss: 0.09009 \n",
      "Validation loss decreased (0.081374 --> 0.080282).  Saving model ...\n",
      "[219/300] train_loss: 0.06719 valid_loss: 0.08197 test_loss: 0.09228 \n",
      "[220/300] train_loss: 0.06811 valid_loss: 0.08268 test_loss: 0.09127 \n",
      "[221/300] train_loss: 0.06747 valid_loss: 0.08108 test_loss: 0.09203 \n",
      "[222/300] train_loss: 0.06819 valid_loss: 0.08208 test_loss: 0.09223 \n",
      "[223/300] train_loss: 0.06735 valid_loss: 0.08176 test_loss: 0.09080 \n",
      "[224/300] train_loss: 0.06889 valid_loss: 0.08252 test_loss: 0.09134 \n",
      "[225/300] train_loss: 0.06983 valid_loss: 0.08129 test_loss: 0.09093 \n",
      "[226/300] train_loss: 0.06798 valid_loss: 0.08075 test_loss: 0.09056 \n",
      "[227/300] train_loss: 0.06794 valid_loss: 0.08006 test_loss: 0.09139 \n",
      "Validation loss decreased (0.080282 --> 0.080065).  Saving model ...\n",
      "[228/300] train_loss: 0.06787 valid_loss: 0.07997 test_loss: 0.08963 \n",
      "Validation loss decreased (0.080065 --> 0.079971).  Saving model ...\n",
      "[229/300] train_loss: 0.06678 valid_loss: 0.08207 test_loss: 0.09144 \n",
      "[230/300] train_loss: 0.07083 valid_loss: 0.08057 test_loss: 0.09000 \n",
      "[231/300] train_loss: 0.06856 valid_loss: 0.08228 test_loss: 0.09093 \n",
      "[232/300] train_loss: 0.06652 valid_loss: 0.08127 test_loss: 0.08984 \n",
      "[233/300] train_loss: 0.06958 valid_loss: 0.08014 test_loss: 0.08861 \n",
      "[234/300] train_loss: 0.06658 valid_loss: 0.07982 test_loss: 0.09033 \n",
      "Validation loss decreased (0.079971 --> 0.079818).  Saving model ...\n",
      "[235/300] train_loss: 0.06744 valid_loss: 0.08058 test_loss: 0.09078 \n",
      "[236/300] train_loss: 0.06631 valid_loss: 0.08083 test_loss: 0.08972 \n",
      "[237/300] train_loss: 0.06454 valid_loss: 0.07975 test_loss: 0.08887 \n",
      "Validation loss decreased (0.079818 --> 0.079746).  Saving model ...\n",
      "[238/300] train_loss: 0.06616 valid_loss: 0.08321 test_loss: 0.09172 \n",
      "[239/300] train_loss: 0.06739 valid_loss: 0.08112 test_loss: 0.09003 \n",
      "[240/300] train_loss: 0.06647 valid_loss: 0.07909 test_loss: 0.08897 \n",
      "Validation loss decreased (0.079746 --> 0.079085).  Saving model ...\n",
      "[241/300] train_loss: 0.06647 valid_loss: 0.07996 test_loss: 0.08856 \n",
      "[242/300] train_loss: 0.06578 valid_loss: 0.08043 test_loss: 0.09015 \n",
      "[243/300] train_loss: 0.06587 valid_loss: 0.08054 test_loss: 0.09019 \n",
      "[244/300] train_loss: 0.06447 valid_loss: 0.08079 test_loss: 0.08954 \n",
      "[245/300] train_loss: 0.06438 valid_loss: 0.07974 test_loss: 0.08829 \n",
      "[246/300] train_loss: 0.06635 valid_loss: 0.07954 test_loss: 0.08954 \n",
      "[247/300] train_loss: 0.06793 valid_loss: 0.07953 test_loss: 0.08992 \n",
      "[248/300] train_loss: 0.06613 valid_loss: 0.08010 test_loss: 0.09060 \n",
      "[249/300] train_loss: 0.06531 valid_loss: 0.08129 test_loss: 0.09029 \n",
      "[250/300] train_loss: 0.06704 valid_loss: 0.07835 test_loss: 0.08814 \n",
      "Validation loss decreased (0.079085 --> 0.078349).  Saving model ...\n",
      "[251/300] train_loss: 0.06663 valid_loss: 0.08029 test_loss: 0.08821 \n",
      "[252/300] train_loss: 0.06570 valid_loss: 0.08018 test_loss: 0.08890 \n",
      "[253/300] train_loss: 0.06504 valid_loss: 0.07879 test_loss: 0.08883 \n",
      "[254/300] train_loss: 0.06407 valid_loss: 0.07898 test_loss: 0.08869 \n",
      "[255/300] train_loss: 0.06424 valid_loss: 0.07999 test_loss: 0.08915 \n",
      "[256/300] train_loss: 0.06589 valid_loss: 0.07841 test_loss: 0.08782 \n",
      "[257/300] train_loss: 0.06256 valid_loss: 0.07945 test_loss: 0.09013 \n",
      "[258/300] train_loss: 0.06421 valid_loss: 0.07910 test_loss: 0.08838 \n",
      "[259/300] train_loss: 0.06476 valid_loss: 0.07786 test_loss: 0.08687 \n",
      "Validation loss decreased (0.078349 --> 0.077865).  Saving model ...\n",
      "[260/300] train_loss: 0.06561 valid_loss: 0.07927 test_loss: 0.08750 \n",
      "[261/300] train_loss: 0.06409 valid_loss: 0.07844 test_loss: 0.08750 \n",
      "[262/300] train_loss: 0.06532 valid_loss: 0.07868 test_loss: 0.08817 \n",
      "[263/300] train_loss: 0.06647 valid_loss: 0.07997 test_loss: 0.08898 \n",
      "[264/300] train_loss: 0.06532 valid_loss: 0.07898 test_loss: 0.08871 \n",
      "[265/300] train_loss: 0.06352 valid_loss: 0.07820 test_loss: 0.08852 \n",
      "[266/300] train_loss: 0.06371 valid_loss: 0.07880 test_loss: 0.08896 \n",
      "[267/300] train_loss: 0.06240 valid_loss: 0.07988 test_loss: 0.08919 \n",
      "[268/300] train_loss: 0.06488 valid_loss: 0.08026 test_loss: 0.08819 \n",
      "[269/300] train_loss: 0.06251 valid_loss: 0.07858 test_loss: 0.08845 \n",
      "[270/300] train_loss: 0.06544 valid_loss: 0.07907 test_loss: 0.08979 \n",
      "[271/300] train_loss: 0.06398 valid_loss: 0.08009 test_loss: 0.08854 \n",
      "[272/300] train_loss: 0.06618 valid_loss: 0.07873 test_loss: 0.08742 \n",
      "[273/300] train_loss: 0.06462 valid_loss: 0.07806 test_loss: 0.08867 \n",
      "[274/300] train_loss: 0.06319 valid_loss: 0.07816 test_loss: 0.08775 \n",
      "[275/300] train_loss: 0.06801 valid_loss: 0.07950 test_loss: 0.08703 \n",
      "[276/300] train_loss: 0.06378 valid_loss: 0.07846 test_loss: 0.08707 \n",
      "[277/300] train_loss: 0.06336 valid_loss: 0.07836 test_loss: 0.08771 \n",
      "[278/300] train_loss: 0.06381 valid_loss: 0.07993 test_loss: 0.08952 \n",
      "[279/300] train_loss: 0.06312 valid_loss: 0.07909 test_loss: 0.08796 \n",
      "[280/300] train_loss: 0.06379 valid_loss: 0.07726 test_loss: 0.08780 \n",
      "Validation loss decreased (0.077865 --> 0.077264).  Saving model ...\n",
      "[281/300] train_loss: 0.06257 valid_loss: 0.07841 test_loss: 0.08869 \n",
      "[282/300] train_loss: 0.06394 valid_loss: 0.07887 test_loss: 0.08768 \n",
      "[283/300] train_loss: 0.06147 valid_loss: 0.07968 test_loss: 0.09048 \n",
      "[284/300] train_loss: 0.06310 valid_loss: 0.07747 test_loss: 0.08708 \n",
      "[285/300] train_loss: 0.06115 valid_loss: 0.07752 test_loss: 0.08767 \n",
      "[286/300] train_loss: 0.06505 valid_loss: 0.07737 test_loss: 0.08682 \n",
      "[287/300] train_loss: 0.06304 valid_loss: 0.07817 test_loss: 0.08649 \n",
      "[288/300] train_loss: 0.06273 valid_loss: 0.07876 test_loss: 0.08741 \n",
      "[289/300] train_loss: 0.06395 valid_loss: 0.07869 test_loss: 0.08845 \n",
      "[290/300] train_loss: 0.06250 valid_loss: 0.07719 test_loss: 0.08665 \n",
      "Validation loss decreased (0.077264 --> 0.077185).  Saving model ...\n",
      "[291/300] train_loss: 0.06375 valid_loss: 0.07870 test_loss: 0.08837 \n",
      "[292/300] train_loss: 0.06134 valid_loss: 0.07715 test_loss: 0.08804 \n",
      "Validation loss decreased (0.077185 --> 0.077154).  Saving model ...\n",
      "[293/300] train_loss: 0.06402 valid_loss: 0.07894 test_loss: 0.08731 \n",
      "[294/300] train_loss: 0.06369 valid_loss: 0.07903 test_loss: 0.08701 \n",
      "[295/300] train_loss: 0.06373 valid_loss: 0.07736 test_loss: 0.08696 \n",
      "[296/300] train_loss: 0.06397 valid_loss: 0.07776 test_loss: 0.08613 \n",
      "[297/300] train_loss: 0.06131 valid_loss: 0.07747 test_loss: 0.08793 \n",
      "[298/300] train_loss: 0.05940 valid_loss: 0.07729 test_loss: 0.08686 \n",
      "[299/300] train_loss: 0.06189 valid_loss: 0.07648 test_loss: 0.08621 \n",
      "Validation loss decreased (0.077154 --> 0.076484).  Saving model ...\n",
      "[300/300] train_loss: 0.06307 valid_loss: 0.07680 test_loss: 0.08683 \n",
      "TRAINING MODEL 2\n",
      "[  1/300] train_loss: 0.63621 valid_loss: 0.53143 test_loss: 0.53825 \n",
      "Validation loss decreased (inf --> 0.531427).  Saving model ...\n",
      "[  2/300] train_loss: 0.42848 valid_loss: 0.37876 test_loss: 0.38575 \n",
      "Validation loss decreased (0.531427 --> 0.378761).  Saving model ...\n",
      "[  3/300] train_loss: 0.31357 valid_loss: 0.30587 test_loss: 0.31460 \n",
      "Validation loss decreased (0.378761 --> 0.305865).  Saving model ...\n",
      "[  4/300] train_loss: 0.25819 valid_loss: 0.25648 test_loss: 0.26838 \n",
      "Validation loss decreased (0.305865 --> 0.256478).  Saving model ...\n",
      "[  5/300] train_loss: 0.22444 valid_loss: 0.23191 test_loss: 0.24368 \n",
      "Validation loss decreased (0.256478 --> 0.231907).  Saving model ...\n",
      "[  6/300] train_loss: 0.20536 valid_loss: 0.21212 test_loss: 0.22538 \n",
      "Validation loss decreased (0.231907 --> 0.212119).  Saving model ...\n",
      "[  7/300] train_loss: 0.19618 valid_loss: 0.20004 test_loss: 0.21496 \n",
      "Validation loss decreased (0.212119 --> 0.200042).  Saving model ...\n",
      "[  8/300] train_loss: 0.18492 valid_loss: 0.19127 test_loss: 0.20526 \n",
      "Validation loss decreased (0.200042 --> 0.191269).  Saving model ...\n",
      "[  9/300] train_loss: 0.17937 valid_loss: 0.18337 test_loss: 0.19651 \n",
      "Validation loss decreased (0.191269 --> 0.183365).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17239 valid_loss: 0.17379 test_loss: 0.18684 \n",
      "Validation loss decreased (0.183365 --> 0.173790).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16926 valid_loss: 0.17232 test_loss: 0.18657 \n",
      "Validation loss decreased (0.173790 --> 0.172323).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16310 valid_loss: 0.16612 test_loss: 0.17969 \n",
      "Validation loss decreased (0.172323 --> 0.166115).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15967 valid_loss: 0.15951 test_loss: 0.17361 \n",
      "Validation loss decreased (0.166115 --> 0.159514).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15457 valid_loss: 0.15793 test_loss: 0.16680 \n",
      "Validation loss decreased (0.159514 --> 0.157932).  Saving model ...\n",
      "[ 15/300] train_loss: 0.15131 valid_loss: 0.15783 test_loss: 0.16932 \n",
      "Validation loss decreased (0.157932 --> 0.157833).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14589 valid_loss: 0.15322 test_loss: 0.16257 \n",
      "Validation loss decreased (0.157833 --> 0.153220).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13919 valid_loss: 0.14969 test_loss: 0.15985 \n",
      "Validation loss decreased (0.153220 --> 0.149693).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14078 valid_loss: 0.14970 test_loss: 0.16119 \n",
      "[ 19/300] train_loss: 0.14046 valid_loss: 0.14901 test_loss: 0.16126 \n",
      "Validation loss decreased (0.149693 --> 0.149008).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13954 valid_loss: 0.14370 test_loss: 0.15370 \n",
      "Validation loss decreased (0.149008 --> 0.143704).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13780 valid_loss: 0.14305 test_loss: 0.15443 \n",
      "Validation loss decreased (0.143704 --> 0.143050).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13428 valid_loss: 0.13994 test_loss: 0.15328 \n",
      "Validation loss decreased (0.143050 --> 0.139940).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12965 valid_loss: 0.14021 test_loss: 0.15347 \n",
      "[ 24/300] train_loss: 0.12864 valid_loss: 0.13679 test_loss: 0.14894 \n",
      "Validation loss decreased (0.139940 --> 0.136792).  Saving model ...\n",
      "[ 25/300] train_loss: 0.13062 valid_loss: 0.13582 test_loss: 0.14900 \n",
      "Validation loss decreased (0.136792 --> 0.135824).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12390 valid_loss: 0.13536 test_loss: 0.14698 \n",
      "Validation loss decreased (0.135824 --> 0.135357).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12500 valid_loss: 0.13509 test_loss: 0.14755 \n",
      "Validation loss decreased (0.135357 --> 0.135087).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12433 valid_loss: 0.13030 test_loss: 0.14269 \n",
      "Validation loss decreased (0.135087 --> 0.130304).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12470 valid_loss: 0.12858 test_loss: 0.14175 \n",
      "Validation loss decreased (0.130304 --> 0.128580).  Saving model ...\n",
      "[ 30/300] train_loss: 0.12032 valid_loss: 0.12930 test_loss: 0.14236 \n",
      "[ 31/300] train_loss: 0.11986 valid_loss: 0.12730 test_loss: 0.14081 \n",
      "Validation loss decreased (0.128580 --> 0.127301).  Saving model ...\n",
      "[ 32/300] train_loss: 0.12082 valid_loss: 0.12912 test_loss: 0.13987 \n",
      "[ 33/300] train_loss: 0.11803 valid_loss: 0.12557 test_loss: 0.13746 \n",
      "Validation loss decreased (0.127301 --> 0.125567).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11842 valid_loss: 0.12686 test_loss: 0.13912 \n",
      "[ 35/300] train_loss: 0.11522 valid_loss: 0.12676 test_loss: 0.13871 \n",
      "[ 36/300] train_loss: 0.11352 valid_loss: 0.12220 test_loss: 0.13566 \n",
      "Validation loss decreased (0.125567 --> 0.122201).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11480 valid_loss: 0.12157 test_loss: 0.13637 \n",
      "Validation loss decreased (0.122201 --> 0.121574).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11203 valid_loss: 0.12096 test_loss: 0.13450 \n",
      "Validation loss decreased (0.121574 --> 0.120960).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11110 valid_loss: 0.11849 test_loss: 0.13120 \n",
      "Validation loss decreased (0.120960 --> 0.118493).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11156 valid_loss: 0.11697 test_loss: 0.13012 \n",
      "Validation loss decreased (0.118493 --> 0.116970).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11206 valid_loss: 0.11883 test_loss: 0.13164 \n",
      "[ 42/300] train_loss: 0.11408 valid_loss: 0.11862 test_loss: 0.13003 \n",
      "[ 43/300] train_loss: 0.11004 valid_loss: 0.11570 test_loss: 0.12768 \n",
      "Validation loss decreased (0.116970 --> 0.115698).  Saving model ...\n",
      "[ 44/300] train_loss: 0.11202 valid_loss: 0.11564 test_loss: 0.12873 \n",
      "Validation loss decreased (0.115698 --> 0.115638).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10857 valid_loss: 0.11758 test_loss: 0.13068 \n",
      "[ 46/300] train_loss: 0.10560 valid_loss: 0.11629 test_loss: 0.12931 \n",
      "[ 47/300] train_loss: 0.10773 valid_loss: 0.11284 test_loss: 0.12597 \n",
      "Validation loss decreased (0.115638 --> 0.112836).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10739 valid_loss: 0.11407 test_loss: 0.12546 \n",
      "[ 49/300] train_loss: 0.10597 valid_loss: 0.11504 test_loss: 0.12726 \n",
      "[ 50/300] train_loss: 0.10726 valid_loss: 0.11317 test_loss: 0.12403 \n",
      "[ 51/300] train_loss: 0.10490 valid_loss: 0.11156 test_loss: 0.12514 \n",
      "Validation loss decreased (0.112836 --> 0.111559).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10636 valid_loss: 0.11319 test_loss: 0.12462 \n",
      "[ 53/300] train_loss: 0.10133 valid_loss: 0.11107 test_loss: 0.12268 \n",
      "Validation loss decreased (0.111559 --> 0.111071).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10200 valid_loss: 0.11164 test_loss: 0.12252 \n",
      "[ 55/300] train_loss: 0.10254 valid_loss: 0.10917 test_loss: 0.12217 \n",
      "Validation loss decreased (0.111071 --> 0.109169).  Saving model ...\n",
      "[ 56/300] train_loss: 0.10094 valid_loss: 0.10913 test_loss: 0.12074 \n",
      "Validation loss decreased (0.109169 --> 0.109135).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09796 valid_loss: 0.11061 test_loss: 0.12128 \n",
      "[ 58/300] train_loss: 0.10111 valid_loss: 0.10557 test_loss: 0.11886 \n",
      "Validation loss decreased (0.109135 --> 0.105567).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09741 valid_loss: 0.10703 test_loss: 0.12049 \n",
      "[ 60/300] train_loss: 0.09792 valid_loss: 0.10832 test_loss: 0.11923 \n",
      "[ 61/300] train_loss: 0.09914 valid_loss: 0.10744 test_loss: 0.11933 \n",
      "[ 62/300] train_loss: 0.09727 valid_loss: 0.10614 test_loss: 0.11874 \n",
      "[ 63/300] train_loss: 0.09791 valid_loss: 0.10447 test_loss: 0.11619 \n",
      "Validation loss decreased (0.105567 --> 0.104475).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09962 valid_loss: 0.10355 test_loss: 0.11585 \n",
      "Validation loss decreased (0.104475 --> 0.103548).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09735 valid_loss: 0.10451 test_loss: 0.11598 \n",
      "[ 66/300] train_loss: 0.09273 valid_loss: 0.10405 test_loss: 0.11608 \n",
      "[ 67/300] train_loss: 0.09465 valid_loss: 0.10586 test_loss: 0.11484 \n",
      "[ 68/300] train_loss: 0.09655 valid_loss: 0.10131 test_loss: 0.11365 \n",
      "Validation loss decreased (0.103548 --> 0.101315).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09646 valid_loss: 0.10249 test_loss: 0.11334 \n",
      "[ 70/300] train_loss: 0.09538 valid_loss: 0.10497 test_loss: 0.11621 \n",
      "[ 71/300] train_loss: 0.09519 valid_loss: 0.09892 test_loss: 0.11232 \n",
      "Validation loss decreased (0.101315 --> 0.098920).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09432 valid_loss: 0.10111 test_loss: 0.11373 \n",
      "[ 73/300] train_loss: 0.09306 valid_loss: 0.09938 test_loss: 0.11171 \n",
      "[ 74/300] train_loss: 0.09324 valid_loss: 0.09945 test_loss: 0.11319 \n",
      "[ 75/300] train_loss: 0.09142 valid_loss: 0.10124 test_loss: 0.11355 \n",
      "[ 76/300] train_loss: 0.09193 valid_loss: 0.09928 test_loss: 0.11219 \n",
      "[ 77/300] train_loss: 0.09471 valid_loss: 0.09673 test_loss: 0.10987 \n",
      "Validation loss decreased (0.098920 --> 0.096730).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09289 valid_loss: 0.09884 test_loss: 0.11069 \n",
      "[ 79/300] train_loss: 0.09125 valid_loss: 0.10068 test_loss: 0.11199 \n",
      "[ 80/300] train_loss: 0.09275 valid_loss: 0.09811 test_loss: 0.11053 \n",
      "[ 81/300] train_loss: 0.09065 valid_loss: 0.09814 test_loss: 0.10998 \n",
      "[ 82/300] train_loss: 0.08915 valid_loss: 0.09946 test_loss: 0.11160 \n",
      "[ 83/300] train_loss: 0.09015 valid_loss: 0.09917 test_loss: 0.11057 \n",
      "[ 84/300] train_loss: 0.08988 valid_loss: 0.09806 test_loss: 0.10963 \n",
      "[ 85/300] train_loss: 0.09162 valid_loss: 0.09628 test_loss: 0.10834 \n",
      "Validation loss decreased (0.096730 --> 0.096282).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08793 valid_loss: 0.09576 test_loss: 0.10785 \n",
      "Validation loss decreased (0.096282 --> 0.095756).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08912 valid_loss: 0.09560 test_loss: 0.10793 \n",
      "Validation loss decreased (0.095756 --> 0.095602).  Saving model ...\n",
      "[ 88/300] train_loss: 0.09016 valid_loss: 0.09755 test_loss: 0.10882 \n",
      "[ 89/300] train_loss: 0.08774 valid_loss: 0.09735 test_loss: 0.10794 \n",
      "[ 90/300] train_loss: 0.08805 valid_loss: 0.09498 test_loss: 0.10664 \n",
      "Validation loss decreased (0.095602 --> 0.094983).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08675 valid_loss: 0.09488 test_loss: 0.10655 \n",
      "Validation loss decreased (0.094983 --> 0.094884).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08681 valid_loss: 0.09524 test_loss: 0.10744 \n",
      "[ 93/300] train_loss: 0.08763 valid_loss: 0.09605 test_loss: 0.10790 \n",
      "[ 94/300] train_loss: 0.08804 valid_loss: 0.09467 test_loss: 0.10651 \n",
      "Validation loss decreased (0.094884 --> 0.094669).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08567 valid_loss: 0.09419 test_loss: 0.10654 \n",
      "Validation loss decreased (0.094669 --> 0.094189).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08687 valid_loss: 0.09283 test_loss: 0.10573 \n",
      "Validation loss decreased (0.094189 --> 0.092830).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08620 valid_loss: 0.09400 test_loss: 0.10675 \n",
      "[ 98/300] train_loss: 0.08369 valid_loss: 0.09359 test_loss: 0.10593 \n",
      "[ 99/300] train_loss: 0.08485 valid_loss: 0.09242 test_loss: 0.10546 \n",
      "Validation loss decreased (0.092830 --> 0.092418).  Saving model ...\n",
      "[100/300] train_loss: 0.08514 valid_loss: 0.09235 test_loss: 0.10595 \n",
      "Validation loss decreased (0.092418 --> 0.092346).  Saving model ...\n",
      "[101/300] train_loss: 0.08447 valid_loss: 0.09310 test_loss: 0.10427 \n",
      "[102/300] train_loss: 0.08422 valid_loss: 0.09183 test_loss: 0.10413 \n",
      "Validation loss decreased (0.092346 --> 0.091828).  Saving model ...\n",
      "[103/300] train_loss: 0.08527 valid_loss: 0.09083 test_loss: 0.10379 \n",
      "Validation loss decreased (0.091828 --> 0.090830).  Saving model ...\n",
      "[104/300] train_loss: 0.08273 valid_loss: 0.09768 test_loss: 0.10694 \n",
      "[105/300] train_loss: 0.08616 valid_loss: 0.09267 test_loss: 0.10397 \n",
      "[106/300] train_loss: 0.08359 valid_loss: 0.09070 test_loss: 0.10400 \n",
      "Validation loss decreased (0.090830 --> 0.090696).  Saving model ...\n",
      "[107/300] train_loss: 0.08217 valid_loss: 0.09125 test_loss: 0.10338 \n",
      "[108/300] train_loss: 0.08540 valid_loss: 0.09157 test_loss: 0.10320 \n",
      "[109/300] train_loss: 0.08629 valid_loss: 0.09064 test_loss: 0.10259 \n",
      "Validation loss decreased (0.090696 --> 0.090636).  Saving model ...\n",
      "[110/300] train_loss: 0.08353 valid_loss: 0.08998 test_loss: 0.10315 \n",
      "Validation loss decreased (0.090636 --> 0.089980).  Saving model ...\n",
      "[111/300] train_loss: 0.08404 valid_loss: 0.09128 test_loss: 0.10187 \n",
      "[112/300] train_loss: 0.08235 valid_loss: 0.08994 test_loss: 0.10325 \n",
      "Validation loss decreased (0.089980 --> 0.089937).  Saving model ...\n",
      "[113/300] train_loss: 0.08236 valid_loss: 0.08990 test_loss: 0.10276 \n",
      "Validation loss decreased (0.089937 --> 0.089900).  Saving model ...\n",
      "[114/300] train_loss: 0.08251 valid_loss: 0.08848 test_loss: 0.10140 \n",
      "Validation loss decreased (0.089900 --> 0.088482).  Saving model ...\n",
      "[115/300] train_loss: 0.08302 valid_loss: 0.08989 test_loss: 0.10322 \n",
      "[116/300] train_loss: 0.07982 valid_loss: 0.08973 test_loss: 0.10251 \n",
      "[117/300] train_loss: 0.08319 valid_loss: 0.08888 test_loss: 0.10111 \n",
      "[118/300] train_loss: 0.07960 valid_loss: 0.08946 test_loss: 0.10058 \n",
      "[119/300] train_loss: 0.08006 valid_loss: 0.08990 test_loss: 0.10133 \n",
      "[120/300] train_loss: 0.08181 valid_loss: 0.08967 test_loss: 0.10063 \n",
      "[121/300] train_loss: 0.08421 valid_loss: 0.09142 test_loss: 0.10189 \n",
      "[122/300] train_loss: 0.08047 valid_loss: 0.08898 test_loss: 0.10151 \n",
      "[123/300] train_loss: 0.08042 valid_loss: 0.08801 test_loss: 0.10108 \n",
      "Validation loss decreased (0.088482 --> 0.088011).  Saving model ...\n",
      "[124/300] train_loss: 0.07976 valid_loss: 0.09032 test_loss: 0.10063 \n",
      "[125/300] train_loss: 0.07885 valid_loss: 0.08721 test_loss: 0.09910 \n",
      "Validation loss decreased (0.088011 --> 0.087205).  Saving model ...\n",
      "[126/300] train_loss: 0.08043 valid_loss: 0.08858 test_loss: 0.10021 \n",
      "[127/300] train_loss: 0.07941 valid_loss: 0.08801 test_loss: 0.10014 \n",
      "[128/300] train_loss: 0.07965 valid_loss: 0.08974 test_loss: 0.10047 \n",
      "[129/300] train_loss: 0.08050 valid_loss: 0.08750 test_loss: 0.09963 \n",
      "[130/300] train_loss: 0.07825 valid_loss: 0.08642 test_loss: 0.09974 \n",
      "Validation loss decreased (0.087205 --> 0.086422).  Saving model ...\n",
      "[131/300] train_loss: 0.07634 valid_loss: 0.08900 test_loss: 0.10100 \n",
      "[132/300] train_loss: 0.07857 valid_loss: 0.08650 test_loss: 0.09971 \n",
      "[133/300] train_loss: 0.07988 valid_loss: 0.08679 test_loss: 0.09965 \n",
      "[134/300] train_loss: 0.07524 valid_loss: 0.08561 test_loss: 0.09852 \n",
      "Validation loss decreased (0.086422 --> 0.085614).  Saving model ...\n",
      "[135/300] train_loss: 0.07784 valid_loss: 0.09092 test_loss: 0.10100 \n",
      "[136/300] train_loss: 0.07621 valid_loss: 0.08561 test_loss: 0.09945 \n",
      "Validation loss decreased (0.085614 --> 0.085612).  Saving model ...\n",
      "[137/300] train_loss: 0.07818 valid_loss: 0.08944 test_loss: 0.10043 \n",
      "[138/300] train_loss: 0.07721 valid_loss: 0.08784 test_loss: 0.09899 \n",
      "[139/300] train_loss: 0.07703 valid_loss: 0.08921 test_loss: 0.10061 \n",
      "[140/300] train_loss: 0.07700 valid_loss: 0.08596 test_loss: 0.09735 \n",
      "[141/300] train_loss: 0.07871 valid_loss: 0.08564 test_loss: 0.09840 \n",
      "[142/300] train_loss: 0.07685 valid_loss: 0.08458 test_loss: 0.09690 \n",
      "Validation loss decreased (0.085612 --> 0.084580).  Saving model ...\n",
      "[143/300] train_loss: 0.07849 valid_loss: 0.08377 test_loss: 0.09657 \n",
      "Validation loss decreased (0.084580 --> 0.083770).  Saving model ...\n",
      "[144/300] train_loss: 0.07569 valid_loss: 0.08509 test_loss: 0.09840 \n",
      "[145/300] train_loss: 0.07546 valid_loss: 0.08566 test_loss: 0.09846 \n",
      "[146/300] train_loss: 0.07835 valid_loss: 0.08539 test_loss: 0.09749 \n",
      "[147/300] train_loss: 0.07594 valid_loss: 0.08484 test_loss: 0.09666 \n",
      "[148/300] train_loss: 0.07571 valid_loss: 0.08438 test_loss: 0.09769 \n",
      "[149/300] train_loss: 0.07465 valid_loss: 0.08548 test_loss: 0.09783 \n",
      "[150/300] train_loss: 0.07633 valid_loss: 0.08524 test_loss: 0.09740 \n",
      "[151/300] train_loss: 0.07683 valid_loss: 0.08523 test_loss: 0.09705 \n",
      "[152/300] train_loss: 0.07611 valid_loss: 0.08298 test_loss: 0.09664 \n",
      "Validation loss decreased (0.083770 --> 0.082984).  Saving model ...\n",
      "[153/300] train_loss: 0.07609 valid_loss: 0.08434 test_loss: 0.09786 \n",
      "[154/300] train_loss: 0.07714 valid_loss: 0.08414 test_loss: 0.09629 \n",
      "[155/300] train_loss: 0.07394 valid_loss: 0.08394 test_loss: 0.09618 \n",
      "[156/300] train_loss: 0.07672 valid_loss: 0.08288 test_loss: 0.09517 \n",
      "Validation loss decreased (0.082984 --> 0.082877).  Saving model ...\n",
      "[157/300] train_loss: 0.07636 valid_loss: 0.08408 test_loss: 0.09722 \n",
      "[158/300] train_loss: 0.07599 valid_loss: 0.08490 test_loss: 0.09763 \n",
      "[159/300] train_loss: 0.07205 valid_loss: 0.08393 test_loss: 0.09716 \n",
      "[160/300] train_loss: 0.07687 valid_loss: 0.08386 test_loss: 0.09692 \n",
      "[161/300] train_loss: 0.07398 valid_loss: 0.08324 test_loss: 0.09611 \n",
      "[162/300] train_loss: 0.07381 valid_loss: 0.08306 test_loss: 0.09720 \n",
      "[163/300] train_loss: 0.07175 valid_loss: 0.08474 test_loss: 0.09732 \n",
      "[164/300] train_loss: 0.07163 valid_loss: 0.08359 test_loss: 0.09507 \n",
      "[165/300] train_loss: 0.07427 valid_loss: 0.08442 test_loss: 0.09659 \n",
      "[166/300] train_loss: 0.07374 valid_loss: 0.08340 test_loss: 0.09539 \n",
      "[167/300] train_loss: 0.07311 valid_loss: 0.08242 test_loss: 0.09484 \n",
      "Validation loss decreased (0.082877 --> 0.082422).  Saving model ...\n",
      "[168/300] train_loss: 0.07316 valid_loss: 0.08172 test_loss: 0.09508 \n",
      "Validation loss decreased (0.082422 --> 0.081716).  Saving model ...\n",
      "[169/300] train_loss: 0.07364 valid_loss: 0.08294 test_loss: 0.09560 \n",
      "[170/300] train_loss: 0.07267 valid_loss: 0.08100 test_loss: 0.09435 \n",
      "Validation loss decreased (0.081716 --> 0.081005).  Saving model ...\n",
      "[171/300] train_loss: 0.07208 valid_loss: 0.08160 test_loss: 0.09592 \n",
      "[172/300] train_loss: 0.07239 valid_loss: 0.08205 test_loss: 0.09467 \n",
      "[173/300] train_loss: 0.07192 valid_loss: 0.08171 test_loss: 0.09537 \n",
      "[174/300] train_loss: 0.07331 valid_loss: 0.08050 test_loss: 0.09328 \n",
      "Validation loss decreased (0.081005 --> 0.080504).  Saving model ...\n",
      "[175/300] train_loss: 0.07253 valid_loss: 0.08020 test_loss: 0.09374 \n",
      "Validation loss decreased (0.080504 --> 0.080202).  Saving model ...\n",
      "[176/300] train_loss: 0.07082 valid_loss: 0.08257 test_loss: 0.09510 \n",
      "[177/300] train_loss: 0.07019 valid_loss: 0.08421 test_loss: 0.09676 \n",
      "[178/300] train_loss: 0.07146 valid_loss: 0.08096 test_loss: 0.09502 \n",
      "[179/300] train_loss: 0.07009 valid_loss: 0.08133 test_loss: 0.09583 \n",
      "[180/300] train_loss: 0.07039 valid_loss: 0.08446 test_loss: 0.09544 \n",
      "[181/300] train_loss: 0.07275 valid_loss: 0.08121 test_loss: 0.09397 \n",
      "[182/300] train_loss: 0.07169 valid_loss: 0.08200 test_loss: 0.09382 \n",
      "[183/300] train_loss: 0.07280 valid_loss: 0.08224 test_loss: 0.09479 \n",
      "[184/300] train_loss: 0.07194 valid_loss: 0.08202 test_loss: 0.09462 \n",
      "[185/300] train_loss: 0.07128 valid_loss: 0.08159 test_loss: 0.09434 \n",
      "[186/300] train_loss: 0.07120 valid_loss: 0.08264 test_loss: 0.09442 \n",
      "[187/300] train_loss: 0.07174 valid_loss: 0.08259 test_loss: 0.09551 \n",
      "[188/300] train_loss: 0.07261 valid_loss: 0.08072 test_loss: 0.09273 \n",
      "[189/300] train_loss: 0.07223 valid_loss: 0.08065 test_loss: 0.09314 \n",
      "[190/300] train_loss: 0.07169 valid_loss: 0.08118 test_loss: 0.09409 \n",
      "[191/300] train_loss: 0.06892 valid_loss: 0.08108 test_loss: 0.09429 \n",
      "[192/300] train_loss: 0.07073 valid_loss: 0.07994 test_loss: 0.09387 \n",
      "Validation loss decreased (0.080202 --> 0.079942).  Saving model ...\n",
      "[193/300] train_loss: 0.07287 valid_loss: 0.08130 test_loss: 0.09388 \n",
      "[194/300] train_loss: 0.07075 valid_loss: 0.08008 test_loss: 0.09215 \n",
      "[195/300] train_loss: 0.07021 valid_loss: 0.07933 test_loss: 0.09276 \n",
      "Validation loss decreased (0.079942 --> 0.079331).  Saving model ...\n",
      "[196/300] train_loss: 0.06826 valid_loss: 0.07993 test_loss: 0.09321 \n",
      "[197/300] train_loss: 0.07138 valid_loss: 0.08221 test_loss: 0.09280 \n",
      "[198/300] train_loss: 0.06914 valid_loss: 0.07975 test_loss: 0.09232 \n",
      "[199/300] train_loss: 0.06989 valid_loss: 0.07973 test_loss: 0.09241 \n",
      "[200/300] train_loss: 0.07125 valid_loss: 0.07872 test_loss: 0.09170 \n",
      "Validation loss decreased (0.079331 --> 0.078716).  Saving model ...\n",
      "[201/300] train_loss: 0.07092 valid_loss: 0.08044 test_loss: 0.09232 \n",
      "[202/300] train_loss: 0.06847 valid_loss: 0.08009 test_loss: 0.09290 \n",
      "[203/300] train_loss: 0.06794 valid_loss: 0.08038 test_loss: 0.09474 \n",
      "[204/300] train_loss: 0.07023 valid_loss: 0.08073 test_loss: 0.09210 \n",
      "[205/300] train_loss: 0.06574 valid_loss: 0.07964 test_loss: 0.09254 \n",
      "[206/300] train_loss: 0.07019 valid_loss: 0.07997 test_loss: 0.09303 \n",
      "[207/300] train_loss: 0.06843 valid_loss: 0.07979 test_loss: 0.09222 \n",
      "[208/300] train_loss: 0.07007 valid_loss: 0.07950 test_loss: 0.09221 \n",
      "[209/300] train_loss: 0.06689 valid_loss: 0.08144 test_loss: 0.09486 \n",
      "[210/300] train_loss: 0.06688 valid_loss: 0.07824 test_loss: 0.09104 \n",
      "Validation loss decreased (0.078716 --> 0.078237).  Saving model ...\n",
      "[211/300] train_loss: 0.06842 valid_loss: 0.07804 test_loss: 0.09112 \n",
      "Validation loss decreased (0.078237 --> 0.078043).  Saving model ...\n",
      "[212/300] train_loss: 0.06903 valid_loss: 0.07910 test_loss: 0.09145 \n",
      "[213/300] train_loss: 0.06894 valid_loss: 0.07926 test_loss: 0.09223 \n",
      "[214/300] train_loss: 0.06935 valid_loss: 0.07808 test_loss: 0.09119 \n",
      "[215/300] train_loss: 0.06730 valid_loss: 0.07891 test_loss: 0.09209 \n",
      "[216/300] train_loss: 0.06721 valid_loss: 0.07925 test_loss: 0.09151 \n",
      "[217/300] train_loss: 0.06861 valid_loss: 0.08173 test_loss: 0.09439 \n",
      "[218/300] train_loss: 0.06540 valid_loss: 0.07849 test_loss: 0.09234 \n",
      "[219/300] train_loss: 0.06644 valid_loss: 0.07884 test_loss: 0.09143 \n",
      "[220/300] train_loss: 0.06721 valid_loss: 0.07890 test_loss: 0.09041 \n",
      "[221/300] train_loss: 0.06638 valid_loss: 0.07793 test_loss: 0.09087 \n",
      "Validation loss decreased (0.078043 --> 0.077928).  Saving model ...\n",
      "[222/300] train_loss: 0.06750 valid_loss: 0.07800 test_loss: 0.08997 \n",
      "[223/300] train_loss: 0.06678 valid_loss: 0.07980 test_loss: 0.09295 \n",
      "[224/300] train_loss: 0.06619 valid_loss: 0.07735 test_loss: 0.09054 \n",
      "Validation loss decreased (0.077928 --> 0.077353).  Saving model ...\n",
      "[225/300] train_loss: 0.06618 valid_loss: 0.07845 test_loss: 0.09226 \n",
      "[226/300] train_loss: 0.06407 valid_loss: 0.07745 test_loss: 0.09090 \n",
      "[227/300] train_loss: 0.06717 valid_loss: 0.07842 test_loss: 0.09130 \n",
      "[228/300] train_loss: 0.06524 valid_loss: 0.07783 test_loss: 0.09178 \n",
      "[229/300] train_loss: 0.06675 valid_loss: 0.07722 test_loss: 0.09001 \n",
      "Validation loss decreased (0.077353 --> 0.077225).  Saving model ...\n",
      "[230/300] train_loss: 0.06926 valid_loss: 0.07726 test_loss: 0.09051 \n",
      "[231/300] train_loss: 0.06614 valid_loss: 0.07886 test_loss: 0.08985 \n",
      "[232/300] train_loss: 0.06658 valid_loss: 0.07708 test_loss: 0.08996 \n",
      "Validation loss decreased (0.077225 --> 0.077085).  Saving model ...\n",
      "[233/300] train_loss: 0.06579 valid_loss: 0.07887 test_loss: 0.09049 \n",
      "[234/300] train_loss: 0.06454 valid_loss: 0.07840 test_loss: 0.09083 \n",
      "[235/300] train_loss: 0.06633 valid_loss: 0.07917 test_loss: 0.09189 \n",
      "[236/300] train_loss: 0.06709 valid_loss: 0.07737 test_loss: 0.08866 \n",
      "[237/300] train_loss: 0.06388 valid_loss: 0.07783 test_loss: 0.09033 \n",
      "[238/300] train_loss: 0.06749 valid_loss: 0.07793 test_loss: 0.09004 \n",
      "[239/300] train_loss: 0.06686 valid_loss: 0.07760 test_loss: 0.08964 \n",
      "[240/300] train_loss: 0.06648 valid_loss: 0.07851 test_loss: 0.08960 \n",
      "[241/300] train_loss: 0.06435 valid_loss: 0.07793 test_loss: 0.08943 \n",
      "[242/300] train_loss: 0.06731 valid_loss: 0.07628 test_loss: 0.08928 \n",
      "Validation loss decreased (0.077085 --> 0.076275).  Saving model ...\n",
      "[243/300] train_loss: 0.06467 valid_loss: 0.07817 test_loss: 0.09008 \n",
      "[244/300] train_loss: 0.06469 valid_loss: 0.07723 test_loss: 0.08945 \n",
      "[245/300] train_loss: 0.06482 valid_loss: 0.07714 test_loss: 0.09141 \n",
      "[246/300] train_loss: 0.06814 valid_loss: 0.07958 test_loss: 0.09255 \n",
      "[247/300] train_loss: 0.06447 valid_loss: 0.07675 test_loss: 0.09062 \n",
      "[248/300] train_loss: 0.06433 valid_loss: 0.07770 test_loss: 0.08970 \n",
      "[249/300] train_loss: 0.06497 valid_loss: 0.07739 test_loss: 0.08925 \n",
      "[250/300] train_loss: 0.06496 valid_loss: 0.07635 test_loss: 0.09019 \n",
      "[251/300] train_loss: 0.06538 valid_loss: 0.07689 test_loss: 0.08945 \n",
      "[252/300] train_loss: 0.06501 valid_loss: 0.07790 test_loss: 0.09038 \n",
      "[253/300] train_loss: 0.06493 valid_loss: 0.07817 test_loss: 0.09053 \n",
      "[254/300] train_loss: 0.06413 valid_loss: 0.07618 test_loss: 0.08981 \n",
      "Validation loss decreased (0.076275 --> 0.076177).  Saving model ...\n",
      "[255/300] train_loss: 0.06451 valid_loss: 0.07657 test_loss: 0.08890 \n",
      "[256/300] train_loss: 0.06569 valid_loss: 0.08242 test_loss: 0.09172 \n",
      "[257/300] train_loss: 0.06501 valid_loss: 0.07691 test_loss: 0.08860 \n",
      "[258/300] train_loss: 0.06418 valid_loss: 0.07655 test_loss: 0.08946 \n",
      "[259/300] train_loss: 0.06321 valid_loss: 0.07631 test_loss: 0.09102 \n",
      "[260/300] train_loss: 0.06403 valid_loss: 0.07801 test_loss: 0.09010 \n",
      "[261/300] train_loss: 0.06553 valid_loss: 0.07567 test_loss: 0.08877 \n",
      "Validation loss decreased (0.076177 --> 0.075672).  Saving model ...\n",
      "[262/300] train_loss: 0.06476 valid_loss: 0.07591 test_loss: 0.08951 \n",
      "[263/300] train_loss: 0.06404 valid_loss: 0.07665 test_loss: 0.09007 \n",
      "[264/300] train_loss: 0.06581 valid_loss: 0.07618 test_loss: 0.08957 \n",
      "[265/300] train_loss: 0.06268 valid_loss: 0.07586 test_loss: 0.08953 \n",
      "[266/300] train_loss: 0.06381 valid_loss: 0.07724 test_loss: 0.09004 \n",
      "[267/300] train_loss: 0.06257 valid_loss: 0.07619 test_loss: 0.08932 \n",
      "[268/300] train_loss: 0.06401 valid_loss: 0.07653 test_loss: 0.08934 \n",
      "[269/300] train_loss: 0.06307 valid_loss: 0.07631 test_loss: 0.08864 \n",
      "[270/300] train_loss: 0.06463 valid_loss: 0.07669 test_loss: 0.08988 \n",
      "[271/300] train_loss: 0.06331 valid_loss: 0.07555 test_loss: 0.08807 \n",
      "Validation loss decreased (0.075672 --> 0.075553).  Saving model ...\n",
      "[272/300] train_loss: 0.06253 valid_loss: 0.07516 test_loss: 0.08903 \n",
      "Validation loss decreased (0.075553 --> 0.075159).  Saving model ...\n",
      "[273/300] train_loss: 0.06178 valid_loss: 0.07470 test_loss: 0.08863 \n",
      "Validation loss decreased (0.075159 --> 0.074695).  Saving model ...\n",
      "[274/300] train_loss: 0.06272 valid_loss: 0.07541 test_loss: 0.08836 \n",
      "[275/300] train_loss: 0.06180 valid_loss: 0.07562 test_loss: 0.08862 \n",
      "[276/300] train_loss: 0.06578 valid_loss: 0.07530 test_loss: 0.08833 \n",
      "[277/300] train_loss: 0.06438 valid_loss: 0.07564 test_loss: 0.08686 \n",
      "[278/300] train_loss: 0.06262 valid_loss: 0.07703 test_loss: 0.08955 \n",
      "[279/300] train_loss: 0.06171 valid_loss: 0.07557 test_loss: 0.08890 \n",
      "[280/300] train_loss: 0.06399 valid_loss: 0.07597 test_loss: 0.08821 \n",
      "[281/300] train_loss: 0.06437 valid_loss: 0.07645 test_loss: 0.08903 \n",
      "[282/300] train_loss: 0.06145 valid_loss: 0.07555 test_loss: 0.08866 \n",
      "[283/300] train_loss: 0.06274 valid_loss: 0.07601 test_loss: 0.08660 \n",
      "[284/300] train_loss: 0.06123 valid_loss: 0.07576 test_loss: 0.08793 \n",
      "[285/300] train_loss: 0.06066 valid_loss: 0.07743 test_loss: 0.08859 \n",
      "[286/300] train_loss: 0.06174 valid_loss: 0.07422 test_loss: 0.08950 \n",
      "Validation loss decreased (0.074695 --> 0.074225).  Saving model ...\n",
      "[287/300] train_loss: 0.06290 valid_loss: 0.07537 test_loss: 0.08738 \n",
      "[288/300] train_loss: 0.06243 valid_loss: 0.07543 test_loss: 0.08834 \n",
      "[289/300] train_loss: 0.06147 valid_loss: 0.07623 test_loss: 0.08768 \n",
      "[290/300] train_loss: 0.06056 valid_loss: 0.07542 test_loss: 0.08786 \n",
      "[291/300] train_loss: 0.05990 valid_loss: 0.07633 test_loss: 0.08909 \n",
      "[292/300] train_loss: 0.06176 valid_loss: 0.07522 test_loss: 0.08713 \n",
      "[293/300] train_loss: 0.06261 valid_loss: 0.07573 test_loss: 0.08816 \n",
      "[294/300] train_loss: 0.06140 valid_loss: 0.07435 test_loss: 0.08733 \n",
      "[295/300] train_loss: 0.06185 valid_loss: 0.07434 test_loss: 0.08707 \n",
      "[296/300] train_loss: 0.06142 valid_loss: 0.07582 test_loss: 0.08749 \n",
      "[297/300] train_loss: 0.06139 valid_loss: 0.07444 test_loss: 0.08720 \n",
      "[298/300] train_loss: 0.06230 valid_loss: 0.07672 test_loss: 0.08925 \n",
      "[299/300] train_loss: 0.06223 valid_loss: 0.07417 test_loss: 0.08872 \n",
      "Validation loss decreased (0.074225 --> 0.074167).  Saving model ...\n",
      "[300/300] train_loss: 0.06116 valid_loss: 0.07422 test_loss: 0.08800 \n",
      "TRAINING MODEL 3\n",
      "[  1/300] train_loss: 0.56597 valid_loss: 0.46553 test_loss: 0.47477 \n",
      "Validation loss decreased (inf --> 0.465530).  Saving model ...\n",
      "[  2/300] train_loss: 0.38307 valid_loss: 0.34699 test_loss: 0.35637 \n",
      "Validation loss decreased (0.465530 --> 0.346989).  Saving model ...\n",
      "[  3/300] train_loss: 0.28977 valid_loss: 0.27387 test_loss: 0.28552 \n",
      "Validation loss decreased (0.346989 --> 0.273872).  Saving model ...\n",
      "[  4/300] train_loss: 0.24107 valid_loss: 0.24090 test_loss: 0.25545 \n",
      "Validation loss decreased (0.273872 --> 0.240899).  Saving model ...\n",
      "[  5/300] train_loss: 0.21093 valid_loss: 0.21770 test_loss: 0.23355 \n",
      "Validation loss decreased (0.240899 --> 0.217700).  Saving model ...\n",
      "[  6/300] train_loss: 0.19984 valid_loss: 0.20394 test_loss: 0.22015 \n",
      "Validation loss decreased (0.217700 --> 0.203941).  Saving model ...\n",
      "[  7/300] train_loss: 0.18824 valid_loss: 0.19597 test_loss: 0.21196 \n",
      "Validation loss decreased (0.203941 --> 0.195967).  Saving model ...\n",
      "[  8/300] train_loss: 0.18443 valid_loss: 0.18363 test_loss: 0.19837 \n",
      "Validation loss decreased (0.195967 --> 0.183633).  Saving model ...\n",
      "[  9/300] train_loss: 0.17286 valid_loss: 0.17980 test_loss: 0.19454 \n",
      "Validation loss decreased (0.183633 --> 0.179804).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16893 valid_loss: 0.17270 test_loss: 0.18575 \n",
      "Validation loss decreased (0.179804 --> 0.172704).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16447 valid_loss: 0.16491 test_loss: 0.17688 \n",
      "Validation loss decreased (0.172704 --> 0.164912).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16118 valid_loss: 0.16513 test_loss: 0.17628 \n",
      "[ 13/300] train_loss: 0.15696 valid_loss: 0.16241 test_loss: 0.17313 \n",
      "Validation loss decreased (0.164912 --> 0.162412).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15103 valid_loss: 0.15760 test_loss: 0.16739 \n",
      "Validation loss decreased (0.162412 --> 0.157604).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14714 valid_loss: 0.15350 test_loss: 0.16402 \n",
      "Validation loss decreased (0.157604 --> 0.153498).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14435 valid_loss: 0.15243 test_loss: 0.16029 \n",
      "Validation loss decreased (0.153498 --> 0.152432).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14278 valid_loss: 0.15242 test_loss: 0.16339 \n",
      "Validation loss decreased (0.152432 --> 0.152423).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13759 valid_loss: 0.15013 test_loss: 0.15884 \n",
      "Validation loss decreased (0.152423 --> 0.150126).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13376 valid_loss: 0.14625 test_loss: 0.15430 \n",
      "Validation loss decreased (0.150126 --> 0.146254).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13496 valid_loss: 0.14793 test_loss: 0.15547 \n",
      "[ 21/300] train_loss: 0.13459 valid_loss: 0.14292 test_loss: 0.15064 \n",
      "Validation loss decreased (0.146254 --> 0.142923).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13289 valid_loss: 0.13956 test_loss: 0.15243 \n",
      "Validation loss decreased (0.142923 --> 0.139560).  Saving model ...\n",
      "[ 23/300] train_loss: 0.13115 valid_loss: 0.14016 test_loss: 0.15047 \n",
      "[ 24/300] train_loss: 0.12661 valid_loss: 0.14244 test_loss: 0.15026 \n",
      "[ 25/300] train_loss: 0.12544 valid_loss: 0.13345 test_loss: 0.14454 \n",
      "Validation loss decreased (0.139560 --> 0.133450).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12483 valid_loss: 0.14360 test_loss: 0.15100 \n",
      "[ 27/300] train_loss: 0.12319 valid_loss: 0.13088 test_loss: 0.14346 \n",
      "Validation loss decreased (0.133450 --> 0.130879).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12019 valid_loss: 0.13167 test_loss: 0.14215 \n",
      "[ 29/300] train_loss: 0.12053 valid_loss: 0.12971 test_loss: 0.14082 \n",
      "Validation loss decreased (0.130879 --> 0.129710).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11976 valid_loss: 0.13029 test_loss: 0.14237 \n",
      "[ 31/300] train_loss: 0.11993 valid_loss: 0.12981 test_loss: 0.14108 \n",
      "[ 32/300] train_loss: 0.11544 valid_loss: 0.12765 test_loss: 0.13986 \n",
      "Validation loss decreased (0.129710 --> 0.127655).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11572 valid_loss: 0.12693 test_loss: 0.13782 \n",
      "Validation loss decreased (0.127655 --> 0.126930).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11503 valid_loss: 0.12827 test_loss: 0.13856 \n",
      "[ 35/300] train_loss: 0.11364 valid_loss: 0.12433 test_loss: 0.13626 \n",
      "Validation loss decreased (0.126930 --> 0.124326).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11376 valid_loss: 0.12685 test_loss: 0.13683 \n",
      "[ 37/300] train_loss: 0.11329 valid_loss: 0.12513 test_loss: 0.13641 \n",
      "[ 38/300] train_loss: 0.11119 valid_loss: 0.11993 test_loss: 0.13256 \n",
      "Validation loss decreased (0.124326 --> 0.119931).  Saving model ...\n",
      "[ 39/300] train_loss: 0.10792 valid_loss: 0.11968 test_loss: 0.13035 \n",
      "Validation loss decreased (0.119931 --> 0.119683).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10934 valid_loss: 0.12300 test_loss: 0.13409 \n",
      "[ 41/300] train_loss: 0.10587 valid_loss: 0.11919 test_loss: 0.13124 \n",
      "Validation loss decreased (0.119683 --> 0.119188).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10856 valid_loss: 0.11984 test_loss: 0.12968 \n",
      "[ 43/300] train_loss: 0.10835 valid_loss: 0.12027 test_loss: 0.13059 \n",
      "[ 44/300] train_loss: 0.10803 valid_loss: 0.11644 test_loss: 0.13021 \n",
      "Validation loss decreased (0.119188 --> 0.116443).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10728 valid_loss: 0.11523 test_loss: 0.12647 \n",
      "Validation loss decreased (0.116443 --> 0.115229).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10498 valid_loss: 0.11359 test_loss: 0.12557 \n",
      "Validation loss decreased (0.115229 --> 0.113593).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10136 valid_loss: 0.11727 test_loss: 0.13010 \n",
      "[ 48/300] train_loss: 0.10181 valid_loss: 0.11377 test_loss: 0.12584 \n",
      "[ 49/300] train_loss: 0.10248 valid_loss: 0.11113 test_loss: 0.12276 \n",
      "Validation loss decreased (0.113593 --> 0.111129).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10259 valid_loss: 0.11052 test_loss: 0.12159 \n",
      "Validation loss decreased (0.111129 --> 0.110517).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10294 valid_loss: 0.11495 test_loss: 0.12627 \n",
      "[ 52/300] train_loss: 0.10239 valid_loss: 0.11249 test_loss: 0.12422 \n",
      "[ 53/300] train_loss: 0.09984 valid_loss: 0.10973 test_loss: 0.12165 \n",
      "Validation loss decreased (0.110517 --> 0.109732).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10073 valid_loss: 0.10969 test_loss: 0.12318 \n",
      "Validation loss decreased (0.109732 --> 0.109687).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09901 valid_loss: 0.11061 test_loss: 0.12382 \n",
      "[ 56/300] train_loss: 0.10029 valid_loss: 0.10916 test_loss: 0.12124 \n",
      "Validation loss decreased (0.109687 --> 0.109164).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09941 valid_loss: 0.10678 test_loss: 0.11826 \n",
      "Validation loss decreased (0.109164 --> 0.106783).  Saving model ...\n",
      "[ 58/300] train_loss: 0.09870 valid_loss: 0.10779 test_loss: 0.11870 \n",
      "[ 59/300] train_loss: 0.09677 valid_loss: 0.10779 test_loss: 0.12048 \n",
      "[ 60/300] train_loss: 0.09574 valid_loss: 0.10928 test_loss: 0.12211 \n",
      "[ 61/300] train_loss: 0.09326 valid_loss: 0.10595 test_loss: 0.11944 \n",
      "Validation loss decreased (0.106783 --> 0.105949).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09708 valid_loss: 0.11058 test_loss: 0.12417 \n",
      "[ 63/300] train_loss: 0.09745 valid_loss: 0.10556 test_loss: 0.11793 \n",
      "Validation loss decreased (0.105949 --> 0.105565).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09379 valid_loss: 0.10498 test_loss: 0.11884 \n",
      "Validation loss decreased (0.105565 --> 0.104981).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09570 valid_loss: 0.10496 test_loss: 0.11926 \n",
      "Validation loss decreased (0.104981 --> 0.104956).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09407 valid_loss: 0.10534 test_loss: 0.11705 \n",
      "[ 67/300] train_loss: 0.09510 valid_loss: 0.10583 test_loss: 0.11692 \n",
      "[ 68/300] train_loss: 0.09513 valid_loss: 0.10231 test_loss: 0.11513 \n",
      "Validation loss decreased (0.104956 --> 0.102310).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09386 valid_loss: 0.10496 test_loss: 0.11606 \n",
      "[ 70/300] train_loss: 0.09317 valid_loss: 0.10329 test_loss: 0.11873 \n",
      "[ 71/300] train_loss: 0.09240 valid_loss: 0.10366 test_loss: 0.11656 \n",
      "[ 72/300] train_loss: 0.09255 valid_loss: 0.10239 test_loss: 0.11417 \n",
      "[ 73/300] train_loss: 0.09390 valid_loss: 0.10528 test_loss: 0.11923 \n",
      "[ 74/300] train_loss: 0.09169 valid_loss: 0.09930 test_loss: 0.11082 \n",
      "Validation loss decreased (0.102310 --> 0.099297).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09283 valid_loss: 0.10101 test_loss: 0.11373 \n",
      "[ 76/300] train_loss: 0.09151 valid_loss: 0.10247 test_loss: 0.11297 \n",
      "[ 77/300] train_loss: 0.09188 valid_loss: 0.10046 test_loss: 0.11441 \n",
      "[ 78/300] train_loss: 0.08918 valid_loss: 0.09985 test_loss: 0.11341 \n",
      "[ 79/300] train_loss: 0.09087 valid_loss: 0.09923 test_loss: 0.11078 \n",
      "Validation loss decreased (0.099297 --> 0.099233).  Saving model ...\n",
      "[ 80/300] train_loss: 0.08910 valid_loss: 0.10185 test_loss: 0.11260 \n",
      "[ 81/300] train_loss: 0.09224 valid_loss: 0.09820 test_loss: 0.11095 \n",
      "Validation loss decreased (0.099233 --> 0.098195).  Saving model ...\n",
      "[ 82/300] train_loss: 0.08932 valid_loss: 0.10056 test_loss: 0.11215 \n",
      "[ 83/300] train_loss: 0.08950 valid_loss: 0.10075 test_loss: 0.11331 \n",
      "[ 84/300] train_loss: 0.08903 valid_loss: 0.09957 test_loss: 0.11092 \n",
      "[ 85/300] train_loss: 0.08801 valid_loss: 0.09814 test_loss: 0.10975 \n",
      "Validation loss decreased (0.098195 --> 0.098140).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08979 valid_loss: 0.09967 test_loss: 0.11152 \n",
      "[ 87/300] train_loss: 0.08766 valid_loss: 0.09631 test_loss: 0.10773 \n",
      "Validation loss decreased (0.098140 --> 0.096307).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08738 valid_loss: 0.09736 test_loss: 0.10837 \n",
      "[ 89/300] train_loss: 0.08832 valid_loss: 0.09648 test_loss: 0.10885 \n",
      "[ 90/300] train_loss: 0.08751 valid_loss: 0.09843 test_loss: 0.10982 \n",
      "[ 91/300] train_loss: 0.08804 valid_loss: 0.09393 test_loss: 0.10678 \n",
      "Validation loss decreased (0.096307 --> 0.093934).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08498 valid_loss: 0.09611 test_loss: 0.10845 \n",
      "[ 93/300] train_loss: 0.08603 valid_loss: 0.09430 test_loss: 0.10725 \n",
      "[ 94/300] train_loss: 0.08517 valid_loss: 0.09605 test_loss: 0.10628 \n",
      "[ 95/300] train_loss: 0.08452 valid_loss: 0.09398 test_loss: 0.10657 \n",
      "[ 96/300] train_loss: 0.08553 valid_loss: 0.09466 test_loss: 0.10595 \n",
      "[ 97/300] train_loss: 0.08498 valid_loss: 0.09326 test_loss: 0.10571 \n",
      "Validation loss decreased (0.093934 --> 0.093264).  Saving model ...\n",
      "[ 98/300] train_loss: 0.08645 valid_loss: 0.09714 test_loss: 0.10911 \n",
      "[ 99/300] train_loss: 0.08512 valid_loss: 0.09401 test_loss: 0.10555 \n",
      "[100/300] train_loss: 0.08552 valid_loss: 0.09510 test_loss: 0.10811 \n",
      "[101/300] train_loss: 0.08293 valid_loss: 0.09481 test_loss: 0.10493 \n",
      "[102/300] train_loss: 0.08422 valid_loss: 0.09253 test_loss: 0.10604 \n",
      "Validation loss decreased (0.093264 --> 0.092532).  Saving model ...\n",
      "[103/300] train_loss: 0.08456 valid_loss: 0.09359 test_loss: 0.10480 \n",
      "[104/300] train_loss: 0.08303 valid_loss: 0.09416 test_loss: 0.10580 \n",
      "[105/300] train_loss: 0.08243 valid_loss: 0.09305 test_loss: 0.10573 \n",
      "[106/300] train_loss: 0.08168 valid_loss: 0.09294 test_loss: 0.10435 \n",
      "[107/300] train_loss: 0.08362 valid_loss: 0.09299 test_loss: 0.10466 \n",
      "[108/300] train_loss: 0.08205 valid_loss: 0.09344 test_loss: 0.10546 \n",
      "[109/300] train_loss: 0.08032 valid_loss: 0.09308 test_loss: 0.10380 \n",
      "[110/300] train_loss: 0.08148 valid_loss: 0.09027 test_loss: 0.10166 \n",
      "Validation loss decreased (0.092532 --> 0.090271).  Saving model ...\n",
      "[111/300] train_loss: 0.08473 valid_loss: 0.09211 test_loss: 0.10286 \n",
      "[112/300] train_loss: 0.08017 valid_loss: 0.09245 test_loss: 0.10387 \n",
      "[113/300] train_loss: 0.08206 valid_loss: 0.09424 test_loss: 0.10620 \n",
      "[114/300] train_loss: 0.08052 valid_loss: 0.08960 test_loss: 0.10112 \n",
      "Validation loss decreased (0.090271 --> 0.089604).  Saving model ...\n",
      "[115/300] train_loss: 0.08261 valid_loss: 0.09054 test_loss: 0.10300 \n",
      "[116/300] train_loss: 0.08131 valid_loss: 0.09095 test_loss: 0.10565 \n",
      "[117/300] train_loss: 0.08031 valid_loss: 0.09236 test_loss: 0.10424 \n",
      "[118/300] train_loss: 0.08018 valid_loss: 0.08960 test_loss: 0.10070 \n",
      "Validation loss decreased (0.089604 --> 0.089597).  Saving model ...\n",
      "[119/300] train_loss: 0.08129 valid_loss: 0.08871 test_loss: 0.10020 \n",
      "Validation loss decreased (0.089597 --> 0.088711).  Saving model ...\n",
      "[120/300] train_loss: 0.08185 valid_loss: 0.09275 test_loss: 0.10377 \n",
      "[121/300] train_loss: 0.08060 valid_loss: 0.09054 test_loss: 0.10126 \n",
      "[122/300] train_loss: 0.07794 valid_loss: 0.09037 test_loss: 0.10223 \n",
      "[123/300] train_loss: 0.07948 valid_loss: 0.08917 test_loss: 0.10058 \n",
      "[124/300] train_loss: 0.07861 valid_loss: 0.09054 test_loss: 0.10236 \n",
      "[125/300] train_loss: 0.08062 valid_loss: 0.08863 test_loss: 0.09978 \n",
      "Validation loss decreased (0.088711 --> 0.088626).  Saving model ...\n",
      "[126/300] train_loss: 0.08086 valid_loss: 0.08873 test_loss: 0.09885 \n",
      "[127/300] train_loss: 0.07913 valid_loss: 0.08818 test_loss: 0.09960 \n",
      "Validation loss decreased (0.088626 --> 0.088179).  Saving model ...\n",
      "[128/300] train_loss: 0.08113 valid_loss: 0.08818 test_loss: 0.10051 \n",
      "[129/300] train_loss: 0.07862 valid_loss: 0.08952 test_loss: 0.10041 \n",
      "[130/300] train_loss: 0.07797 valid_loss: 0.09004 test_loss: 0.10040 \n",
      "[131/300] train_loss: 0.07454 valid_loss: 0.09022 test_loss: 0.10045 \n",
      "[132/300] train_loss: 0.07854 valid_loss: 0.08949 test_loss: 0.10157 \n",
      "[133/300] train_loss: 0.07785 valid_loss: 0.08860 test_loss: 0.10015 \n",
      "[134/300] train_loss: 0.07873 valid_loss: 0.08814 test_loss: 0.09985 \n",
      "Validation loss decreased (0.088179 --> 0.088139).  Saving model ...\n",
      "[135/300] train_loss: 0.07703 valid_loss: 0.08713 test_loss: 0.09935 \n",
      "Validation loss decreased (0.088139 --> 0.087131).  Saving model ...\n",
      "[136/300] train_loss: 0.07781 valid_loss: 0.08917 test_loss: 0.10050 \n",
      "[137/300] train_loss: 0.07854 valid_loss: 0.08817 test_loss: 0.09932 \n",
      "[138/300] train_loss: 0.07725 valid_loss: 0.08773 test_loss: 0.09989 \n",
      "[139/300] train_loss: 0.07589 valid_loss: 0.08889 test_loss: 0.10028 \n",
      "[140/300] train_loss: 0.07626 valid_loss: 0.08611 test_loss: 0.09875 \n",
      "Validation loss decreased (0.087131 --> 0.086113).  Saving model ...\n",
      "[141/300] train_loss: 0.07737 valid_loss: 0.08681 test_loss: 0.09937 \n",
      "[142/300] train_loss: 0.07823 valid_loss: 0.08819 test_loss: 0.09958 \n",
      "[143/300] train_loss: 0.07871 valid_loss: 0.08769 test_loss: 0.09881 \n",
      "[144/300] train_loss: 0.07648 valid_loss: 0.08564 test_loss: 0.09813 \n",
      "Validation loss decreased (0.086113 --> 0.085639).  Saving model ...\n",
      "[145/300] train_loss: 0.07797 valid_loss: 0.08649 test_loss: 0.09813 \n",
      "[146/300] train_loss: 0.07960 valid_loss: 0.08694 test_loss: 0.09819 \n",
      "[147/300] train_loss: 0.07531 valid_loss: 0.08764 test_loss: 0.09925 \n",
      "[148/300] train_loss: 0.07639 valid_loss: 0.08707 test_loss: 0.09847 \n",
      "[149/300] train_loss: 0.07656 valid_loss: 0.08767 test_loss: 0.09825 \n",
      "[150/300] train_loss: 0.07726 valid_loss: 0.08576 test_loss: 0.09808 \n",
      "[151/300] train_loss: 0.07780 valid_loss: 0.08491 test_loss: 0.09614 \n",
      "Validation loss decreased (0.085639 --> 0.084914).  Saving model ...\n",
      "[152/300] train_loss: 0.07354 valid_loss: 0.08733 test_loss: 0.10073 \n",
      "[153/300] train_loss: 0.07469 valid_loss: 0.08666 test_loss: 0.09916 \n",
      "[154/300] train_loss: 0.07353 valid_loss: 0.08720 test_loss: 0.09863 \n",
      "[155/300] train_loss: 0.07123 valid_loss: 0.08438 test_loss: 0.09629 \n",
      "Validation loss decreased (0.084914 --> 0.084379).  Saving model ...\n",
      "[156/300] train_loss: 0.07599 valid_loss: 0.08476 test_loss: 0.09545 \n",
      "[157/300] train_loss: 0.07496 valid_loss: 0.08526 test_loss: 0.09680 \n",
      "[158/300] train_loss: 0.07421 valid_loss: 0.09023 test_loss: 0.10198 \n",
      "[159/300] train_loss: 0.07585 valid_loss: 0.08782 test_loss: 0.09818 \n",
      "[160/300] train_loss: 0.07680 valid_loss: 0.08560 test_loss: 0.09657 \n",
      "[161/300] train_loss: 0.07230 valid_loss: 0.08474 test_loss: 0.09678 \n",
      "[162/300] train_loss: 0.07547 valid_loss: 0.08428 test_loss: 0.09677 \n",
      "Validation loss decreased (0.084379 --> 0.084280).  Saving model ...\n",
      "[163/300] train_loss: 0.07577 valid_loss: 0.08524 test_loss: 0.09798 \n",
      "[164/300] train_loss: 0.07308 valid_loss: 0.08342 test_loss: 0.09564 \n",
      "Validation loss decreased (0.084280 --> 0.083420).  Saving model ...\n",
      "[165/300] train_loss: 0.07386 valid_loss: 0.08335 test_loss: 0.09528 \n",
      "Validation loss decreased (0.083420 --> 0.083348).  Saving model ...\n",
      "[166/300] train_loss: 0.07507 valid_loss: 0.08294 test_loss: 0.09440 \n",
      "Validation loss decreased (0.083348 --> 0.082944).  Saving model ...\n",
      "[167/300] train_loss: 0.07314 valid_loss: 0.08417 test_loss: 0.09637 \n",
      "[168/300] train_loss: 0.07315 valid_loss: 0.08406 test_loss: 0.09612 \n",
      "[169/300] train_loss: 0.07146 valid_loss: 0.08342 test_loss: 0.09484 \n",
      "[170/300] train_loss: 0.07140 valid_loss: 0.08587 test_loss: 0.09585 \n",
      "[171/300] train_loss: 0.07357 valid_loss: 0.08326 test_loss: 0.09531 \n",
      "[172/300] train_loss: 0.07190 valid_loss: 0.08397 test_loss: 0.09575 \n",
      "[173/300] train_loss: 0.07148 valid_loss: 0.08502 test_loss: 0.09620 \n",
      "[174/300] train_loss: 0.07373 valid_loss: 0.08277 test_loss: 0.09383 \n",
      "Validation loss decreased (0.082944 --> 0.082774).  Saving model ...\n",
      "[175/300] train_loss: 0.07442 valid_loss: 0.08373 test_loss: 0.09463 \n",
      "[176/300] train_loss: 0.07248 valid_loss: 0.08188 test_loss: 0.09434 \n",
      "Validation loss decreased (0.082774 --> 0.081881).  Saving model ...\n",
      "[177/300] train_loss: 0.07076 valid_loss: 0.08246 test_loss: 0.09516 \n",
      "[178/300] train_loss: 0.07093 valid_loss: 0.08323 test_loss: 0.09486 \n",
      "[179/300] train_loss: 0.07128 valid_loss: 0.08372 test_loss: 0.09548 \n",
      "[180/300] train_loss: 0.07132 valid_loss: 0.08367 test_loss: 0.09406 \n",
      "[181/300] train_loss: 0.06951 valid_loss: 0.08317 test_loss: 0.09404 \n",
      "[182/300] train_loss: 0.07168 valid_loss: 0.08245 test_loss: 0.09281 \n",
      "[183/300] train_loss: 0.07253 valid_loss: 0.08305 test_loss: 0.09479 \n",
      "[184/300] train_loss: 0.07067 valid_loss: 0.08176 test_loss: 0.09313 \n",
      "Validation loss decreased (0.081881 --> 0.081760).  Saving model ...\n",
      "[185/300] train_loss: 0.06971 valid_loss: 0.08259 test_loss: 0.09419 \n",
      "[186/300] train_loss: 0.07171 valid_loss: 0.08200 test_loss: 0.09339 \n",
      "[187/300] train_loss: 0.06911 valid_loss: 0.08116 test_loss: 0.09233 \n",
      "Validation loss decreased (0.081760 --> 0.081157).  Saving model ...\n",
      "[188/300] train_loss: 0.07053 valid_loss: 0.08197 test_loss: 0.09283 \n",
      "[189/300] train_loss: 0.06976 valid_loss: 0.08272 test_loss: 0.09344 \n",
      "[190/300] train_loss: 0.07347 valid_loss: 0.08182 test_loss: 0.09353 \n",
      "[191/300] train_loss: 0.07332 valid_loss: 0.08328 test_loss: 0.09392 \n",
      "[192/300] train_loss: 0.06830 valid_loss: 0.08367 test_loss: 0.09588 \n",
      "[193/300] train_loss: 0.07149 valid_loss: 0.08044 test_loss: 0.09271 \n",
      "Validation loss decreased (0.081157 --> 0.080440).  Saving model ...\n",
      "[194/300] train_loss: 0.07110 valid_loss: 0.08248 test_loss: 0.09650 \n",
      "[195/300] train_loss: 0.06942 valid_loss: 0.08202 test_loss: 0.09398 \n",
      "[196/300] train_loss: 0.06884 valid_loss: 0.08084 test_loss: 0.09296 \n",
      "[197/300] train_loss: 0.07211 valid_loss: 0.08167 test_loss: 0.09319 \n",
      "[198/300] train_loss: 0.07101 valid_loss: 0.08209 test_loss: 0.09435 \n",
      "[199/300] train_loss: 0.06785 valid_loss: 0.08415 test_loss: 0.09544 \n",
      "[200/300] train_loss: 0.06862 valid_loss: 0.08012 test_loss: 0.09188 \n",
      "Validation loss decreased (0.080440 --> 0.080120).  Saving model ...\n",
      "[201/300] train_loss: 0.07097 valid_loss: 0.08077 test_loss: 0.09373 \n",
      "[202/300] train_loss: 0.07050 valid_loss: 0.07928 test_loss: 0.09323 \n",
      "Validation loss decreased (0.080120 --> 0.079281).  Saving model ...\n",
      "[203/300] train_loss: 0.06894 valid_loss: 0.08045 test_loss: 0.09302 \n",
      "[204/300] train_loss: 0.06820 valid_loss: 0.08089 test_loss: 0.09364 \n",
      "[205/300] train_loss: 0.06899 valid_loss: 0.08089 test_loss: 0.09372 \n",
      "[206/300] train_loss: 0.06733 valid_loss: 0.08184 test_loss: 0.09394 \n",
      "[207/300] train_loss: 0.06952 valid_loss: 0.08003 test_loss: 0.09156 \n",
      "[208/300] train_loss: 0.06960 valid_loss: 0.08066 test_loss: 0.09308 \n",
      "[209/300] train_loss: 0.06710 valid_loss: 0.08070 test_loss: 0.09307 \n",
      "[210/300] train_loss: 0.06699 valid_loss: 0.08071 test_loss: 0.09251 \n",
      "[211/300] train_loss: 0.07022 valid_loss: 0.08056 test_loss: 0.09307 \n",
      "[212/300] train_loss: 0.06892 valid_loss: 0.08006 test_loss: 0.09158 \n",
      "[213/300] train_loss: 0.06750 valid_loss: 0.08012 test_loss: 0.09239 \n",
      "[214/300] train_loss: 0.06917 valid_loss: 0.08030 test_loss: 0.09187 \n",
      "[215/300] train_loss: 0.06803 valid_loss: 0.08053 test_loss: 0.09149 \n",
      "[216/300] train_loss: 0.06748 valid_loss: 0.08210 test_loss: 0.09302 \n",
      "[217/300] train_loss: 0.06693 valid_loss: 0.07988 test_loss: 0.09133 \n",
      "[218/300] train_loss: 0.06733 valid_loss: 0.08036 test_loss: 0.09253 \n",
      "[219/300] train_loss: 0.06844 valid_loss: 0.07917 test_loss: 0.09175 \n",
      "Validation loss decreased (0.079281 --> 0.079165).  Saving model ...\n",
      "[220/300] train_loss: 0.06826 valid_loss: 0.07937 test_loss: 0.09173 \n",
      "[221/300] train_loss: 0.06757 valid_loss: 0.08059 test_loss: 0.09279 \n",
      "[222/300] train_loss: 0.07035 valid_loss: 0.08057 test_loss: 0.09291 \n",
      "[223/300] train_loss: 0.06688 valid_loss: 0.07941 test_loss: 0.09201 \n",
      "[224/300] train_loss: 0.06677 valid_loss: 0.07859 test_loss: 0.09214 \n",
      "Validation loss decreased (0.079165 --> 0.078588).  Saving model ...\n",
      "[225/300] train_loss: 0.06746 valid_loss: 0.07948 test_loss: 0.09213 \n",
      "[226/300] train_loss: 0.06762 valid_loss: 0.07941 test_loss: 0.09257 \n",
      "[227/300] train_loss: 0.06543 valid_loss: 0.07936 test_loss: 0.09104 \n",
      "[228/300] train_loss: 0.06684 valid_loss: 0.07836 test_loss: 0.09104 \n",
      "Validation loss decreased (0.078588 --> 0.078363).  Saving model ...\n",
      "[229/300] train_loss: 0.06789 valid_loss: 0.07850 test_loss: 0.09204 \n",
      "[230/300] train_loss: 0.06692 valid_loss: 0.07927 test_loss: 0.09124 \n",
      "[231/300] train_loss: 0.06535 valid_loss: 0.07906 test_loss: 0.09135 \n",
      "[232/300] train_loss: 0.06843 valid_loss: 0.08106 test_loss: 0.09207 \n",
      "[233/300] train_loss: 0.06852 valid_loss: 0.08038 test_loss: 0.09229 \n",
      "[234/300] train_loss: 0.06822 valid_loss: 0.07730 test_loss: 0.09052 \n",
      "Validation loss decreased (0.078363 --> 0.077295).  Saving model ...\n",
      "[235/300] train_loss: 0.06589 valid_loss: 0.07678 test_loss: 0.09001 \n",
      "Validation loss decreased (0.077295 --> 0.076783).  Saving model ...\n",
      "[236/300] train_loss: 0.06668 valid_loss: 0.07648 test_loss: 0.09051 \n",
      "Validation loss decreased (0.076783 --> 0.076476).  Saving model ...\n",
      "[237/300] train_loss: 0.06518 valid_loss: 0.07792 test_loss: 0.09031 \n",
      "[238/300] train_loss: 0.06540 valid_loss: 0.07807 test_loss: 0.09116 \n",
      "[239/300] train_loss: 0.06602 valid_loss: 0.07950 test_loss: 0.09260 \n",
      "[240/300] train_loss: 0.06727 valid_loss: 0.07877 test_loss: 0.09062 \n",
      "[241/300] train_loss: 0.06703 valid_loss: 0.07860 test_loss: 0.09153 \n",
      "[242/300] train_loss: 0.06770 valid_loss: 0.07686 test_loss: 0.09118 \n",
      "[243/300] train_loss: 0.06545 valid_loss: 0.07770 test_loss: 0.09144 \n",
      "[244/300] train_loss: 0.06708 valid_loss: 0.07935 test_loss: 0.09347 \n",
      "[245/300] train_loss: 0.06547 valid_loss: 0.07883 test_loss: 0.09054 \n",
      "[246/300] train_loss: 0.06775 valid_loss: 0.07724 test_loss: 0.08992 \n",
      "[247/300] train_loss: 0.06540 valid_loss: 0.08013 test_loss: 0.09330 \n",
      "[248/300] train_loss: 0.06494 valid_loss: 0.07785 test_loss: 0.09070 \n",
      "[249/300] train_loss: 0.06626 valid_loss: 0.07799 test_loss: 0.08956 \n",
      "[250/300] train_loss: 0.06543 valid_loss: 0.07715 test_loss: 0.08835 \n",
      "[251/300] train_loss: 0.06657 valid_loss: 0.07845 test_loss: 0.08951 \n",
      "[252/300] train_loss: 0.06499 valid_loss: 0.07847 test_loss: 0.08933 \n",
      "[253/300] train_loss: 0.06612 valid_loss: 0.07757 test_loss: 0.09015 \n",
      "[254/300] train_loss: 0.06545 valid_loss: 0.07731 test_loss: 0.08990 \n",
      "[255/300] train_loss: 0.06511 valid_loss: 0.07636 test_loss: 0.08876 \n",
      "Validation loss decreased (0.076476 --> 0.076359).  Saving model ...\n",
      "[256/300] train_loss: 0.06376 valid_loss: 0.07629 test_loss: 0.08972 \n",
      "Validation loss decreased (0.076359 --> 0.076293).  Saving model ...\n",
      "[257/300] train_loss: 0.06562 valid_loss: 0.07785 test_loss: 0.08895 \n",
      "[258/300] train_loss: 0.06505 valid_loss: 0.07946 test_loss: 0.09126 \n",
      "[259/300] train_loss: 0.06229 valid_loss: 0.07831 test_loss: 0.09034 \n",
      "[260/300] train_loss: 0.06389 valid_loss: 0.07612 test_loss: 0.08941 \n",
      "Validation loss decreased (0.076293 --> 0.076125).  Saving model ...\n",
      "[261/300] train_loss: 0.06430 valid_loss: 0.07674 test_loss: 0.08817 \n",
      "[262/300] train_loss: 0.06432 valid_loss: 0.07738 test_loss: 0.08895 \n",
      "[263/300] train_loss: 0.06590 valid_loss: 0.07661 test_loss: 0.08846 \n",
      "[264/300] train_loss: 0.06459 valid_loss: 0.08055 test_loss: 0.09154 \n",
      "[265/300] train_loss: 0.06266 valid_loss: 0.07659 test_loss: 0.08857 \n",
      "[266/300] train_loss: 0.06087 valid_loss: 0.07750 test_loss: 0.08880 \n",
      "[267/300] train_loss: 0.06405 valid_loss: 0.07682 test_loss: 0.08857 \n",
      "[268/300] train_loss: 0.06435 valid_loss: 0.07605 test_loss: 0.08855 \n",
      "Validation loss decreased (0.076125 --> 0.076051).  Saving model ...\n",
      "[269/300] train_loss: 0.06272 valid_loss: 0.07678 test_loss: 0.08880 \n",
      "[270/300] train_loss: 0.06221 valid_loss: 0.07556 test_loss: 0.08867 \n",
      "Validation loss decreased (0.076051 --> 0.075558).  Saving model ...\n",
      "[271/300] train_loss: 0.06308 valid_loss: 0.07553 test_loss: 0.08912 \n",
      "Validation loss decreased (0.075558 --> 0.075527).  Saving model ...\n",
      "[272/300] train_loss: 0.06513 valid_loss: 0.07668 test_loss: 0.08988 \n",
      "[273/300] train_loss: 0.06223 valid_loss: 0.07728 test_loss: 0.08954 \n",
      "[274/300] train_loss: 0.06385 valid_loss: 0.07668 test_loss: 0.08971 \n",
      "[275/300] train_loss: 0.06476 valid_loss: 0.07649 test_loss: 0.08928 \n",
      "[276/300] train_loss: 0.06406 valid_loss: 0.07585 test_loss: 0.08883 \n",
      "[277/300] train_loss: 0.06283 valid_loss: 0.07800 test_loss: 0.09013 \n",
      "[278/300] train_loss: 0.06348 valid_loss: 0.07652 test_loss: 0.08952 \n",
      "[279/300] train_loss: 0.06342 valid_loss: 0.07655 test_loss: 0.08953 \n",
      "[280/300] train_loss: 0.06240 valid_loss: 0.07765 test_loss: 0.08927 \n",
      "[281/300] train_loss: 0.06490 valid_loss: 0.07687 test_loss: 0.08857 \n",
      "[282/300] train_loss: 0.06404 valid_loss: 0.07793 test_loss: 0.09020 \n",
      "[283/300] train_loss: 0.06273 valid_loss: 0.07700 test_loss: 0.08827 \n",
      "[284/300] train_loss: 0.06311 valid_loss: 0.07768 test_loss: 0.08877 \n",
      "[285/300] train_loss: 0.06239 valid_loss: 0.07742 test_loss: 0.08972 \n",
      "[286/300] train_loss: 0.06300 valid_loss: 0.07519 test_loss: 0.08789 \n",
      "Validation loss decreased (0.075527 --> 0.075194).  Saving model ...\n",
      "[287/300] train_loss: 0.06132 valid_loss: 0.07678 test_loss: 0.08824 \n",
      "[288/300] train_loss: 0.06376 valid_loss: 0.07689 test_loss: 0.08921 \n",
      "[289/300] train_loss: 0.06191 valid_loss: 0.07578 test_loss: 0.08840 \n",
      "[290/300] train_loss: 0.06413 valid_loss: 0.07636 test_loss: 0.08795 \n",
      "[291/300] train_loss: 0.06158 valid_loss: 0.07588 test_loss: 0.08790 \n",
      "[292/300] train_loss: 0.06206 valid_loss: 0.07617 test_loss: 0.08839 \n",
      "[293/300] train_loss: 0.06198 valid_loss: 0.07465 test_loss: 0.08792 \n",
      "Validation loss decreased (0.075194 --> 0.074649).  Saving model ...\n",
      "[294/300] train_loss: 0.06139 valid_loss: 0.07697 test_loss: 0.08883 \n",
      "[295/300] train_loss: 0.05842 valid_loss: 0.07511 test_loss: 0.08934 \n",
      "[296/300] train_loss: 0.06244 valid_loss: 0.07579 test_loss: 0.08737 \n",
      "[297/300] train_loss: 0.06303 valid_loss: 0.07546 test_loss: 0.08840 \n",
      "[298/300] train_loss: 0.06072 valid_loss: 0.07645 test_loss: 0.08775 \n",
      "[299/300] train_loss: 0.06008 valid_loss: 0.07551 test_loss: 0.08762 \n",
      "[300/300] train_loss: 0.06335 valid_loss: 0.07584 test_loss: 0.08694 \n",
      "TRAINING MODEL 4\n",
      "[  1/300] train_loss: 0.53184 valid_loss: 0.41761 test_loss: 0.42382 \n",
      "Validation loss decreased (inf --> 0.417610).  Saving model ...\n",
      "[  2/300] train_loss: 0.32908 valid_loss: 0.31315 test_loss: 0.31957 \n",
      "Validation loss decreased (0.417610 --> 0.313148).  Saving model ...\n",
      "[  3/300] train_loss: 0.26262 valid_loss: 0.26316 test_loss: 0.27744 \n",
      "Validation loss decreased (0.313148 --> 0.263157).  Saving model ...\n",
      "[  4/300] train_loss: 0.22517 valid_loss: 0.23205 test_loss: 0.24729 \n",
      "Validation loss decreased (0.263157 --> 0.232047).  Saving model ...\n",
      "[  5/300] train_loss: 0.20604 valid_loss: 0.21159 test_loss: 0.22978 \n",
      "Validation loss decreased (0.232047 --> 0.211587).  Saving model ...\n",
      "[  6/300] train_loss: 0.19563 valid_loss: 0.19807 test_loss: 0.21484 \n",
      "Validation loss decreased (0.211587 --> 0.198069).  Saving model ...\n",
      "[  7/300] train_loss: 0.18525 valid_loss: 0.18530 test_loss: 0.20173 \n",
      "Validation loss decreased (0.198069 --> 0.185301).  Saving model ...\n",
      "[  8/300] train_loss: 0.17462 valid_loss: 0.18000 test_loss: 0.19326 \n",
      "Validation loss decreased (0.185301 --> 0.180005).  Saving model ...\n",
      "[  9/300] train_loss: 0.16975 valid_loss: 0.17203 test_loss: 0.18697 \n",
      "Validation loss decreased (0.180005 --> 0.172026).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16466 valid_loss: 0.16676 test_loss: 0.17966 \n",
      "Validation loss decreased (0.172026 --> 0.166760).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16259 valid_loss: 0.16181 test_loss: 0.17562 \n",
      "Validation loss decreased (0.166760 --> 0.161815).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15555 valid_loss: 0.15893 test_loss: 0.17248 \n",
      "Validation loss decreased (0.161815 --> 0.158928).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15192 valid_loss: 0.15301 test_loss: 0.16588 \n",
      "Validation loss decreased (0.158928 --> 0.153009).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14878 valid_loss: 0.14967 test_loss: 0.16550 \n",
      "Validation loss decreased (0.153009 --> 0.149671).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14408 valid_loss: 0.14477 test_loss: 0.15800 \n",
      "Validation loss decreased (0.149671 --> 0.144767).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13694 valid_loss: 0.14541 test_loss: 0.15972 \n",
      "[ 17/300] train_loss: 0.13900 valid_loss: 0.14086 test_loss: 0.15689 \n",
      "Validation loss decreased (0.144767 --> 0.140859).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13338 valid_loss: 0.13978 test_loss: 0.15643 \n",
      "Validation loss decreased (0.140859 --> 0.139775).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13374 valid_loss: 0.13476 test_loss: 0.14916 \n",
      "Validation loss decreased (0.139775 --> 0.134755).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13163 valid_loss: 0.13345 test_loss: 0.15039 \n",
      "Validation loss decreased (0.134755 --> 0.133445).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12874 valid_loss: 0.13288 test_loss: 0.14741 \n",
      "Validation loss decreased (0.133445 --> 0.132879).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12786 valid_loss: 0.13284 test_loss: 0.14837 \n",
      "Validation loss decreased (0.132879 --> 0.132839).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12454 valid_loss: 0.13016 test_loss: 0.14367 \n",
      "Validation loss decreased (0.132839 --> 0.130159).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12370 valid_loss: 0.12792 test_loss: 0.14289 \n",
      "Validation loss decreased (0.130159 --> 0.127921).  Saving model ...\n",
      "[ 25/300] train_loss: 0.11883 valid_loss: 0.12692 test_loss: 0.14116 \n",
      "Validation loss decreased (0.127921 --> 0.126920).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12401 valid_loss: 0.12482 test_loss: 0.13960 \n",
      "Validation loss decreased (0.126920 --> 0.124819).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12013 valid_loss: 0.12430 test_loss: 0.13889 \n",
      "Validation loss decreased (0.124819 --> 0.124301).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12004 valid_loss: 0.12529 test_loss: 0.14012 \n",
      "[ 29/300] train_loss: 0.11918 valid_loss: 0.12611 test_loss: 0.13951 \n",
      "[ 30/300] train_loss: 0.11640 valid_loss: 0.12587 test_loss: 0.13863 \n",
      "[ 31/300] train_loss: 0.11378 valid_loss: 0.12466 test_loss: 0.14090 \n",
      "[ 32/300] train_loss: 0.11961 valid_loss: 0.12446 test_loss: 0.13931 \n",
      "[ 33/300] train_loss: 0.11468 valid_loss: 0.12163 test_loss: 0.13494 \n",
      "Validation loss decreased (0.124301 --> 0.121627).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11517 valid_loss: 0.12284 test_loss: 0.13619 \n",
      "[ 35/300] train_loss: 0.11254 valid_loss: 0.11946 test_loss: 0.13278 \n",
      "Validation loss decreased (0.121627 --> 0.119461).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11412 valid_loss: 0.11965 test_loss: 0.13356 \n",
      "[ 37/300] train_loss: 0.11113 valid_loss: 0.11686 test_loss: 0.13114 \n",
      "Validation loss decreased (0.119461 --> 0.116862).  Saving model ...\n",
      "[ 38/300] train_loss: 0.10986 valid_loss: 0.11625 test_loss: 0.13113 \n",
      "Validation loss decreased (0.116862 --> 0.116255).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11058 valid_loss: 0.11608 test_loss: 0.12889 \n",
      "Validation loss decreased (0.116255 --> 0.116080).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10933 valid_loss: 0.11655 test_loss: 0.13260 \n",
      "[ 41/300] train_loss: 0.11055 valid_loss: 0.11403 test_loss: 0.12883 \n",
      "Validation loss decreased (0.116080 --> 0.114031).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10555 valid_loss: 0.11778 test_loss: 0.12927 \n",
      "[ 43/300] train_loss: 0.10753 valid_loss: 0.11388 test_loss: 0.12569 \n",
      "Validation loss decreased (0.114031 --> 0.113883).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10507 valid_loss: 0.11341 test_loss: 0.12654 \n",
      "Validation loss decreased (0.113883 --> 0.113406).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10375 valid_loss: 0.11332 test_loss: 0.12768 \n",
      "Validation loss decreased (0.113406 --> 0.113316).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10402 valid_loss: 0.10985 test_loss: 0.12379 \n",
      "Validation loss decreased (0.113316 --> 0.109845).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10434 valid_loss: 0.11284 test_loss: 0.12444 \n",
      "[ 48/300] train_loss: 0.10429 valid_loss: 0.11323 test_loss: 0.12528 \n",
      "[ 49/300] train_loss: 0.10168 valid_loss: 0.11185 test_loss: 0.12366 \n",
      "[ 50/300] train_loss: 0.10276 valid_loss: 0.11002 test_loss: 0.12302 \n",
      "[ 51/300] train_loss: 0.10273 valid_loss: 0.10970 test_loss: 0.12382 \n",
      "Validation loss decreased (0.109845 --> 0.109704).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10058 valid_loss: 0.11015 test_loss: 0.12400 \n",
      "[ 53/300] train_loss: 0.09933 valid_loss: 0.11213 test_loss: 0.12331 \n",
      "[ 54/300] train_loss: 0.10367 valid_loss: 0.10714 test_loss: 0.12072 \n",
      "Validation loss decreased (0.109704 --> 0.107137).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09955 valid_loss: 0.10854 test_loss: 0.12002 \n",
      "[ 56/300] train_loss: 0.10126 valid_loss: 0.10562 test_loss: 0.11774 \n",
      "Validation loss decreased (0.107137 --> 0.105618).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09791 valid_loss: 0.11082 test_loss: 0.12240 \n",
      "[ 58/300] train_loss: 0.09976 valid_loss: 0.10638 test_loss: 0.11835 \n",
      "[ 59/300] train_loss: 0.09639 valid_loss: 0.10594 test_loss: 0.11803 \n",
      "[ 60/300] train_loss: 0.09548 valid_loss: 0.10336 test_loss: 0.11602 \n",
      "Validation loss decreased (0.105618 --> 0.103364).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09584 valid_loss: 0.10531 test_loss: 0.11716 \n",
      "[ 62/300] train_loss: 0.09341 valid_loss: 0.10713 test_loss: 0.11780 \n",
      "[ 63/300] train_loss: 0.09845 valid_loss: 0.10393 test_loss: 0.11672 \n",
      "[ 64/300] train_loss: 0.09535 valid_loss: 0.10517 test_loss: 0.11827 \n",
      "[ 65/300] train_loss: 0.09485 valid_loss: 0.10418 test_loss: 0.11464 \n",
      "[ 66/300] train_loss: 0.09422 valid_loss: 0.10303 test_loss: 0.11470 \n",
      "Validation loss decreased (0.103364 --> 0.103031).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09116 valid_loss: 0.10322 test_loss: 0.11414 \n",
      "[ 68/300] train_loss: 0.09301 valid_loss: 0.10441 test_loss: 0.11489 \n",
      "[ 69/300] train_loss: 0.09157 valid_loss: 0.10127 test_loss: 0.11254 \n",
      "Validation loss decreased (0.103031 --> 0.101274).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09231 valid_loss: 0.10380 test_loss: 0.11387 \n",
      "[ 71/300] train_loss: 0.09082 valid_loss: 0.10218 test_loss: 0.11296 \n",
      "[ 72/300] train_loss: 0.09308 valid_loss: 0.10053 test_loss: 0.11128 \n",
      "Validation loss decreased (0.101274 --> 0.100529).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09356 valid_loss: 0.10081 test_loss: 0.11193 \n",
      "[ 74/300] train_loss: 0.09316 valid_loss: 0.10057 test_loss: 0.11129 \n",
      "[ 75/300] train_loss: 0.09129 valid_loss: 0.10169 test_loss: 0.11242 \n",
      "[ 76/300] train_loss: 0.08923 valid_loss: 0.10023 test_loss: 0.11195 \n",
      "Validation loss decreased (0.100529 --> 0.100234).  Saving model ...\n",
      "[ 77/300] train_loss: 0.08894 valid_loss: 0.09908 test_loss: 0.11140 \n",
      "Validation loss decreased (0.100234 --> 0.099076).  Saving model ...\n",
      "[ 78/300] train_loss: 0.08923 valid_loss: 0.10017 test_loss: 0.11263 \n",
      "[ 79/300] train_loss: 0.08937 valid_loss: 0.10045 test_loss: 0.11217 \n",
      "[ 80/300] train_loss: 0.09190 valid_loss: 0.09800 test_loss: 0.10876 \n",
      "Validation loss decreased (0.099076 --> 0.097999).  Saving model ...\n",
      "[ 81/300] train_loss: 0.08934 valid_loss: 0.09882 test_loss: 0.10875 \n",
      "[ 82/300] train_loss: 0.08842 valid_loss: 0.09724 test_loss: 0.10823 \n",
      "Validation loss decreased (0.097999 --> 0.097240).  Saving model ...\n",
      "[ 83/300] train_loss: 0.08765 valid_loss: 0.09767 test_loss: 0.10961 \n",
      "[ 84/300] train_loss: 0.08693 valid_loss: 0.09912 test_loss: 0.11044 \n",
      "[ 85/300] train_loss: 0.08700 valid_loss: 0.09705 test_loss: 0.10812 \n",
      "Validation loss decreased (0.097240 --> 0.097045).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08903 valid_loss: 0.09649 test_loss: 0.10770 \n",
      "Validation loss decreased (0.097045 --> 0.096489).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08443 valid_loss: 0.09374 test_loss: 0.10692 \n",
      "Validation loss decreased (0.096489 --> 0.093740).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08714 valid_loss: 0.09771 test_loss: 0.10761 \n",
      "[ 89/300] train_loss: 0.08709 valid_loss: 0.09588 test_loss: 0.10677 \n",
      "[ 90/300] train_loss: 0.08633 valid_loss: 0.09832 test_loss: 0.10929 \n",
      "[ 91/300] train_loss: 0.08544 valid_loss: 0.09768 test_loss: 0.10799 \n",
      "[ 92/300] train_loss: 0.08695 valid_loss: 0.09716 test_loss: 0.10902 \n",
      "[ 93/300] train_loss: 0.08376 valid_loss: 0.09554 test_loss: 0.10622 \n",
      "[ 94/300] train_loss: 0.08834 valid_loss: 0.09544 test_loss: 0.10570 \n",
      "[ 95/300] train_loss: 0.08527 valid_loss: 0.09488 test_loss: 0.10570 \n",
      "[ 96/300] train_loss: 0.08583 valid_loss: 0.09403 test_loss: 0.10539 \n",
      "[ 97/300] train_loss: 0.08433 valid_loss: 0.09620 test_loss: 0.10689 \n",
      "[ 98/300] train_loss: 0.08446 valid_loss: 0.09518 test_loss: 0.10475 \n",
      "[ 99/300] train_loss: 0.08311 valid_loss: 0.09503 test_loss: 0.10711 \n",
      "[100/300] train_loss: 0.08256 valid_loss: 0.09351 test_loss: 0.10429 \n",
      "Validation loss decreased (0.093740 --> 0.093513).  Saving model ...\n",
      "[101/300] train_loss: 0.08290 valid_loss: 0.09252 test_loss: 0.10394 \n",
      "Validation loss decreased (0.093513 --> 0.092524).  Saving model ...\n",
      "[102/300] train_loss: 0.08177 valid_loss: 0.09429 test_loss: 0.10670 \n",
      "[103/300] train_loss: 0.08159 valid_loss: 0.09443 test_loss: 0.10566 \n",
      "[104/300] train_loss: 0.08290 valid_loss: 0.09094 test_loss: 0.10336 \n",
      "Validation loss decreased (0.092524 --> 0.090942).  Saving model ...\n",
      "[105/300] train_loss: 0.08443 valid_loss: 0.09261 test_loss: 0.10330 \n",
      "[106/300] train_loss: 0.07999 valid_loss: 0.09261 test_loss: 0.10382 \n",
      "[107/300] train_loss: 0.07883 valid_loss: 0.09422 test_loss: 0.10521 \n",
      "[108/300] train_loss: 0.08209 valid_loss: 0.09193 test_loss: 0.10124 \n",
      "[109/300] train_loss: 0.08117 valid_loss: 0.09097 test_loss: 0.10199 \n",
      "[110/300] train_loss: 0.08090 valid_loss: 0.09073 test_loss: 0.10328 \n",
      "Validation loss decreased (0.090942 --> 0.090730).  Saving model ...\n",
      "[111/300] train_loss: 0.08051 valid_loss: 0.09047 test_loss: 0.10203 \n",
      "Validation loss decreased (0.090730 --> 0.090465).  Saving model ...\n",
      "[112/300] train_loss: 0.08407 valid_loss: 0.08968 test_loss: 0.10173 \n",
      "Validation loss decreased (0.090465 --> 0.089681).  Saving model ...\n",
      "[113/300] train_loss: 0.08184 valid_loss: 0.08909 test_loss: 0.10153 \n",
      "Validation loss decreased (0.089681 --> 0.089092).  Saving model ...\n",
      "[114/300] train_loss: 0.08001 valid_loss: 0.08858 test_loss: 0.10028 \n",
      "Validation loss decreased (0.089092 --> 0.088576).  Saving model ...\n",
      "[115/300] train_loss: 0.08184 valid_loss: 0.08925 test_loss: 0.10235 \n",
      "[116/300] train_loss: 0.08032 valid_loss: 0.08848 test_loss: 0.10129 \n",
      "Validation loss decreased (0.088576 --> 0.088479).  Saving model ...\n",
      "[117/300] train_loss: 0.08002 valid_loss: 0.08931 test_loss: 0.10119 \n",
      "[118/300] train_loss: 0.08215 valid_loss: 0.08775 test_loss: 0.09990 \n",
      "Validation loss decreased (0.088479 --> 0.087753).  Saving model ...\n",
      "[119/300] train_loss: 0.07744 valid_loss: 0.08746 test_loss: 0.10023 \n",
      "Validation loss decreased (0.087753 --> 0.087458).  Saving model ...\n",
      "[120/300] train_loss: 0.08001 valid_loss: 0.08928 test_loss: 0.10067 \n",
      "[121/300] train_loss: 0.07870 valid_loss: 0.08801 test_loss: 0.10048 \n",
      "[122/300] train_loss: 0.07890 valid_loss: 0.08776 test_loss: 0.10064 \n",
      "[123/300] train_loss: 0.07943 valid_loss: 0.08833 test_loss: 0.10263 \n",
      "[124/300] train_loss: 0.07807 valid_loss: 0.08831 test_loss: 0.10046 \n",
      "[125/300] train_loss: 0.07769 valid_loss: 0.08961 test_loss: 0.10223 \n",
      "[126/300] train_loss: 0.07818 valid_loss: 0.08741 test_loss: 0.09960 \n",
      "Validation loss decreased (0.087458 --> 0.087407).  Saving model ...\n",
      "[127/300] train_loss: 0.07872 valid_loss: 0.08919 test_loss: 0.10016 \n",
      "[128/300] train_loss: 0.07748 valid_loss: 0.08647 test_loss: 0.09879 \n",
      "Validation loss decreased (0.087407 --> 0.086474).  Saving model ...\n",
      "[129/300] train_loss: 0.07887 valid_loss: 0.08835 test_loss: 0.10120 \n",
      "[130/300] train_loss: 0.07481 valid_loss: 0.08766 test_loss: 0.10049 \n",
      "[131/300] train_loss: 0.07812 valid_loss: 0.08914 test_loss: 0.10116 \n",
      "[132/300] train_loss: 0.07637 valid_loss: 0.08775 test_loss: 0.10093 \n",
      "[133/300] train_loss: 0.07967 valid_loss: 0.08846 test_loss: 0.10030 \n",
      "[134/300] train_loss: 0.07599 valid_loss: 0.08962 test_loss: 0.10237 \n",
      "[135/300] train_loss: 0.07617 valid_loss: 0.08543 test_loss: 0.09872 \n",
      "Validation loss decreased (0.086474 --> 0.085430).  Saving model ...\n",
      "[136/300] train_loss: 0.07481 valid_loss: 0.08742 test_loss: 0.10028 \n",
      "[137/300] train_loss: 0.07837 valid_loss: 0.08474 test_loss: 0.09880 \n",
      "Validation loss decreased (0.085430 --> 0.084742).  Saving model ...\n",
      "[138/300] train_loss: 0.07628 valid_loss: 0.08477 test_loss: 0.09720 \n",
      "[139/300] train_loss: 0.07691 valid_loss: 0.08624 test_loss: 0.09770 \n",
      "[140/300] train_loss: 0.07497 valid_loss: 0.08665 test_loss: 0.09880 \n",
      "[141/300] train_loss: 0.07418 valid_loss: 0.08584 test_loss: 0.09789 \n",
      "[142/300] train_loss: 0.07297 valid_loss: 0.08481 test_loss: 0.09823 \n",
      "[143/300] train_loss: 0.07423 valid_loss: 0.08480 test_loss: 0.09673 \n",
      "[144/300] train_loss: 0.07525 valid_loss: 0.08570 test_loss: 0.09740 \n",
      "[145/300] train_loss: 0.07329 valid_loss: 0.08517 test_loss: 0.09713 \n",
      "[146/300] train_loss: 0.07273 valid_loss: 0.08475 test_loss: 0.09686 \n",
      "[147/300] train_loss: 0.07405 valid_loss: 0.08532 test_loss: 0.09865 \n",
      "[148/300] train_loss: 0.07407 valid_loss: 0.08455 test_loss: 0.09603 \n",
      "Validation loss decreased (0.084742 --> 0.084550).  Saving model ...\n",
      "[149/300] train_loss: 0.07322 valid_loss: 0.08480 test_loss: 0.09802 \n",
      "[150/300] train_loss: 0.07318 valid_loss: 0.08424 test_loss: 0.09655 \n",
      "Validation loss decreased (0.084550 --> 0.084242).  Saving model ...\n",
      "[151/300] train_loss: 0.07451 valid_loss: 0.08375 test_loss: 0.09663 \n",
      "Validation loss decreased (0.084242 --> 0.083755).  Saving model ...\n",
      "[152/300] train_loss: 0.07634 valid_loss: 0.08406 test_loss: 0.09705 \n",
      "[153/300] train_loss: 0.07283 valid_loss: 0.08402 test_loss: 0.09598 \n",
      "[154/300] train_loss: 0.07573 valid_loss: 0.08339 test_loss: 0.09678 \n",
      "Validation loss decreased (0.083755 --> 0.083391).  Saving model ...\n",
      "[155/300] train_loss: 0.07241 valid_loss: 0.08388 test_loss: 0.09704 \n",
      "[156/300] train_loss: 0.07408 valid_loss: 0.08391 test_loss: 0.09594 \n",
      "[157/300] train_loss: 0.07373 valid_loss: 0.08482 test_loss: 0.09781 \n",
      "[158/300] train_loss: 0.07406 valid_loss: 0.08290 test_loss: 0.09514 \n",
      "Validation loss decreased (0.083391 --> 0.082901).  Saving model ...\n",
      "[159/300] train_loss: 0.07269 valid_loss: 0.08523 test_loss: 0.09788 \n",
      "[160/300] train_loss: 0.07113 valid_loss: 0.08299 test_loss: 0.09562 \n",
      "[161/300] train_loss: 0.07383 valid_loss: 0.08376 test_loss: 0.09595 \n",
      "[162/300] train_loss: 0.07115 valid_loss: 0.08246 test_loss: 0.09669 \n",
      "Validation loss decreased (0.082901 --> 0.082465).  Saving model ...\n",
      "[163/300] train_loss: 0.07382 valid_loss: 0.08318 test_loss: 0.09525 \n",
      "[164/300] train_loss: 0.07191 valid_loss: 0.08422 test_loss: 0.09682 \n",
      "[165/300] train_loss: 0.07297 valid_loss: 0.08263 test_loss: 0.09497 \n",
      "[166/300] train_loss: 0.07255 valid_loss: 0.08186 test_loss: 0.09538 \n",
      "Validation loss decreased (0.082465 --> 0.081862).  Saving model ...\n",
      "[167/300] train_loss: 0.07200 valid_loss: 0.08085 test_loss: 0.09472 \n",
      "Validation loss decreased (0.081862 --> 0.080854).  Saving model ...\n",
      "[168/300] train_loss: 0.07170 valid_loss: 0.08295 test_loss: 0.09593 \n",
      "[169/300] train_loss: 0.07178 valid_loss: 0.08070 test_loss: 0.09409 \n",
      "Validation loss decreased (0.080854 --> 0.080704).  Saving model ...\n",
      "[170/300] train_loss: 0.07082 valid_loss: 0.08306 test_loss: 0.09631 \n",
      "[171/300] train_loss: 0.07142 valid_loss: 0.08194 test_loss: 0.09471 \n",
      "[172/300] train_loss: 0.07207 valid_loss: 0.08056 test_loss: 0.09450 \n",
      "Validation loss decreased (0.080704 --> 0.080562).  Saving model ...\n",
      "[173/300] train_loss: 0.07288 valid_loss: 0.08117 test_loss: 0.09449 \n",
      "[174/300] train_loss: 0.07146 valid_loss: 0.08238 test_loss: 0.09526 \n",
      "[175/300] train_loss: 0.06949 valid_loss: 0.08261 test_loss: 0.09551 \n",
      "[176/300] train_loss: 0.07049 valid_loss: 0.08108 test_loss: 0.09390 \n",
      "[177/300] train_loss: 0.07269 valid_loss: 0.08136 test_loss: 0.09434 \n",
      "[178/300] train_loss: 0.07246 valid_loss: 0.08200 test_loss: 0.09495 \n",
      "[179/300] train_loss: 0.06998 valid_loss: 0.08258 test_loss: 0.09535 \n",
      "[180/300] train_loss: 0.07204 valid_loss: 0.08196 test_loss: 0.09529 \n",
      "[181/300] train_loss: 0.07009 valid_loss: 0.08150 test_loss: 0.09497 \n",
      "[182/300] train_loss: 0.07090 valid_loss: 0.08109 test_loss: 0.09467 \n",
      "[183/300] train_loss: 0.07029 valid_loss: 0.08246 test_loss: 0.09495 \n",
      "[184/300] train_loss: 0.07186 valid_loss: 0.08097 test_loss: 0.09334 \n",
      "[185/300] train_loss: 0.07053 valid_loss: 0.07993 test_loss: 0.09352 \n",
      "Validation loss decreased (0.080562 --> 0.079928).  Saving model ...\n",
      "[186/300] train_loss: 0.06937 valid_loss: 0.08145 test_loss: 0.09377 \n",
      "[187/300] train_loss: 0.07011 valid_loss: 0.08093 test_loss: 0.09441 \n",
      "[188/300] train_loss: 0.07016 valid_loss: 0.08058 test_loss: 0.09222 \n",
      "[189/300] train_loss: 0.06895 valid_loss: 0.08166 test_loss: 0.09429 \n",
      "[190/300] train_loss: 0.06962 valid_loss: 0.08051 test_loss: 0.09296 \n",
      "[191/300] train_loss: 0.06929 valid_loss: 0.07999 test_loss: 0.09197 \n",
      "[192/300] train_loss: 0.06919 valid_loss: 0.08041 test_loss: 0.09264 \n",
      "[193/300] train_loss: 0.07048 valid_loss: 0.08099 test_loss: 0.09448 \n",
      "[194/300] train_loss: 0.06846 valid_loss: 0.08130 test_loss: 0.09508 \n",
      "[195/300] train_loss: 0.06719 valid_loss: 0.08231 test_loss: 0.09526 \n",
      "[196/300] train_loss: 0.06869 valid_loss: 0.08094 test_loss: 0.09341 \n",
      "[197/300] train_loss: 0.06859 valid_loss: 0.08178 test_loss: 0.09453 \n",
      "[198/300] train_loss: 0.06770 valid_loss: 0.08045 test_loss: 0.09363 \n",
      "[199/300] train_loss: 0.06937 valid_loss: 0.07923 test_loss: 0.09181 \n",
      "Validation loss decreased (0.079928 --> 0.079234).  Saving model ...\n",
      "[200/300] train_loss: 0.06838 valid_loss: 0.08065 test_loss: 0.09265 \n",
      "[201/300] train_loss: 0.06744 valid_loss: 0.08030 test_loss: 0.09470 \n",
      "[202/300] train_loss: 0.06830 valid_loss: 0.08065 test_loss: 0.09270 \n",
      "[203/300] train_loss: 0.06671 valid_loss: 0.08052 test_loss: 0.09368 \n",
      "[204/300] train_loss: 0.06767 valid_loss: 0.08091 test_loss: 0.09296 \n",
      "[205/300] train_loss: 0.06851 valid_loss: 0.08081 test_loss: 0.09269 \n",
      "[206/300] train_loss: 0.06734 valid_loss: 0.07990 test_loss: 0.09153 \n",
      "[207/300] train_loss: 0.06850 valid_loss: 0.07961 test_loss: 0.09153 \n",
      "[208/300] train_loss: 0.06622 valid_loss: 0.07960 test_loss: 0.09213 \n",
      "[209/300] train_loss: 0.06948 valid_loss: 0.07955 test_loss: 0.09359 \n",
      "[210/300] train_loss: 0.06935 valid_loss: 0.07890 test_loss: 0.09125 \n",
      "Validation loss decreased (0.079234 --> 0.078897).  Saving model ...\n",
      "[211/300] train_loss: 0.06710 valid_loss: 0.07948 test_loss: 0.09273 \n",
      "[212/300] train_loss: 0.06698 valid_loss: 0.07922 test_loss: 0.09250 \n",
      "[213/300] train_loss: 0.06543 valid_loss: 0.07924 test_loss: 0.09227 \n",
      "[214/300] train_loss: 0.06580 valid_loss: 0.07947 test_loss: 0.09223 \n",
      "[215/300] train_loss: 0.06558 valid_loss: 0.07918 test_loss: 0.09194 \n",
      "[216/300] train_loss: 0.06573 valid_loss: 0.07859 test_loss: 0.09130 \n",
      "Validation loss decreased (0.078897 --> 0.078587).  Saving model ...\n",
      "[217/300] train_loss: 0.06772 valid_loss: 0.07902 test_loss: 0.09238 \n",
      "[218/300] train_loss: 0.06613 valid_loss: 0.07912 test_loss: 0.09147 \n",
      "[219/300] train_loss: 0.06911 valid_loss: 0.08038 test_loss: 0.09299 \n",
      "[220/300] train_loss: 0.06756 valid_loss: 0.07858 test_loss: 0.09267 \n",
      "Validation loss decreased (0.078587 --> 0.078575).  Saving model ...\n",
      "[221/300] train_loss: 0.06614 valid_loss: 0.08024 test_loss: 0.09220 \n",
      "[222/300] train_loss: 0.06617 valid_loss: 0.07728 test_loss: 0.09083 \n",
      "Validation loss decreased (0.078575 --> 0.077278).  Saving model ...\n",
      "[223/300] train_loss: 0.06755 valid_loss: 0.07863 test_loss: 0.09159 \n",
      "[224/300] train_loss: 0.07010 valid_loss: 0.07789 test_loss: 0.09077 \n",
      "[225/300] train_loss: 0.06681 valid_loss: 0.07718 test_loss: 0.09068 \n",
      "Validation loss decreased (0.077278 --> 0.077175).  Saving model ...\n",
      "[226/300] train_loss: 0.06645 valid_loss: 0.08012 test_loss: 0.09266 \n",
      "[227/300] train_loss: 0.06630 valid_loss: 0.07777 test_loss: 0.09087 \n",
      "[228/300] train_loss: 0.06538 valid_loss: 0.07866 test_loss: 0.09243 \n",
      "[229/300] train_loss: 0.06561 valid_loss: 0.07771 test_loss: 0.09103 \n",
      "[230/300] train_loss: 0.06464 valid_loss: 0.07787 test_loss: 0.09088 \n",
      "[231/300] train_loss: 0.06705 valid_loss: 0.07978 test_loss: 0.09149 \n",
      "[232/300] train_loss: 0.06414 valid_loss: 0.08062 test_loss: 0.09291 \n",
      "[233/300] train_loss: 0.06640 valid_loss: 0.07828 test_loss: 0.09146 \n",
      "[234/300] train_loss: 0.06506 valid_loss: 0.07854 test_loss: 0.09131 \n",
      "[235/300] train_loss: 0.06376 valid_loss: 0.07675 test_loss: 0.08932 \n",
      "Validation loss decreased (0.077175 --> 0.076746).  Saving model ...\n",
      "[236/300] train_loss: 0.06537 valid_loss: 0.07972 test_loss: 0.09158 \n",
      "[237/300] train_loss: 0.06683 valid_loss: 0.07815 test_loss: 0.09020 \n",
      "[238/300] train_loss: 0.06644 valid_loss: 0.07820 test_loss: 0.09009 \n",
      "[239/300] train_loss: 0.06608 valid_loss: 0.07806 test_loss: 0.09089 \n",
      "[240/300] train_loss: 0.06505 valid_loss: 0.07692 test_loss: 0.09080 \n",
      "[241/300] train_loss: 0.06358 valid_loss: 0.07860 test_loss: 0.09211 \n",
      "[242/300] train_loss: 0.06393 valid_loss: 0.07686 test_loss: 0.09051 \n",
      "[243/300] train_loss: 0.06639 valid_loss: 0.07910 test_loss: 0.09279 \n",
      "[244/300] train_loss: 0.06647 valid_loss: 0.07660 test_loss: 0.08943 \n",
      "Validation loss decreased (0.076746 --> 0.076601).  Saving model ...\n",
      "[245/300] train_loss: 0.06501 valid_loss: 0.07743 test_loss: 0.08953 \n",
      "[246/300] train_loss: 0.06703 valid_loss: 0.07879 test_loss: 0.09071 \n",
      "[247/300] train_loss: 0.06497 valid_loss: 0.07685 test_loss: 0.09143 \n",
      "[248/300] train_loss: 0.06459 valid_loss: 0.07690 test_loss: 0.08974 \n",
      "[249/300] train_loss: 0.06258 valid_loss: 0.07614 test_loss: 0.08916 \n",
      "Validation loss decreased (0.076601 --> 0.076142).  Saving model ...\n",
      "[250/300] train_loss: 0.06616 valid_loss: 0.07542 test_loss: 0.08938 \n",
      "Validation loss decreased (0.076142 --> 0.075417).  Saving model ...\n",
      "[251/300] train_loss: 0.06445 valid_loss: 0.07920 test_loss: 0.09160 \n",
      "[252/300] train_loss: 0.06473 valid_loss: 0.07559 test_loss: 0.08861 \n",
      "[253/300] train_loss: 0.06410 valid_loss: 0.07657 test_loss: 0.08974 \n",
      "[254/300] train_loss: 0.06359 valid_loss: 0.07828 test_loss: 0.09101 \n",
      "[255/300] train_loss: 0.06416 valid_loss: 0.07732 test_loss: 0.09005 \n",
      "[256/300] train_loss: 0.06405 valid_loss: 0.07648 test_loss: 0.09005 \n",
      "[257/300] train_loss: 0.06488 valid_loss: 0.07722 test_loss: 0.08955 \n",
      "[258/300] train_loss: 0.06544 valid_loss: 0.07672 test_loss: 0.08964 \n",
      "[259/300] train_loss: 0.06342 valid_loss: 0.07593 test_loss: 0.08912 \n",
      "[260/300] train_loss: 0.06386 valid_loss: 0.07687 test_loss: 0.09064 \n",
      "[261/300] train_loss: 0.06300 valid_loss: 0.07836 test_loss: 0.09120 \n",
      "[262/300] train_loss: 0.06346 valid_loss: 0.07779 test_loss: 0.09121 \n",
      "[263/300] train_loss: 0.06228 valid_loss: 0.07650 test_loss: 0.08944 \n",
      "[264/300] train_loss: 0.06260 valid_loss: 0.07560 test_loss: 0.08807 \n",
      "[265/300] train_loss: 0.06464 valid_loss: 0.07625 test_loss: 0.09028 \n",
      "[266/300] train_loss: 0.06330 valid_loss: 0.07544 test_loss: 0.08813 \n",
      "[267/300] train_loss: 0.06237 valid_loss: 0.07711 test_loss: 0.08950 \n",
      "[268/300] train_loss: 0.06352 valid_loss: 0.07594 test_loss: 0.08988 \n",
      "[269/300] train_loss: 0.06247 valid_loss: 0.07710 test_loss: 0.08968 \n",
      "[270/300] train_loss: 0.05979 valid_loss: 0.07491 test_loss: 0.08871 \n",
      "Validation loss decreased (0.075417 --> 0.074912).  Saving model ...\n",
      "[271/300] train_loss: 0.06269 valid_loss: 0.07632 test_loss: 0.08947 \n",
      "[272/300] train_loss: 0.06308 valid_loss: 0.07613 test_loss: 0.09053 \n",
      "[273/300] train_loss: 0.05917 valid_loss: 0.07532 test_loss: 0.08931 \n",
      "[274/300] train_loss: 0.06158 valid_loss: 0.07581 test_loss: 0.08963 \n",
      "[275/300] train_loss: 0.06415 valid_loss: 0.07648 test_loss: 0.08985 \n",
      "[276/300] train_loss: 0.06245 valid_loss: 0.07538 test_loss: 0.08876 \n",
      "[277/300] train_loss: 0.06084 valid_loss: 0.07584 test_loss: 0.08871 \n",
      "[278/300] train_loss: 0.06123 valid_loss: 0.07610 test_loss: 0.08832 \n",
      "[279/300] train_loss: 0.06263 valid_loss: 0.07554 test_loss: 0.09018 \n",
      "[280/300] train_loss: 0.06097 valid_loss: 0.07547 test_loss: 0.08906 \n",
      "[281/300] train_loss: 0.06248 valid_loss: 0.07531 test_loss: 0.08866 \n",
      "[282/300] train_loss: 0.06268 valid_loss: 0.07637 test_loss: 0.08942 \n",
      "[283/300] train_loss: 0.06136 valid_loss: 0.07566 test_loss: 0.08924 \n",
      "[284/300] train_loss: 0.06076 valid_loss: 0.07539 test_loss: 0.08813 \n",
      "[285/300] train_loss: 0.06211 valid_loss: 0.07669 test_loss: 0.08989 \n",
      "[286/300] train_loss: 0.06017 valid_loss: 0.07477 test_loss: 0.08828 \n",
      "Validation loss decreased (0.074912 --> 0.074771).  Saving model ...\n",
      "[287/300] train_loss: 0.06083 valid_loss: 0.07588 test_loss: 0.08876 \n",
      "[288/300] train_loss: 0.06129 valid_loss: 0.07555 test_loss: 0.08884 \n",
      "[289/300] train_loss: 0.06135 valid_loss: 0.07555 test_loss: 0.08803 \n",
      "[290/300] train_loss: 0.05968 valid_loss: 0.07611 test_loss: 0.08929 \n",
      "[291/300] train_loss: 0.06049 valid_loss: 0.07479 test_loss: 0.08816 \n",
      "[292/300] train_loss: 0.05987 valid_loss: 0.07739 test_loss: 0.09044 \n",
      "[293/300] train_loss: 0.05981 valid_loss: 0.07554 test_loss: 0.08887 \n",
      "[294/300] train_loss: 0.06067 valid_loss: 0.07570 test_loss: 0.08873 \n",
      "[295/300] train_loss: 0.06282 valid_loss: 0.07499 test_loss: 0.08750 \n",
      "[296/300] train_loss: 0.06098 valid_loss: 0.07518 test_loss: 0.08844 \n",
      "[297/300] train_loss: 0.06140 valid_loss: 0.07502 test_loss: 0.08692 \n",
      "[298/300] train_loss: 0.06072 valid_loss: 0.07600 test_loss: 0.08794 \n",
      "[299/300] train_loss: 0.05934 valid_loss: 0.07587 test_loss: 0.08809 \n",
      "[300/300] train_loss: 0.06128 valid_loss: 0.07469 test_loss: 0.08856 \n",
      "Validation loss decreased (0.074771 --> 0.074692).  Saving model ...\n",
      "TRAINING MODEL 5\n",
      "[  1/300] train_loss: 0.53603 valid_loss: 0.43235 test_loss: 0.43839 \n",
      "Validation loss decreased (inf --> 0.432348).  Saving model ...\n",
      "[  2/300] train_loss: 0.35346 valid_loss: 0.32805 test_loss: 0.33661 \n",
      "Validation loss decreased (0.432348 --> 0.328047).  Saving model ...\n",
      "[  3/300] train_loss: 0.27346 valid_loss: 0.26468 test_loss: 0.27613 \n",
      "Validation loss decreased (0.328047 --> 0.264684).  Saving model ...\n",
      "[  4/300] train_loss: 0.22850 valid_loss: 0.22833 test_loss: 0.24204 \n",
      "Validation loss decreased (0.264684 --> 0.228330).  Saving model ...\n",
      "[  5/300] train_loss: 0.21034 valid_loss: 0.20754 test_loss: 0.22047 \n",
      "Validation loss decreased (0.228330 --> 0.207542).  Saving model ...\n",
      "[  6/300] train_loss: 0.19367 valid_loss: 0.19446 test_loss: 0.20803 \n",
      "Validation loss decreased (0.207542 --> 0.194458).  Saving model ...\n",
      "[  7/300] train_loss: 0.18534 valid_loss: 0.18388 test_loss: 0.19583 \n",
      "Validation loss decreased (0.194458 --> 0.183881).  Saving model ...\n",
      "[  8/300] train_loss: 0.17780 valid_loss: 0.17552 test_loss: 0.18867 \n",
      "Validation loss decreased (0.183881 --> 0.175520).  Saving model ...\n",
      "[  9/300] train_loss: 0.16968 valid_loss: 0.17147 test_loss: 0.18641 \n",
      "Validation loss decreased (0.175520 --> 0.171474).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16473 valid_loss: 0.16491 test_loss: 0.17628 \n",
      "Validation loss decreased (0.171474 --> 0.164907).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15733 valid_loss: 0.16073 test_loss: 0.17245 \n",
      "Validation loss decreased (0.164907 --> 0.160729).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15474 valid_loss: 0.15657 test_loss: 0.16834 \n",
      "Validation loss decreased (0.160729 --> 0.156574).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14979 valid_loss: 0.15456 test_loss: 0.16723 \n",
      "Validation loss decreased (0.156574 --> 0.154557).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14453 valid_loss: 0.15037 test_loss: 0.15952 \n",
      "Validation loss decreased (0.154557 --> 0.150371).  Saving model ...\n",
      "[ 15/300] train_loss: 0.13896 valid_loss: 0.14351 test_loss: 0.15853 \n",
      "Validation loss decreased (0.150371 --> 0.143510).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13531 valid_loss: 0.14119 test_loss: 0.15369 \n",
      "Validation loss decreased (0.143510 --> 0.141188).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13512 valid_loss: 0.13858 test_loss: 0.15032 \n",
      "Validation loss decreased (0.141188 --> 0.138576).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13421 valid_loss: 0.13822 test_loss: 0.15160 \n",
      "Validation loss decreased (0.138576 --> 0.138220).  Saving model ...\n",
      "[ 19/300] train_loss: 0.12872 valid_loss: 0.13545 test_loss: 0.15029 \n",
      "Validation loss decreased (0.138220 --> 0.135451).  Saving model ...\n",
      "[ 20/300] train_loss: 0.12601 valid_loss: 0.13255 test_loss: 0.14636 \n",
      "Validation loss decreased (0.135451 --> 0.132555).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12639 valid_loss: 0.13097 test_loss: 0.14406 \n",
      "Validation loss decreased (0.132555 --> 0.130975).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12420 valid_loss: 0.12899 test_loss: 0.14124 \n",
      "Validation loss decreased (0.130975 --> 0.128993).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12406 valid_loss: 0.12690 test_loss: 0.13944 \n",
      "Validation loss decreased (0.128993 --> 0.126900).  Saving model ...\n",
      "[ 24/300] train_loss: 0.11844 valid_loss: 0.12453 test_loss: 0.13708 \n",
      "Validation loss decreased (0.126900 --> 0.124533).  Saving model ...\n",
      "[ 25/300] train_loss: 0.11956 valid_loss: 0.12527 test_loss: 0.13799 \n",
      "[ 26/300] train_loss: 0.11851 valid_loss: 0.12407 test_loss: 0.13707 \n",
      "Validation loss decreased (0.124533 --> 0.124070).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11756 valid_loss: 0.12181 test_loss: 0.13621 \n",
      "Validation loss decreased (0.124070 --> 0.121808).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11704 valid_loss: 0.11835 test_loss: 0.13383 \n",
      "Validation loss decreased (0.121808 --> 0.118348).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11579 valid_loss: 0.11941 test_loss: 0.13247 \n",
      "[ 30/300] train_loss: 0.11555 valid_loss: 0.12015 test_loss: 0.13479 \n",
      "[ 31/300] train_loss: 0.11309 valid_loss: 0.11716 test_loss: 0.13163 \n",
      "Validation loss decreased (0.118348 --> 0.117156).  Saving model ...\n",
      "[ 32/300] train_loss: 0.10960 valid_loss: 0.11776 test_loss: 0.13126 \n",
      "[ 33/300] train_loss: 0.11161 valid_loss: 0.11565 test_loss: 0.12977 \n",
      "Validation loss decreased (0.117156 --> 0.115651).  Saving model ...\n",
      "[ 34/300] train_loss: 0.10966 valid_loss: 0.11436 test_loss: 0.12842 \n",
      "Validation loss decreased (0.115651 --> 0.114363).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11242 valid_loss: 0.11363 test_loss: 0.12662 \n",
      "Validation loss decreased (0.114363 --> 0.113633).  Saving model ...\n",
      "[ 36/300] train_loss: 0.10957 valid_loss: 0.11338 test_loss: 0.12697 \n",
      "Validation loss decreased (0.113633 --> 0.113382).  Saving model ...\n",
      "[ 37/300] train_loss: 0.10831 valid_loss: 0.11483 test_loss: 0.12629 \n",
      "[ 38/300] train_loss: 0.10537 valid_loss: 0.11238 test_loss: 0.12530 \n",
      "Validation loss decreased (0.113382 --> 0.112378).  Saving model ...\n",
      "[ 39/300] train_loss: 0.10709 valid_loss: 0.11152 test_loss: 0.12503 \n",
      "Validation loss decreased (0.112378 --> 0.111522).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10478 valid_loss: 0.11202 test_loss: 0.12457 \n",
      "[ 41/300] train_loss: 0.10502 valid_loss: 0.11178 test_loss: 0.12432 \n",
      "[ 42/300] train_loss: 0.10246 valid_loss: 0.10905 test_loss: 0.12254 \n",
      "Validation loss decreased (0.111522 --> 0.109050).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10449 valid_loss: 0.10997 test_loss: 0.12447 \n",
      "[ 44/300] train_loss: 0.10532 valid_loss: 0.10903 test_loss: 0.12153 \n",
      "Validation loss decreased (0.109050 --> 0.109034).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10220 valid_loss: 0.10829 test_loss: 0.12010 \n",
      "Validation loss decreased (0.109034 --> 0.108289).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10609 valid_loss: 0.10746 test_loss: 0.11986 \n",
      "Validation loss decreased (0.108289 --> 0.107460).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10138 valid_loss: 0.10707 test_loss: 0.12054 \n",
      "Validation loss decreased (0.107460 --> 0.107071).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10155 valid_loss: 0.10637 test_loss: 0.11881 \n",
      "Validation loss decreased (0.107071 --> 0.106367).  Saving model ...\n",
      "[ 49/300] train_loss: 0.09900 valid_loss: 0.10813 test_loss: 0.11969 \n",
      "[ 50/300] train_loss: 0.09953 valid_loss: 0.10526 test_loss: 0.11847 \n",
      "Validation loss decreased (0.106367 --> 0.105263).  Saving model ...\n",
      "[ 51/300] train_loss: 0.09880 valid_loss: 0.10628 test_loss: 0.11930 \n",
      "[ 52/300] train_loss: 0.09944 valid_loss: 0.10379 test_loss: 0.11668 \n",
      "Validation loss decreased (0.105263 --> 0.103792).  Saving model ...\n",
      "[ 53/300] train_loss: 0.09904 valid_loss: 0.10224 test_loss: 0.11605 \n",
      "Validation loss decreased (0.103792 --> 0.102237).  Saving model ...\n",
      "[ 54/300] train_loss: 0.09883 valid_loss: 0.10325 test_loss: 0.11986 \n",
      "[ 55/300] train_loss: 0.09739 valid_loss: 0.10338 test_loss: 0.11625 \n",
      "[ 56/300] train_loss: 0.09438 valid_loss: 0.10269 test_loss: 0.11505 \n",
      "[ 57/300] train_loss: 0.09598 valid_loss: 0.10298 test_loss: 0.11560 \n",
      "[ 58/300] train_loss: 0.09651 valid_loss: 0.10095 test_loss: 0.11274 \n",
      "Validation loss decreased (0.102237 --> 0.100945).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09459 valid_loss: 0.10194 test_loss: 0.11330 \n",
      "[ 60/300] train_loss: 0.09722 valid_loss: 0.10408 test_loss: 0.11347 \n",
      "[ 61/300] train_loss: 0.09631 valid_loss: 0.10251 test_loss: 0.11305 \n",
      "[ 62/300] train_loss: 0.09377 valid_loss: 0.10072 test_loss: 0.11158 \n",
      "Validation loss decreased (0.100945 --> 0.100718).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09676 valid_loss: 0.09901 test_loss: 0.11182 \n",
      "Validation loss decreased (0.100718 --> 0.099014).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09415 valid_loss: 0.09890 test_loss: 0.11060 \n",
      "Validation loss decreased (0.099014 --> 0.098903).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09310 valid_loss: 0.09999 test_loss: 0.11148 \n",
      "[ 66/300] train_loss: 0.09315 valid_loss: 0.10124 test_loss: 0.11191 \n",
      "[ 67/300] train_loss: 0.09145 valid_loss: 0.10027 test_loss: 0.11197 \n",
      "[ 68/300] train_loss: 0.09126 valid_loss: 0.09806 test_loss: 0.11002 \n",
      "Validation loss decreased (0.098903 --> 0.098059).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09115 valid_loss: 0.09775 test_loss: 0.11067 \n",
      "Validation loss decreased (0.098059 --> 0.097753).  Saving model ...\n",
      "[ 70/300] train_loss: 0.08997 valid_loss: 0.09846 test_loss: 0.11015 \n",
      "[ 71/300] train_loss: 0.09226 valid_loss: 0.09677 test_loss: 0.10918 \n",
      "Validation loss decreased (0.097753 --> 0.096767).  Saving model ...\n",
      "[ 72/300] train_loss: 0.08937 valid_loss: 0.09783 test_loss: 0.10969 \n",
      "[ 73/300] train_loss: 0.08941 valid_loss: 0.09785 test_loss: 0.10976 \n",
      "[ 74/300] train_loss: 0.09194 valid_loss: 0.09627 test_loss: 0.10709 \n",
      "Validation loss decreased (0.096767 --> 0.096272).  Saving model ...\n",
      "[ 75/300] train_loss: 0.08903 valid_loss: 0.09490 test_loss: 0.10763 \n",
      "Validation loss decreased (0.096272 --> 0.094902).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09035 valid_loss: 0.09507 test_loss: 0.10693 \n",
      "[ 77/300] train_loss: 0.08853 valid_loss: 0.09586 test_loss: 0.10818 \n",
      "[ 78/300] train_loss: 0.08659 valid_loss: 0.09451 test_loss: 0.10850 \n",
      "Validation loss decreased (0.094902 --> 0.094509).  Saving model ...\n",
      "[ 79/300] train_loss: 0.08761 valid_loss: 0.09472 test_loss: 0.10605 \n",
      "[ 80/300] train_loss: 0.08908 valid_loss: 0.09587 test_loss: 0.10521 \n",
      "[ 81/300] train_loss: 0.08613 valid_loss: 0.09519 test_loss: 0.10537 \n",
      "[ 82/300] train_loss: 0.08609 valid_loss: 0.09285 test_loss: 0.10429 \n",
      "Validation loss decreased (0.094509 --> 0.092846).  Saving model ...\n",
      "[ 83/300] train_loss: 0.08736 valid_loss: 0.09355 test_loss: 0.10487 \n",
      "[ 84/300] train_loss: 0.08883 valid_loss: 0.09367 test_loss: 0.10475 \n",
      "[ 85/300] train_loss: 0.08563 valid_loss: 0.09174 test_loss: 0.10435 \n",
      "Validation loss decreased (0.092846 --> 0.091740).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08537 valid_loss: 0.09340 test_loss: 0.10412 \n",
      "[ 87/300] train_loss: 0.08839 valid_loss: 0.09193 test_loss: 0.10450 \n",
      "[ 88/300] train_loss: 0.08822 valid_loss: 0.09397 test_loss: 0.10522 \n",
      "[ 89/300] train_loss: 0.08298 valid_loss: 0.09510 test_loss: 0.10604 \n",
      "[ 90/300] train_loss: 0.08590 valid_loss: 0.09303 test_loss: 0.10381 \n",
      "[ 91/300] train_loss: 0.08325 valid_loss: 0.09212 test_loss: 0.10360 \n",
      "[ 92/300] train_loss: 0.08310 valid_loss: 0.09184 test_loss: 0.10312 \n",
      "[ 93/300] train_loss: 0.08222 valid_loss: 0.09036 test_loss: 0.10212 \n",
      "Validation loss decreased (0.091740 --> 0.090365).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08487 valid_loss: 0.09083 test_loss: 0.10142 \n",
      "[ 95/300] train_loss: 0.08425 valid_loss: 0.09020 test_loss: 0.10241 \n",
      "Validation loss decreased (0.090365 --> 0.090198).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08447 valid_loss: 0.09130 test_loss: 0.10252 \n",
      "[ 97/300] train_loss: 0.08225 valid_loss: 0.09104 test_loss: 0.10205 \n",
      "[ 98/300] train_loss: 0.08236 valid_loss: 0.09012 test_loss: 0.10131 \n",
      "Validation loss decreased (0.090198 --> 0.090121).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08384 valid_loss: 0.09082 test_loss: 0.10139 \n",
      "[100/300] train_loss: 0.08289 valid_loss: 0.09130 test_loss: 0.10192 \n",
      "[101/300] train_loss: 0.08433 valid_loss: 0.09253 test_loss: 0.10292 \n",
      "[102/300] train_loss: 0.08443 valid_loss: 0.08887 test_loss: 0.10006 \n",
      "Validation loss decreased (0.090121 --> 0.088869).  Saving model ...\n",
      "[103/300] train_loss: 0.08310 valid_loss: 0.09034 test_loss: 0.10090 \n",
      "[104/300] train_loss: 0.08178 valid_loss: 0.09186 test_loss: 0.10304 \n",
      "[105/300] train_loss: 0.08352 valid_loss: 0.09002 test_loss: 0.10033 \n",
      "[106/300] train_loss: 0.08327 valid_loss: 0.08850 test_loss: 0.10152 \n",
      "Validation loss decreased (0.088869 --> 0.088500).  Saving model ...\n",
      "[107/300] train_loss: 0.08243 valid_loss: 0.08879 test_loss: 0.10008 \n",
      "[108/300] train_loss: 0.08115 valid_loss: 0.08804 test_loss: 0.09820 \n",
      "Validation loss decreased (0.088500 --> 0.088035).  Saving model ...\n",
      "[109/300] train_loss: 0.08234 valid_loss: 0.08848 test_loss: 0.09899 \n",
      "[110/300] train_loss: 0.08054 valid_loss: 0.08921 test_loss: 0.09946 \n",
      "[111/300] train_loss: 0.08089 valid_loss: 0.08793 test_loss: 0.09835 \n",
      "Validation loss decreased (0.088035 --> 0.087934).  Saving model ...\n",
      "[112/300] train_loss: 0.07873 valid_loss: 0.08762 test_loss: 0.09749 \n",
      "Validation loss decreased (0.087934 --> 0.087621).  Saving model ...\n",
      "[113/300] train_loss: 0.08041 valid_loss: 0.08723 test_loss: 0.09785 \n",
      "Validation loss decreased (0.087621 --> 0.087226).  Saving model ...\n",
      "[114/300] train_loss: 0.08123 valid_loss: 0.08662 test_loss: 0.09791 \n",
      "Validation loss decreased (0.087226 --> 0.086623).  Saving model ...\n",
      "[115/300] train_loss: 0.08066 valid_loss: 0.08667 test_loss: 0.09805 \n",
      "[116/300] train_loss: 0.07823 valid_loss: 0.08732 test_loss: 0.09827 \n",
      "[117/300] train_loss: 0.07854 valid_loss: 0.08660 test_loss: 0.09790 \n",
      "Validation loss decreased (0.086623 --> 0.086598).  Saving model ...\n",
      "[118/300] train_loss: 0.07958 valid_loss: 0.08702 test_loss: 0.09756 \n",
      "[119/300] train_loss: 0.07707 valid_loss: 0.08609 test_loss: 0.09733 \n",
      "Validation loss decreased (0.086598 --> 0.086089).  Saving model ...\n",
      "[120/300] train_loss: 0.07807 valid_loss: 0.08538 test_loss: 0.09625 \n",
      "Validation loss decreased (0.086089 --> 0.085384).  Saving model ...\n",
      "[121/300] train_loss: 0.07961 valid_loss: 0.08573 test_loss: 0.09776 \n",
      "[122/300] train_loss: 0.08042 valid_loss: 0.08691 test_loss: 0.09659 \n",
      "[123/300] train_loss: 0.08037 valid_loss: 0.08623 test_loss: 0.09889 \n",
      "[124/300] train_loss: 0.07852 valid_loss: 0.08597 test_loss: 0.09776 \n",
      "[125/300] train_loss: 0.07753 valid_loss: 0.08797 test_loss: 0.09857 \n",
      "[126/300] train_loss: 0.07683 valid_loss: 0.08703 test_loss: 0.09744 \n",
      "[127/300] train_loss: 0.07714 valid_loss: 0.08474 test_loss: 0.09726 \n",
      "Validation loss decreased (0.085384 --> 0.084738).  Saving model ...\n",
      "[128/300] train_loss: 0.07717 valid_loss: 0.08616 test_loss: 0.09741 \n",
      "[129/300] train_loss: 0.07841 valid_loss: 0.08552 test_loss: 0.09673 \n",
      "[130/300] train_loss: 0.07474 valid_loss: 0.08524 test_loss: 0.09672 \n",
      "[131/300] train_loss: 0.07858 valid_loss: 0.08613 test_loss: 0.09680 \n",
      "[132/300] train_loss: 0.07679 valid_loss: 0.08578 test_loss: 0.09605 \n",
      "[133/300] train_loss: 0.07607 valid_loss: 0.08538 test_loss: 0.09653 \n",
      "[134/300] train_loss: 0.07634 valid_loss: 0.08603 test_loss: 0.09618 \n",
      "[135/300] train_loss: 0.07569 valid_loss: 0.08399 test_loss: 0.09575 \n",
      "Validation loss decreased (0.084738 --> 0.083988).  Saving model ...\n",
      "[136/300] train_loss: 0.07596 valid_loss: 0.08484 test_loss: 0.09509 \n",
      "[137/300] train_loss: 0.07430 valid_loss: 0.08617 test_loss: 0.09654 \n",
      "[138/300] train_loss: 0.07535 valid_loss: 0.08286 test_loss: 0.09521 \n",
      "Validation loss decreased (0.083988 --> 0.082859).  Saving model ...\n",
      "[139/300] train_loss: 0.07618 valid_loss: 0.08509 test_loss: 0.09516 \n",
      "[140/300] train_loss: 0.07642 valid_loss: 0.08325 test_loss: 0.09537 \n",
      "[141/300] train_loss: 0.07519 valid_loss: 0.08358 test_loss: 0.09436 \n",
      "[142/300] train_loss: 0.07606 valid_loss: 0.08260 test_loss: 0.09447 \n",
      "Validation loss decreased (0.082859 --> 0.082600).  Saving model ...\n",
      "[143/300] train_loss: 0.07862 valid_loss: 0.08368 test_loss: 0.09447 \n",
      "[144/300] train_loss: 0.07504 valid_loss: 0.08743 test_loss: 0.09775 \n",
      "[145/300] train_loss: 0.07458 valid_loss: 0.08398 test_loss: 0.09456 \n",
      "[146/300] train_loss: 0.07371 valid_loss: 0.08429 test_loss: 0.09431 \n",
      "[147/300] train_loss: 0.07586 valid_loss: 0.08202 test_loss: 0.09468 \n",
      "Validation loss decreased (0.082600 --> 0.082022).  Saving model ...\n",
      "[148/300] train_loss: 0.07574 valid_loss: 0.08446 test_loss: 0.09374 \n",
      "[149/300] train_loss: 0.07420 valid_loss: 0.08207 test_loss: 0.09358 \n",
      "[150/300] train_loss: 0.07706 valid_loss: 0.08414 test_loss: 0.09520 \n",
      "[151/300] train_loss: 0.07496 valid_loss: 0.08302 test_loss: 0.09318 \n",
      "[152/300] train_loss: 0.07629 valid_loss: 0.08442 test_loss: 0.09494 \n",
      "[153/300] train_loss: 0.07373 valid_loss: 0.08250 test_loss: 0.09324 \n",
      "[154/300] train_loss: 0.07527 valid_loss: 0.08177 test_loss: 0.09265 \n",
      "Validation loss decreased (0.082022 --> 0.081774).  Saving model ...\n",
      "[155/300] train_loss: 0.07477 valid_loss: 0.08339 test_loss: 0.09361 \n",
      "[156/300] train_loss: 0.07456 valid_loss: 0.08066 test_loss: 0.09332 \n",
      "Validation loss decreased (0.081774 --> 0.080664).  Saving model ...\n",
      "[157/300] train_loss: 0.07554 valid_loss: 0.08294 test_loss: 0.09410 \n",
      "[158/300] train_loss: 0.07200 valid_loss: 0.08179 test_loss: 0.09259 \n",
      "[159/300] train_loss: 0.07500 valid_loss: 0.08264 test_loss: 0.09355 \n",
      "[160/300] train_loss: 0.07550 valid_loss: 0.08080 test_loss: 0.09252 \n",
      "[161/300] train_loss: 0.07293 valid_loss: 0.08175 test_loss: 0.09292 \n",
      "[162/300] train_loss: 0.07387 valid_loss: 0.08174 test_loss: 0.09348 \n",
      "[163/300] train_loss: 0.07293 valid_loss: 0.08357 test_loss: 0.09315 \n",
      "[164/300] train_loss: 0.07235 valid_loss: 0.08345 test_loss: 0.09345 \n",
      "[165/300] train_loss: 0.07383 valid_loss: 0.08271 test_loss: 0.09403 \n",
      "[166/300] train_loss: 0.07259 valid_loss: 0.08211 test_loss: 0.09215 \n",
      "[167/300] train_loss: 0.07129 valid_loss: 0.08158 test_loss: 0.09288 \n",
      "[168/300] train_loss: 0.07286 valid_loss: 0.08251 test_loss: 0.09186 \n",
      "[169/300] train_loss: 0.07046 valid_loss: 0.08158 test_loss: 0.09213 \n",
      "[170/300] train_loss: 0.07241 valid_loss: 0.08124 test_loss: 0.09208 \n",
      "[171/300] train_loss: 0.06977 valid_loss: 0.08157 test_loss: 0.09110 \n",
      "[172/300] train_loss: 0.07149 valid_loss: 0.08130 test_loss: 0.09108 \n",
      "[173/300] train_loss: 0.06891 valid_loss: 0.08198 test_loss: 0.09170 \n",
      "[174/300] train_loss: 0.07340 valid_loss: 0.08038 test_loss: 0.09043 \n",
      "Validation loss decreased (0.080664 --> 0.080381).  Saving model ...\n",
      "[175/300] train_loss: 0.07014 valid_loss: 0.08077 test_loss: 0.09051 \n",
      "[176/300] train_loss: 0.07197 valid_loss: 0.08177 test_loss: 0.09241 \n",
      "[177/300] train_loss: 0.07000 valid_loss: 0.08175 test_loss: 0.09116 \n",
      "[178/300] train_loss: 0.07134 valid_loss: 0.08082 test_loss: 0.09204 \n",
      "[179/300] train_loss: 0.07487 valid_loss: 0.08291 test_loss: 0.09127 \n",
      "[180/300] train_loss: 0.06983 valid_loss: 0.07989 test_loss: 0.09079 \n",
      "Validation loss decreased (0.080381 --> 0.079895).  Saving model ...\n",
      "[181/300] train_loss: 0.07060 valid_loss: 0.08165 test_loss: 0.09301 \n",
      "[182/300] train_loss: 0.07212 valid_loss: 0.08051 test_loss: 0.09016 \n",
      "[183/300] train_loss: 0.07085 valid_loss: 0.08028 test_loss: 0.09045 \n",
      "[184/300] train_loss: 0.07073 valid_loss: 0.08069 test_loss: 0.09209 \n",
      "[185/300] train_loss: 0.07043 valid_loss: 0.07997 test_loss: 0.09088 \n",
      "[186/300] train_loss: 0.06791 valid_loss: 0.08007 test_loss: 0.09040 \n",
      "[187/300] train_loss: 0.06780 valid_loss: 0.08291 test_loss: 0.09220 \n",
      "[188/300] train_loss: 0.06837 valid_loss: 0.07918 test_loss: 0.08996 \n",
      "Validation loss decreased (0.079895 --> 0.079177).  Saving model ...\n",
      "[189/300] train_loss: 0.06807 valid_loss: 0.08042 test_loss: 0.09061 \n",
      "[190/300] train_loss: 0.07100 valid_loss: 0.07952 test_loss: 0.09164 \n",
      "[191/300] train_loss: 0.06924 valid_loss: 0.07974 test_loss: 0.09022 \n",
      "[192/300] train_loss: 0.06862 valid_loss: 0.07961 test_loss: 0.08943 \n",
      "[193/300] train_loss: 0.07048 valid_loss: 0.08301 test_loss: 0.09113 \n",
      "[194/300] train_loss: 0.06885 valid_loss: 0.07978 test_loss: 0.09014 \n",
      "[195/300] train_loss: 0.06881 valid_loss: 0.07902 test_loss: 0.08886 \n",
      "Validation loss decreased (0.079177 --> 0.079023).  Saving model ...\n",
      "[196/300] train_loss: 0.06799 valid_loss: 0.07902 test_loss: 0.08935 \n",
      "Validation loss decreased (0.079023 --> 0.079022).  Saving model ...\n",
      "[197/300] train_loss: 0.06855 valid_loss: 0.08087 test_loss: 0.08996 \n",
      "[198/300] train_loss: 0.06894 valid_loss: 0.07941 test_loss: 0.08940 \n",
      "[199/300] train_loss: 0.06906 valid_loss: 0.08115 test_loss: 0.09019 \n",
      "[200/300] train_loss: 0.06733 valid_loss: 0.07898 test_loss: 0.08887 \n",
      "Validation loss decreased (0.079022 --> 0.078977).  Saving model ...\n",
      "[201/300] train_loss: 0.06729 valid_loss: 0.07798 test_loss: 0.08920 \n",
      "Validation loss decreased (0.078977 --> 0.077976).  Saving model ...\n",
      "[202/300] train_loss: 0.06778 valid_loss: 0.08032 test_loss: 0.08900 \n",
      "[203/300] train_loss: 0.06625 valid_loss: 0.07986 test_loss: 0.08987 \n",
      "[204/300] train_loss: 0.06811 valid_loss: 0.07954 test_loss: 0.08996 \n",
      "[205/300] train_loss: 0.06875 valid_loss: 0.07908 test_loss: 0.08875 \n",
      "[206/300] train_loss: 0.06936 valid_loss: 0.07831 test_loss: 0.08865 \n",
      "[207/300] train_loss: 0.07017 valid_loss: 0.07926 test_loss: 0.08874 \n",
      "[208/300] train_loss: 0.06820 valid_loss: 0.07843 test_loss: 0.08847 \n",
      "[209/300] train_loss: 0.06807 valid_loss: 0.07909 test_loss: 0.08934 \n",
      "[210/300] train_loss: 0.06831 valid_loss: 0.07891 test_loss: 0.08912 \n",
      "[211/300] train_loss: 0.06855 valid_loss: 0.07998 test_loss: 0.08937 \n",
      "[212/300] train_loss: 0.06693 valid_loss: 0.07932 test_loss: 0.08934 \n",
      "[213/300] train_loss: 0.06694 valid_loss: 0.07955 test_loss: 0.08885 \n",
      "[214/300] train_loss: 0.06591 valid_loss: 0.07872 test_loss: 0.08830 \n",
      "[215/300] train_loss: 0.06645 valid_loss: 0.07875 test_loss: 0.09005 \n",
      "[216/300] train_loss: 0.06752 valid_loss: 0.07954 test_loss: 0.08947 \n",
      "[217/300] train_loss: 0.06877 valid_loss: 0.07780 test_loss: 0.08793 \n",
      "Validation loss decreased (0.077976 --> 0.077801).  Saving model ...\n",
      "[218/300] train_loss: 0.06532 valid_loss: 0.07821 test_loss: 0.08835 \n",
      "[219/300] train_loss: 0.06833 valid_loss: 0.07748 test_loss: 0.08749 \n",
      "Validation loss decreased (0.077801 --> 0.077483).  Saving model ...\n",
      "[220/300] train_loss: 0.06841 valid_loss: 0.07787 test_loss: 0.08775 \n",
      "[221/300] train_loss: 0.06481 valid_loss: 0.07803 test_loss: 0.08784 \n",
      "[222/300] train_loss: 0.06395 valid_loss: 0.07895 test_loss: 0.08799 \n",
      "[223/300] train_loss: 0.06796 valid_loss: 0.07846 test_loss: 0.08763 \n",
      "[224/300] train_loss: 0.06382 valid_loss: 0.07800 test_loss: 0.08775 \n",
      "[225/300] train_loss: 0.06785 valid_loss: 0.07821 test_loss: 0.08813 \n",
      "[226/300] train_loss: 0.06586 valid_loss: 0.07769 test_loss: 0.08802 \n",
      "[227/300] train_loss: 0.06584 valid_loss: 0.07824 test_loss: 0.08726 \n",
      "[228/300] train_loss: 0.06676 valid_loss: 0.07706 test_loss: 0.08903 \n",
      "Validation loss decreased (0.077483 --> 0.077056).  Saving model ...\n",
      "[229/300] train_loss: 0.06612 valid_loss: 0.07820 test_loss: 0.08846 \n",
      "[230/300] train_loss: 0.06818 valid_loss: 0.07834 test_loss: 0.08807 \n",
      "[231/300] train_loss: 0.06657 valid_loss: 0.07973 test_loss: 0.08869 \n",
      "[232/300] train_loss: 0.06706 valid_loss: 0.07773 test_loss: 0.08770 \n",
      "[233/300] train_loss: 0.06686 valid_loss: 0.07714 test_loss: 0.08860 \n",
      "[234/300] train_loss: 0.06769 valid_loss: 0.07749 test_loss: 0.08753 \n",
      "[235/300] train_loss: 0.06686 valid_loss: 0.07826 test_loss: 0.08762 \n",
      "[236/300] train_loss: 0.06488 valid_loss: 0.07825 test_loss: 0.08808 \n",
      "[237/300] train_loss: 0.06668 valid_loss: 0.07682 test_loss: 0.08911 \n",
      "Validation loss decreased (0.077056 --> 0.076824).  Saving model ...\n",
      "[238/300] train_loss: 0.06542 valid_loss: 0.07763 test_loss: 0.08866 \n",
      "[239/300] train_loss: 0.06344 valid_loss: 0.07881 test_loss: 0.08779 \n",
      "[240/300] train_loss: 0.06445 valid_loss: 0.07755 test_loss: 0.08726 \n",
      "[241/300] train_loss: 0.06473 valid_loss: 0.07743 test_loss: 0.08829 \n",
      "[242/300] train_loss: 0.06187 valid_loss: 0.07666 test_loss: 0.08794 \n",
      "Validation loss decreased (0.076824 --> 0.076660).  Saving model ...\n",
      "[243/300] train_loss: 0.06583 valid_loss: 0.07533 test_loss: 0.08635 \n",
      "Validation loss decreased (0.076660 --> 0.075332).  Saving model ...\n",
      "[244/300] train_loss: 0.06454 valid_loss: 0.07623 test_loss: 0.08589 \n",
      "[245/300] train_loss: 0.06489 valid_loss: 0.07660 test_loss: 0.08755 \n",
      "[246/300] train_loss: 0.06374 valid_loss: 0.07656 test_loss: 0.08837 \n",
      "[247/300] train_loss: 0.06556 valid_loss: 0.07754 test_loss: 0.08834 \n",
      "[248/300] train_loss: 0.06467 valid_loss: 0.07757 test_loss: 0.08714 \n",
      "[249/300] train_loss: 0.06369 valid_loss: 0.07718 test_loss: 0.08666 \n",
      "[250/300] train_loss: 0.06462 valid_loss: 0.07764 test_loss: 0.08673 \n",
      "[251/300] train_loss: 0.06399 valid_loss: 0.07742 test_loss: 0.08795 \n",
      "[252/300] train_loss: 0.06436 valid_loss: 0.07592 test_loss: 0.08700 \n",
      "[253/300] train_loss: 0.06444 valid_loss: 0.07746 test_loss: 0.08718 \n",
      "[254/300] train_loss: 0.06542 valid_loss: 0.07686 test_loss: 0.08704 \n",
      "[255/300] train_loss: 0.06284 valid_loss: 0.07772 test_loss: 0.08735 \n",
      "[256/300] train_loss: 0.06492 valid_loss: 0.07721 test_loss: 0.08774 \n",
      "[257/300] train_loss: 0.06538 valid_loss: 0.07609 test_loss: 0.08663 \n",
      "[258/300] train_loss: 0.06387 valid_loss: 0.07644 test_loss: 0.08684 \n",
      "[259/300] train_loss: 0.06588 valid_loss: 0.07647 test_loss: 0.08604 \n",
      "[260/300] train_loss: 0.06217 valid_loss: 0.07603 test_loss: 0.08621 \n",
      "[261/300] train_loss: 0.06308 valid_loss: 0.07583 test_loss: 0.08642 \n",
      "[262/300] train_loss: 0.06462 valid_loss: 0.07552 test_loss: 0.08556 \n",
      "[263/300] train_loss: 0.06371 valid_loss: 0.07648 test_loss: 0.08635 \n",
      "[264/300] train_loss: 0.06407 valid_loss: 0.07638 test_loss: 0.08663 \n",
      "[265/300] train_loss: 0.06334 valid_loss: 0.07516 test_loss: 0.08503 \n",
      "Validation loss decreased (0.075332 --> 0.075157).  Saving model ...\n",
      "[266/300] train_loss: 0.06440 valid_loss: 0.07584 test_loss: 0.08627 \n",
      "[267/300] train_loss: 0.06282 valid_loss: 0.07523 test_loss: 0.08587 \n",
      "[268/300] train_loss: 0.06168 valid_loss: 0.07544 test_loss: 0.08661 \n",
      "[269/300] train_loss: 0.06311 valid_loss: 0.07615 test_loss: 0.08709 \n",
      "[270/300] train_loss: 0.06543 valid_loss: 0.07615 test_loss: 0.08664 \n",
      "[271/300] train_loss: 0.06292 valid_loss: 0.07648 test_loss: 0.08717 \n",
      "[272/300] train_loss: 0.06234 valid_loss: 0.07523 test_loss: 0.08625 \n",
      "[273/300] train_loss: 0.06265 valid_loss: 0.07542 test_loss: 0.08624 \n",
      "[274/300] train_loss: 0.06338 valid_loss: 0.07679 test_loss: 0.08684 \n",
      "[275/300] train_loss: 0.06196 valid_loss: 0.07596 test_loss: 0.08582 \n",
      "[276/300] train_loss: 0.06331 valid_loss: 0.07615 test_loss: 0.08755 \n",
      "[277/300] train_loss: 0.06336 valid_loss: 0.07622 test_loss: 0.08641 \n",
      "[278/300] train_loss: 0.06515 valid_loss: 0.07770 test_loss: 0.08755 \n",
      "[279/300] train_loss: 0.06324 valid_loss: 0.07595 test_loss: 0.08569 \n",
      "[280/300] train_loss: 0.06241 valid_loss: 0.07546 test_loss: 0.08523 \n",
      "[281/300] train_loss: 0.06194 valid_loss: 0.07665 test_loss: 0.08651 \n",
      "[282/300] train_loss: 0.05961 valid_loss: 0.07642 test_loss: 0.08621 \n",
      "[283/300] train_loss: 0.06164 valid_loss: 0.07584 test_loss: 0.08606 \n",
      "[284/300] train_loss: 0.06343 valid_loss: 0.07638 test_loss: 0.08726 \n",
      "[285/300] train_loss: 0.06211 valid_loss: 0.07545 test_loss: 0.08669 \n",
      "[286/300] train_loss: 0.06311 valid_loss: 0.07509 test_loss: 0.08758 \n",
      "Validation loss decreased (0.075157 --> 0.075092).  Saving model ...\n",
      "[287/300] train_loss: 0.06360 valid_loss: 0.07677 test_loss: 0.08640 \n",
      "[288/300] train_loss: 0.06092 valid_loss: 0.07495 test_loss: 0.08590 \n",
      "Validation loss decreased (0.075092 --> 0.074955).  Saving model ...\n",
      "[289/300] train_loss: 0.06260 valid_loss: 0.07595 test_loss: 0.08629 \n",
      "[290/300] train_loss: 0.06147 valid_loss: 0.07484 test_loss: 0.08648 \n",
      "Validation loss decreased (0.074955 --> 0.074836).  Saving model ...\n",
      "[291/300] train_loss: 0.06184 valid_loss: 0.07552 test_loss: 0.08602 \n",
      "[292/300] train_loss: 0.06237 valid_loss: 0.07526 test_loss: 0.08544 \n",
      "[293/300] train_loss: 0.06241 valid_loss: 0.07521 test_loss: 0.08476 \n",
      "[294/300] train_loss: 0.06415 valid_loss: 0.07484 test_loss: 0.08491 \n",
      "[295/300] train_loss: 0.06179 valid_loss: 0.07550 test_loss: 0.08583 \n",
      "[296/300] train_loss: 0.06375 valid_loss: 0.07546 test_loss: 0.08648 \n",
      "[297/300] train_loss: 0.06083 valid_loss: 0.07568 test_loss: 0.08581 \n",
      "[298/300] train_loss: 0.06267 valid_loss: 0.07655 test_loss: 0.08694 \n",
      "[299/300] train_loss: 0.06228 valid_loss: 0.07461 test_loss: 0.08508 \n",
      "Validation loss decreased (0.074836 --> 0.074610).  Saving model ...\n",
      "[300/300] train_loss: 0.06022 valid_loss: 0.07648 test_loss: 0.08481 \n",
      "TRAINING MODEL 6\n",
      "[  1/300] train_loss: 0.58480 valid_loss: 0.48057 test_loss: 0.49233 \n",
      "Validation loss decreased (inf --> 0.480575).  Saving model ...\n",
      "[  2/300] train_loss: 0.38052 valid_loss: 0.34372 test_loss: 0.35413 \n",
      "Validation loss decreased (0.480575 --> 0.343718).  Saving model ...\n",
      "[  3/300] train_loss: 0.28650 valid_loss: 0.27734 test_loss: 0.28999 \n",
      "Validation loss decreased (0.343718 --> 0.277344).  Saving model ...\n",
      "[  4/300] train_loss: 0.24214 valid_loss: 0.24084 test_loss: 0.25744 \n",
      "Validation loss decreased (0.277344 --> 0.240845).  Saving model ...\n",
      "[  5/300] train_loss: 0.21551 valid_loss: 0.21727 test_loss: 0.23408 \n",
      "Validation loss decreased (0.240845 --> 0.217272).  Saving model ...\n",
      "[  6/300] train_loss: 0.19808 valid_loss: 0.20248 test_loss: 0.21767 \n",
      "Validation loss decreased (0.217272 --> 0.202484).  Saving model ...\n",
      "[  7/300] train_loss: 0.18715 valid_loss: 0.19506 test_loss: 0.21295 \n",
      "Validation loss decreased (0.202484 --> 0.195061).  Saving model ...\n",
      "[  8/300] train_loss: 0.18168 valid_loss: 0.18356 test_loss: 0.19712 \n",
      "Validation loss decreased (0.195061 --> 0.183559).  Saving model ...\n",
      "[  9/300] train_loss: 0.17185 valid_loss: 0.17633 test_loss: 0.18799 \n",
      "Validation loss decreased (0.183559 --> 0.176330).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16793 valid_loss: 0.17052 test_loss: 0.18130 \n",
      "Validation loss decreased (0.176330 --> 0.170520).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16266 valid_loss: 0.16339 test_loss: 0.17485 \n",
      "Validation loss decreased (0.170520 --> 0.163387).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16006 valid_loss: 0.16552 test_loss: 0.17645 \n",
      "[ 13/300] train_loss: 0.15282 valid_loss: 0.15974 test_loss: 0.17200 \n",
      "Validation loss decreased (0.163387 --> 0.159736).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14971 valid_loss: 0.16042 test_loss: 0.17478 \n",
      "[ 15/300] train_loss: 0.14374 valid_loss: 0.16018 test_loss: 0.17524 \n",
      "[ 16/300] train_loss: 0.14421 valid_loss: 0.15103 test_loss: 0.16273 \n",
      "Validation loss decreased (0.159736 --> 0.151032).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13814 valid_loss: 0.14946 test_loss: 0.16061 \n",
      "Validation loss decreased (0.151032 --> 0.149462).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13730 valid_loss: 0.14651 test_loss: 0.15918 \n",
      "Validation loss decreased (0.149462 --> 0.146513).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13752 valid_loss: 0.14523 test_loss: 0.15561 \n",
      "Validation loss decreased (0.146513 --> 0.145233).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13329 valid_loss: 0.14254 test_loss: 0.15459 \n",
      "Validation loss decreased (0.145233 --> 0.142539).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12993 valid_loss: 0.13934 test_loss: 0.15240 \n",
      "Validation loss decreased (0.142539 --> 0.139340).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12716 valid_loss: 0.13992 test_loss: 0.15317 \n",
      "[ 23/300] train_loss: 0.13100 valid_loss: 0.13786 test_loss: 0.15047 \n",
      "Validation loss decreased (0.139340 --> 0.137865).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12716 valid_loss: 0.13655 test_loss: 0.14977 \n",
      "Validation loss decreased (0.137865 --> 0.136554).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12823 valid_loss: 0.13535 test_loss: 0.14843 \n",
      "Validation loss decreased (0.136554 --> 0.135350).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12367 valid_loss: 0.13524 test_loss: 0.14659 \n",
      "Validation loss decreased (0.135350 --> 0.135239).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12554 valid_loss: 0.12963 test_loss: 0.14539 \n",
      "Validation loss decreased (0.135239 --> 0.129629).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12305 valid_loss: 0.13124 test_loss: 0.14475 \n",
      "[ 29/300] train_loss: 0.12137 valid_loss: 0.12918 test_loss: 0.14150 \n",
      "Validation loss decreased (0.129629 --> 0.129183).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11921 valid_loss: 0.12814 test_loss: 0.14203 \n",
      "Validation loss decreased (0.129183 --> 0.128141).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11792 valid_loss: 0.12733 test_loss: 0.14002 \n",
      "Validation loss decreased (0.128141 --> 0.127334).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11964 valid_loss: 0.12646 test_loss: 0.13963 \n",
      "Validation loss decreased (0.127334 --> 0.126458).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11518 valid_loss: 0.12532 test_loss: 0.13898 \n",
      "Validation loss decreased (0.126458 --> 0.125319).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11503 valid_loss: 0.12746 test_loss: 0.14078 \n",
      "[ 35/300] train_loss: 0.11438 valid_loss: 0.12259 test_loss: 0.13585 \n",
      "Validation loss decreased (0.125319 --> 0.122589).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11290 valid_loss: 0.12277 test_loss: 0.13545 \n",
      "[ 37/300] train_loss: 0.11071 valid_loss: 0.12079 test_loss: 0.13661 \n",
      "Validation loss decreased (0.122589 --> 0.120790).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11104 valid_loss: 0.12033 test_loss: 0.13472 \n",
      "Validation loss decreased (0.120790 --> 0.120334).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11261 valid_loss: 0.11890 test_loss: 0.13261 \n",
      "Validation loss decreased (0.120334 --> 0.118896).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11072 valid_loss: 0.11735 test_loss: 0.13061 \n",
      "Validation loss decreased (0.118896 --> 0.117351).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10864 valid_loss: 0.11822 test_loss: 0.13118 \n",
      "[ 42/300] train_loss: 0.10990 valid_loss: 0.11607 test_loss: 0.13161 \n",
      "Validation loss decreased (0.117351 --> 0.116072).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10477 valid_loss: 0.11514 test_loss: 0.12930 \n",
      "Validation loss decreased (0.116072 --> 0.115137).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10752 valid_loss: 0.11713 test_loss: 0.13133 \n",
      "[ 45/300] train_loss: 0.10385 valid_loss: 0.11649 test_loss: 0.12875 \n",
      "[ 46/300] train_loss: 0.10679 valid_loss: 0.11886 test_loss: 0.13242 \n",
      "[ 47/300] train_loss: 0.10526 valid_loss: 0.11414 test_loss: 0.12720 \n",
      "Validation loss decreased (0.115137 --> 0.114141).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10345 valid_loss: 0.11590 test_loss: 0.12674 \n",
      "[ 49/300] train_loss: 0.10301 valid_loss: 0.11493 test_loss: 0.12736 \n",
      "[ 50/300] train_loss: 0.10448 valid_loss: 0.11470 test_loss: 0.12709 \n",
      "[ 51/300] train_loss: 0.10316 valid_loss: 0.11468 test_loss: 0.12658 \n",
      "[ 52/300] train_loss: 0.10316 valid_loss: 0.11064 test_loss: 0.12328 \n",
      "Validation loss decreased (0.114141 --> 0.110644).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10055 valid_loss: 0.11340 test_loss: 0.12518 \n",
      "[ 54/300] train_loss: 0.10108 valid_loss: 0.10825 test_loss: 0.12145 \n",
      "Validation loss decreased (0.110644 --> 0.108252).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09877 valid_loss: 0.11030 test_loss: 0.12249 \n",
      "[ 56/300] train_loss: 0.09871 valid_loss: 0.11154 test_loss: 0.12336 \n",
      "[ 57/300] train_loss: 0.09953 valid_loss: 0.11006 test_loss: 0.12147 \n",
      "[ 58/300] train_loss: 0.09614 valid_loss: 0.10520 test_loss: 0.12143 \n",
      "Validation loss decreased (0.108252 --> 0.105199).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09867 valid_loss: 0.10576 test_loss: 0.11965 \n",
      "[ 60/300] train_loss: 0.09661 valid_loss: 0.10763 test_loss: 0.12131 \n",
      "[ 61/300] train_loss: 0.09786 valid_loss: 0.10732 test_loss: 0.12085 \n",
      "[ 62/300] train_loss: 0.09692 valid_loss: 0.10880 test_loss: 0.12114 \n",
      "[ 63/300] train_loss: 0.09618 valid_loss: 0.10552 test_loss: 0.11757 \n",
      "[ 64/300] train_loss: 0.09760 valid_loss: 0.10517 test_loss: 0.11736 \n",
      "Validation loss decreased (0.105199 --> 0.105171).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09653 valid_loss: 0.10501 test_loss: 0.11771 \n",
      "Validation loss decreased (0.105171 --> 0.105011).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09561 valid_loss: 0.10463 test_loss: 0.11750 \n",
      "Validation loss decreased (0.105011 --> 0.104632).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09415 valid_loss: 0.10486 test_loss: 0.11813 \n",
      "[ 68/300] train_loss: 0.09574 valid_loss: 0.10321 test_loss: 0.11751 \n",
      "Validation loss decreased (0.104632 --> 0.103208).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09465 valid_loss: 0.10184 test_loss: 0.11454 \n",
      "Validation loss decreased (0.103208 --> 0.101836).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09446 valid_loss: 0.10142 test_loss: 0.11469 \n",
      "Validation loss decreased (0.101836 --> 0.101424).  Saving model ...\n",
      "[ 71/300] train_loss: 0.09436 valid_loss: 0.10714 test_loss: 0.12023 \n",
      "[ 72/300] train_loss: 0.09178 valid_loss: 0.10182 test_loss: 0.11613 \n",
      "[ 73/300] train_loss: 0.09296 valid_loss: 0.10313 test_loss: 0.11561 \n",
      "[ 74/300] train_loss: 0.09404 valid_loss: 0.10504 test_loss: 0.11765 \n",
      "[ 75/300] train_loss: 0.09222 valid_loss: 0.09887 test_loss: 0.11405 \n",
      "Validation loss decreased (0.101424 --> 0.098870).  Saving model ...\n",
      "[ 76/300] train_loss: 0.08995 valid_loss: 0.10043 test_loss: 0.11325 \n",
      "[ 77/300] train_loss: 0.09241 valid_loss: 0.09809 test_loss: 0.11169 \n",
      "Validation loss decreased (0.098870 --> 0.098094).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09024 valid_loss: 0.10153 test_loss: 0.11389 \n",
      "[ 79/300] train_loss: 0.09278 valid_loss: 0.09726 test_loss: 0.11039 \n",
      "Validation loss decreased (0.098094 --> 0.097260).  Saving model ...\n",
      "[ 80/300] train_loss: 0.09071 valid_loss: 0.09944 test_loss: 0.11192 \n",
      "[ 81/300] train_loss: 0.09187 valid_loss: 0.10026 test_loss: 0.11280 \n",
      "[ 82/300] train_loss: 0.09133 valid_loss: 0.09786 test_loss: 0.11179 \n",
      "[ 83/300] train_loss: 0.08842 valid_loss: 0.09771 test_loss: 0.11169 \n",
      "[ 84/300] train_loss: 0.09167 valid_loss: 0.09739 test_loss: 0.11021 \n",
      "[ 85/300] train_loss: 0.08809 valid_loss: 0.09857 test_loss: 0.11135 \n",
      "[ 86/300] train_loss: 0.08653 valid_loss: 0.09550 test_loss: 0.10900 \n",
      "Validation loss decreased (0.097260 --> 0.095501).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08912 valid_loss: 0.09680 test_loss: 0.10913 \n",
      "[ 88/300] train_loss: 0.08639 valid_loss: 0.09664 test_loss: 0.11002 \n",
      "[ 89/300] train_loss: 0.08714 valid_loss: 0.09646 test_loss: 0.10883 \n",
      "[ 90/300] train_loss: 0.08626 valid_loss: 0.09589 test_loss: 0.10866 \n",
      "[ 91/300] train_loss: 0.08699 valid_loss: 0.09548 test_loss: 0.10844 \n",
      "Validation loss decreased (0.095501 --> 0.095484).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08710 valid_loss: 0.09501 test_loss: 0.10723 \n",
      "Validation loss decreased (0.095484 --> 0.095012).  Saving model ...\n",
      "[ 93/300] train_loss: 0.08889 valid_loss: 0.09634 test_loss: 0.10974 \n",
      "[ 94/300] train_loss: 0.08477 valid_loss: 0.09635 test_loss: 0.10947 \n",
      "[ 95/300] train_loss: 0.08556 valid_loss: 0.09448 test_loss: 0.10788 \n",
      "Validation loss decreased (0.095012 --> 0.094479).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08768 valid_loss: 0.09600 test_loss: 0.10768 \n",
      "[ 97/300] train_loss: 0.08538 valid_loss: 0.09542 test_loss: 0.10662 \n",
      "[ 98/300] train_loss: 0.08593 valid_loss: 0.09496 test_loss: 0.10752 \n",
      "[ 99/300] train_loss: 0.08592 valid_loss: 0.09371 test_loss: 0.10573 \n",
      "Validation loss decreased (0.094479 --> 0.093713).  Saving model ...\n",
      "[100/300] train_loss: 0.08544 valid_loss: 0.09502 test_loss: 0.10752 \n",
      "[101/300] train_loss: 0.08345 valid_loss: 0.09362 test_loss: 0.10599 \n",
      "Validation loss decreased (0.093713 --> 0.093623).  Saving model ...\n",
      "[102/300] train_loss: 0.08586 valid_loss: 0.09515 test_loss: 0.10694 \n",
      "[103/300] train_loss: 0.08617 valid_loss: 0.09348 test_loss: 0.10518 \n",
      "Validation loss decreased (0.093623 --> 0.093484).  Saving model ...\n",
      "[104/300] train_loss: 0.08316 valid_loss: 0.09388 test_loss: 0.10489 \n",
      "[105/300] train_loss: 0.08157 valid_loss: 0.09233 test_loss: 0.10448 \n",
      "Validation loss decreased (0.093484 --> 0.092330).  Saving model ...\n",
      "[106/300] train_loss: 0.08673 valid_loss: 0.09308 test_loss: 0.10483 \n",
      "[107/300] train_loss: 0.08233 valid_loss: 0.09269 test_loss: 0.10541 \n",
      "[108/300] train_loss: 0.08464 valid_loss: 0.09468 test_loss: 0.10648 \n",
      "[109/300] train_loss: 0.08445 valid_loss: 0.09154 test_loss: 0.10494 \n",
      "Validation loss decreased (0.092330 --> 0.091539).  Saving model ...\n",
      "[110/300] train_loss: 0.08335 valid_loss: 0.09330 test_loss: 0.10536 \n",
      "[111/300] train_loss: 0.08207 valid_loss: 0.09281 test_loss: 0.10597 \n",
      "[112/300] train_loss: 0.08279 valid_loss: 0.09530 test_loss: 0.10618 \n",
      "[113/300] train_loss: 0.08380 valid_loss: 0.09373 test_loss: 0.10579 \n",
      "[114/300] train_loss: 0.08136 valid_loss: 0.09167 test_loss: 0.10555 \n",
      "[115/300] train_loss: 0.08191 valid_loss: 0.09027 test_loss: 0.10357 \n",
      "Validation loss decreased (0.091539 --> 0.090268).  Saving model ...\n",
      "[116/300] train_loss: 0.08138 valid_loss: 0.09051 test_loss: 0.10302 \n",
      "[117/300] train_loss: 0.08092 valid_loss: 0.09012 test_loss: 0.10324 \n",
      "Validation loss decreased (0.090268 --> 0.090116).  Saving model ...\n",
      "[118/300] train_loss: 0.08129 valid_loss: 0.09018 test_loss: 0.10290 \n",
      "[119/300] train_loss: 0.08108 valid_loss: 0.08960 test_loss: 0.10195 \n",
      "Validation loss decreased (0.090116 --> 0.089602).  Saving model ...\n",
      "[120/300] train_loss: 0.08215 valid_loss: 0.08928 test_loss: 0.10276 \n",
      "Validation loss decreased (0.089602 --> 0.089284).  Saving model ...\n",
      "[121/300] train_loss: 0.07763 valid_loss: 0.08882 test_loss: 0.10168 \n",
      "Validation loss decreased (0.089284 --> 0.088818).  Saving model ...\n",
      "[122/300] train_loss: 0.08062 valid_loss: 0.09157 test_loss: 0.10298 \n",
      "[123/300] train_loss: 0.08021 valid_loss: 0.09280 test_loss: 0.10444 \n",
      "[124/300] train_loss: 0.08044 valid_loss: 0.08960 test_loss: 0.10175 \n",
      "[125/300] train_loss: 0.08109 valid_loss: 0.09105 test_loss: 0.10305 \n",
      "[126/300] train_loss: 0.07889 valid_loss: 0.08966 test_loss: 0.10252 \n",
      "[127/300] train_loss: 0.08019 valid_loss: 0.09052 test_loss: 0.10172 \n",
      "[128/300] train_loss: 0.08008 valid_loss: 0.09072 test_loss: 0.10300 \n",
      "[129/300] train_loss: 0.07948 valid_loss: 0.08844 test_loss: 0.10078 \n",
      "Validation loss decreased (0.088818 --> 0.088436).  Saving model ...\n",
      "[130/300] train_loss: 0.07840 valid_loss: 0.08717 test_loss: 0.10043 \n",
      "Validation loss decreased (0.088436 --> 0.087174).  Saving model ...\n",
      "[131/300] train_loss: 0.08020 valid_loss: 0.08797 test_loss: 0.10040 \n",
      "[132/300] train_loss: 0.07768 valid_loss: 0.08671 test_loss: 0.09973 \n",
      "Validation loss decreased (0.087174 --> 0.086711).  Saving model ...\n",
      "[133/300] train_loss: 0.08026 valid_loss: 0.08778 test_loss: 0.10066 \n",
      "[134/300] train_loss: 0.08058 valid_loss: 0.08718 test_loss: 0.10032 \n",
      "[135/300] train_loss: 0.07688 valid_loss: 0.08876 test_loss: 0.10103 \n",
      "[136/300] train_loss: 0.07798 valid_loss: 0.08912 test_loss: 0.09998 \n",
      "[137/300] train_loss: 0.07812 valid_loss: 0.08657 test_loss: 0.09892 \n",
      "Validation loss decreased (0.086711 --> 0.086567).  Saving model ...\n",
      "[138/300] train_loss: 0.07834 valid_loss: 0.08638 test_loss: 0.09922 \n",
      "Validation loss decreased (0.086567 --> 0.086382).  Saving model ...\n",
      "[139/300] train_loss: 0.07874 valid_loss: 0.08764 test_loss: 0.09930 \n",
      "[140/300] train_loss: 0.07678 valid_loss: 0.08783 test_loss: 0.10025 \n",
      "[141/300] train_loss: 0.07852 valid_loss: 0.08759 test_loss: 0.10204 \n",
      "[142/300] train_loss: 0.07717 valid_loss: 0.08690 test_loss: 0.09897 \n",
      "[143/300] train_loss: 0.07733 valid_loss: 0.08686 test_loss: 0.09981 \n",
      "[144/300] train_loss: 0.07592 valid_loss: 0.08769 test_loss: 0.10044 \n",
      "[145/300] train_loss: 0.07671 valid_loss: 0.08740 test_loss: 0.10052 \n",
      "[146/300] train_loss: 0.07759 valid_loss: 0.08571 test_loss: 0.09872 \n",
      "Validation loss decreased (0.086382 --> 0.085712).  Saving model ...\n",
      "[147/300] train_loss: 0.07545 valid_loss: 0.08573 test_loss: 0.09712 \n",
      "[148/300] train_loss: 0.07538 valid_loss: 0.08629 test_loss: 0.09946 \n",
      "[149/300] train_loss: 0.07637 valid_loss: 0.08587 test_loss: 0.09842 \n",
      "[150/300] train_loss: 0.07514 valid_loss: 0.08699 test_loss: 0.09932 \n",
      "[151/300] train_loss: 0.07726 valid_loss: 0.08551 test_loss: 0.09716 \n",
      "Validation loss decreased (0.085712 --> 0.085511).  Saving model ...\n",
      "[152/300] train_loss: 0.07651 valid_loss: 0.08696 test_loss: 0.09922 \n",
      "[153/300] train_loss: 0.07387 valid_loss: 0.08821 test_loss: 0.10080 \n",
      "[154/300] train_loss: 0.07419 valid_loss: 0.08610 test_loss: 0.09662 \n",
      "[155/300] train_loss: 0.07642 valid_loss: 0.08592 test_loss: 0.09805 \n",
      "[156/300] train_loss: 0.07314 valid_loss: 0.08551 test_loss: 0.09797 \n",
      "Validation loss decreased (0.085511 --> 0.085510).  Saving model ...\n",
      "[157/300] train_loss: 0.07313 valid_loss: 0.08619 test_loss: 0.09773 \n",
      "[158/300] train_loss: 0.07548 valid_loss: 0.08625 test_loss: 0.09786 \n",
      "[159/300] train_loss: 0.07615 valid_loss: 0.08537 test_loss: 0.09767 \n",
      "Validation loss decreased (0.085510 --> 0.085374).  Saving model ...\n",
      "[160/300] train_loss: 0.07471 valid_loss: 0.08585 test_loss: 0.09920 \n",
      "[161/300] train_loss: 0.07549 valid_loss: 0.08518 test_loss: 0.09800 \n",
      "Validation loss decreased (0.085374 --> 0.085181).  Saving model ...\n",
      "[162/300] train_loss: 0.07520 valid_loss: 0.08644 test_loss: 0.09951 \n",
      "[163/300] train_loss: 0.07453 valid_loss: 0.08484 test_loss: 0.09685 \n",
      "Validation loss decreased (0.085181 --> 0.084840).  Saving model ...\n",
      "[164/300] train_loss: 0.07266 valid_loss: 0.08380 test_loss: 0.09736 \n",
      "Validation loss decreased (0.084840 --> 0.083802).  Saving model ...\n",
      "[165/300] train_loss: 0.07509 valid_loss: 0.08619 test_loss: 0.09738 \n",
      "[166/300] train_loss: 0.07630 valid_loss: 0.08401 test_loss: 0.09770 \n",
      "[167/300] train_loss: 0.07131 valid_loss: 0.08413 test_loss: 0.09702 \n",
      "[168/300] train_loss: 0.07292 valid_loss: 0.08499 test_loss: 0.09782 \n",
      "[169/300] train_loss: 0.07273 valid_loss: 0.08524 test_loss: 0.09682 \n",
      "[170/300] train_loss: 0.07312 valid_loss: 0.08549 test_loss: 0.09825 \n",
      "[171/300] train_loss: 0.07237 valid_loss: 0.08322 test_loss: 0.09596 \n",
      "Validation loss decreased (0.083802 --> 0.083218).  Saving model ...\n",
      "[172/300] train_loss: 0.07177 valid_loss: 0.08432 test_loss: 0.09740 \n",
      "[173/300] train_loss: 0.07140 valid_loss: 0.08529 test_loss: 0.09809 \n",
      "[174/300] train_loss: 0.07303 valid_loss: 0.08323 test_loss: 0.09918 \n",
      "[175/300] train_loss: 0.07347 valid_loss: 0.08352 test_loss: 0.09548 \n",
      "[176/300] train_loss: 0.07410 valid_loss: 0.08270 test_loss: 0.09590 \n",
      "Validation loss decreased (0.083218 --> 0.082699).  Saving model ...\n",
      "[177/300] train_loss: 0.07068 valid_loss: 0.08434 test_loss: 0.09760 \n",
      "[178/300] train_loss: 0.07488 valid_loss: 0.08296 test_loss: 0.09552 \n",
      "[179/300] train_loss: 0.07268 valid_loss: 0.08676 test_loss: 0.09747 \n",
      "[180/300] train_loss: 0.07511 valid_loss: 0.08298 test_loss: 0.09588 \n",
      "[181/300] train_loss: 0.07178 valid_loss: 0.08307 test_loss: 0.09518 \n",
      "[182/300] train_loss: 0.07141 valid_loss: 0.08379 test_loss: 0.09565 \n",
      "[183/300] train_loss: 0.07161 valid_loss: 0.08260 test_loss: 0.09484 \n",
      "Validation loss decreased (0.082699 --> 0.082603).  Saving model ...\n",
      "[184/300] train_loss: 0.07119 valid_loss: 0.08408 test_loss: 0.09589 \n",
      "[185/300] train_loss: 0.07136 valid_loss: 0.08275 test_loss: 0.09666 \n",
      "[186/300] train_loss: 0.07094 valid_loss: 0.08158 test_loss: 0.09481 \n",
      "Validation loss decreased (0.082603 --> 0.081583).  Saving model ...\n",
      "[187/300] train_loss: 0.07020 valid_loss: 0.08250 test_loss: 0.09616 \n",
      "[188/300] train_loss: 0.07287 valid_loss: 0.08371 test_loss: 0.09508 \n",
      "[189/300] train_loss: 0.07218 valid_loss: 0.08304 test_loss: 0.09530 \n",
      "[190/300] train_loss: 0.07313 valid_loss: 0.08159 test_loss: 0.09481 \n",
      "[191/300] train_loss: 0.07066 valid_loss: 0.08369 test_loss: 0.09588 \n",
      "[192/300] train_loss: 0.07278 valid_loss: 0.08297 test_loss: 0.09508 \n",
      "[193/300] train_loss: 0.07020 valid_loss: 0.08381 test_loss: 0.09530 \n",
      "[194/300] train_loss: 0.06960 valid_loss: 0.08253 test_loss: 0.09471 \n",
      "[195/300] train_loss: 0.06940 valid_loss: 0.08174 test_loss: 0.09498 \n",
      "[196/300] train_loss: 0.07071 valid_loss: 0.08269 test_loss: 0.09432 \n",
      "[197/300] train_loss: 0.07101 valid_loss: 0.08173 test_loss: 0.09501 \n",
      "[198/300] train_loss: 0.07047 valid_loss: 0.08073 test_loss: 0.09404 \n",
      "Validation loss decreased (0.081583 --> 0.080730).  Saving model ...\n",
      "[199/300] train_loss: 0.07051 valid_loss: 0.08179 test_loss: 0.09505 \n",
      "[200/300] train_loss: 0.06927 valid_loss: 0.08150 test_loss: 0.09485 \n",
      "[201/300] train_loss: 0.07084 valid_loss: 0.08009 test_loss: 0.09313 \n",
      "Validation loss decreased (0.080730 --> 0.080094).  Saving model ...\n",
      "[202/300] train_loss: 0.06960 valid_loss: 0.08290 test_loss: 0.09563 \n",
      "[203/300] train_loss: 0.06843 valid_loss: 0.08432 test_loss: 0.09806 \n",
      "[204/300] train_loss: 0.06723 valid_loss: 0.08255 test_loss: 0.09412 \n",
      "[205/300] train_loss: 0.07027 valid_loss: 0.08214 test_loss: 0.09494 \n",
      "[206/300] train_loss: 0.06869 valid_loss: 0.08065 test_loss: 0.09415 \n",
      "[207/300] train_loss: 0.06705 valid_loss: 0.08180 test_loss: 0.09301 \n",
      "[208/300] train_loss: 0.06918 valid_loss: 0.08095 test_loss: 0.09421 \n",
      "[209/300] train_loss: 0.06805 valid_loss: 0.08239 test_loss: 0.09506 \n",
      "[210/300] train_loss: 0.06867 valid_loss: 0.08315 test_loss: 0.09570 \n",
      "[211/300] train_loss: 0.06990 valid_loss: 0.08112 test_loss: 0.09251 \n",
      "[212/300] train_loss: 0.06856 valid_loss: 0.08169 test_loss: 0.09287 \n",
      "[213/300] train_loss: 0.06984 valid_loss: 0.08125 test_loss: 0.09247 \n",
      "[214/300] train_loss: 0.06797 valid_loss: 0.08030 test_loss: 0.09315 \n",
      "[215/300] train_loss: 0.06865 valid_loss: 0.08262 test_loss: 0.09556 \n",
      "[216/300] train_loss: 0.07103 valid_loss: 0.07940 test_loss: 0.09107 \n",
      "Validation loss decreased (0.080094 --> 0.079400).  Saving model ...\n",
      "[217/300] train_loss: 0.06864 valid_loss: 0.08107 test_loss: 0.09257 \n",
      "[218/300] train_loss: 0.06930 valid_loss: 0.08020 test_loss: 0.09264 \n",
      "[219/300] train_loss: 0.06588 valid_loss: 0.07998 test_loss: 0.09300 \n",
      "[220/300] train_loss: 0.06791 valid_loss: 0.08018 test_loss: 0.09317 \n",
      "[221/300] train_loss: 0.06811 valid_loss: 0.07995 test_loss: 0.09240 \n",
      "[222/300] train_loss: 0.06672 valid_loss: 0.08094 test_loss: 0.09301 \n",
      "[223/300] train_loss: 0.06760 valid_loss: 0.07997 test_loss: 0.09205 \n",
      "[224/300] train_loss: 0.06731 valid_loss: 0.08192 test_loss: 0.09326 \n",
      "[225/300] train_loss: 0.06890 valid_loss: 0.07913 test_loss: 0.09177 \n",
      "Validation loss decreased (0.079400 --> 0.079126).  Saving model ...\n",
      "[226/300] train_loss: 0.06675 valid_loss: 0.08197 test_loss: 0.09443 \n",
      "[227/300] train_loss: 0.06841 valid_loss: 0.08172 test_loss: 0.09170 \n",
      "[228/300] train_loss: 0.06668 valid_loss: 0.08245 test_loss: 0.09189 \n",
      "[229/300] train_loss: 0.06809 valid_loss: 0.08038 test_loss: 0.09170 \n",
      "[230/300] train_loss: 0.06709 valid_loss: 0.07968 test_loss: 0.09098 \n",
      "[231/300] train_loss: 0.06640 valid_loss: 0.07938 test_loss: 0.09176 \n",
      "[232/300] train_loss: 0.06711 valid_loss: 0.07995 test_loss: 0.09277 \n",
      "[233/300] train_loss: 0.06870 valid_loss: 0.07956 test_loss: 0.09061 \n",
      "[234/300] train_loss: 0.06691 valid_loss: 0.07935 test_loss: 0.09186 \n",
      "[235/300] train_loss: 0.06546 valid_loss: 0.08142 test_loss: 0.09166 \n",
      "[236/300] train_loss: 0.06588 valid_loss: 0.07928 test_loss: 0.09164 \n",
      "[237/300] train_loss: 0.06757 valid_loss: 0.07870 test_loss: 0.09206 \n",
      "Validation loss decreased (0.079126 --> 0.078703).  Saving model ...\n",
      "[238/300] train_loss: 0.06838 valid_loss: 0.07772 test_loss: 0.09110 \n",
      "Validation loss decreased (0.078703 --> 0.077719).  Saving model ...\n",
      "[239/300] train_loss: 0.06625 valid_loss: 0.07959 test_loss: 0.09179 \n",
      "[240/300] train_loss: 0.06631 valid_loss: 0.07982 test_loss: 0.09137 \n",
      "[241/300] train_loss: 0.06794 valid_loss: 0.07946 test_loss: 0.09084 \n",
      "[242/300] train_loss: 0.06667 valid_loss: 0.07929 test_loss: 0.09082 \n",
      "[243/300] train_loss: 0.06643 valid_loss: 0.07894 test_loss: 0.09025 \n",
      "[244/300] train_loss: 0.06739 valid_loss: 0.07969 test_loss: 0.09203 \n",
      "[245/300] train_loss: 0.06583 valid_loss: 0.08018 test_loss: 0.09161 \n",
      "[246/300] train_loss: 0.06588 valid_loss: 0.08128 test_loss: 0.09162 \n",
      "[247/300] train_loss: 0.06472 valid_loss: 0.07822 test_loss: 0.09240 \n",
      "[248/300] train_loss: 0.06316 valid_loss: 0.07834 test_loss: 0.09045 \n",
      "[249/300] train_loss: 0.06574 valid_loss: 0.07770 test_loss: 0.08994 \n",
      "Validation loss decreased (0.077719 --> 0.077700).  Saving model ...\n",
      "[250/300] train_loss: 0.06572 valid_loss: 0.07895 test_loss: 0.09081 \n",
      "[251/300] train_loss: 0.06619 valid_loss: 0.07832 test_loss: 0.09183 \n",
      "[252/300] train_loss: 0.06533 valid_loss: 0.07999 test_loss: 0.09169 \n",
      "[253/300] train_loss: 0.06526 valid_loss: 0.07821 test_loss: 0.09175 \n",
      "[254/300] train_loss: 0.06501 valid_loss: 0.07892 test_loss: 0.09201 \n",
      "[255/300] train_loss: 0.06468 valid_loss: 0.07758 test_loss: 0.08998 \n",
      "Validation loss decreased (0.077700 --> 0.077578).  Saving model ...\n",
      "[256/300] train_loss: 0.06339 valid_loss: 0.07974 test_loss: 0.09037 \n",
      "[257/300] train_loss: 0.06524 valid_loss: 0.07908 test_loss: 0.09052 \n",
      "[258/300] train_loss: 0.06471 valid_loss: 0.07822 test_loss: 0.08975 \n",
      "[259/300] train_loss: 0.06298 valid_loss: 0.07813 test_loss: 0.09058 \n",
      "[260/300] train_loss: 0.06532 valid_loss: 0.07777 test_loss: 0.08976 \n",
      "[261/300] train_loss: 0.06414 valid_loss: 0.07826 test_loss: 0.09120 \n",
      "[262/300] train_loss: 0.06458 valid_loss: 0.07932 test_loss: 0.09154 \n",
      "[263/300] train_loss: 0.06629 valid_loss: 0.07733 test_loss: 0.08952 \n",
      "Validation loss decreased (0.077578 --> 0.077328).  Saving model ...\n",
      "[264/300] train_loss: 0.06355 valid_loss: 0.07873 test_loss: 0.09073 \n",
      "[265/300] train_loss: 0.06309 valid_loss: 0.07837 test_loss: 0.09081 \n",
      "[266/300] train_loss: 0.06422 valid_loss: 0.07720 test_loss: 0.08973 \n",
      "Validation loss decreased (0.077328 --> 0.077201).  Saving model ...\n",
      "[267/300] train_loss: 0.06376 valid_loss: 0.07795 test_loss: 0.09057 \n",
      "[268/300] train_loss: 0.06584 valid_loss: 0.07715 test_loss: 0.08882 \n",
      "Validation loss decreased (0.077201 --> 0.077153).  Saving model ...\n",
      "[269/300] train_loss: 0.06505 valid_loss: 0.07959 test_loss: 0.09365 \n",
      "[270/300] train_loss: 0.06529 valid_loss: 0.07803 test_loss: 0.09023 \n",
      "[271/300] train_loss: 0.06170 valid_loss: 0.07888 test_loss: 0.09041 \n",
      "[272/300] train_loss: 0.06411 valid_loss: 0.07826 test_loss: 0.09053 \n",
      "[273/300] train_loss: 0.06409 valid_loss: 0.07895 test_loss: 0.09022 \n",
      "[274/300] train_loss: 0.06395 valid_loss: 0.07761 test_loss: 0.09016 \n",
      "[275/300] train_loss: 0.06300 valid_loss: 0.07995 test_loss: 0.09012 \n",
      "[276/300] train_loss: 0.06296 valid_loss: 0.07793 test_loss: 0.08958 \n",
      "[277/300] train_loss: 0.06277 valid_loss: 0.07876 test_loss: 0.08856 \n",
      "[278/300] train_loss: 0.06500 valid_loss: 0.08019 test_loss: 0.09077 \n",
      "[279/300] train_loss: 0.06258 valid_loss: 0.07854 test_loss: 0.09006 \n",
      "[280/300] train_loss: 0.06327 valid_loss: 0.07920 test_loss: 0.09126 \n",
      "[281/300] train_loss: 0.06445 valid_loss: 0.07990 test_loss: 0.09084 \n",
      "[282/300] train_loss: 0.06543 valid_loss: 0.07894 test_loss: 0.08870 \n",
      "[283/300] train_loss: 0.06262 valid_loss: 0.07945 test_loss: 0.08847 \n",
      "[284/300] train_loss: 0.06336 valid_loss: 0.07822 test_loss: 0.08832 \n",
      "[285/300] train_loss: 0.06375 valid_loss: 0.07794 test_loss: 0.08836 \n",
      "[286/300] train_loss: 0.06479 valid_loss: 0.07743 test_loss: 0.08796 \n",
      "[287/300] train_loss: 0.06524 valid_loss: 0.07890 test_loss: 0.08943 \n",
      "[288/300] train_loss: 0.06544 valid_loss: 0.07728 test_loss: 0.08888 \n",
      "[289/300] train_loss: 0.06193 valid_loss: 0.07898 test_loss: 0.08968 \n",
      "[290/300] train_loss: 0.06264 valid_loss: 0.07785 test_loss: 0.08900 \n",
      "[291/300] train_loss: 0.06271 valid_loss: 0.07776 test_loss: 0.08853 \n",
      "[292/300] train_loss: 0.06389 valid_loss: 0.07751 test_loss: 0.08879 \n",
      "[293/300] train_loss: 0.06483 valid_loss: 0.07982 test_loss: 0.08956 \n",
      "[294/300] train_loss: 0.06329 valid_loss: 0.07780 test_loss: 0.08901 \n",
      "[295/300] train_loss: 0.06264 valid_loss: 0.07881 test_loss: 0.08849 \n",
      "[296/300] train_loss: 0.06390 valid_loss: 0.07806 test_loss: 0.08931 \n",
      "[297/300] train_loss: 0.06341 valid_loss: 0.07872 test_loss: 0.09097 \n",
      "[298/300] train_loss: 0.06084 valid_loss: 0.07765 test_loss: 0.08792 \n",
      "[299/300] train_loss: 0.06161 valid_loss: 0.07949 test_loss: 0.08786 \n",
      "[300/300] train_loss: 0.06090 valid_loss: 0.07826 test_loss: 0.08704 \n",
      "TRAINING MODEL 7\n",
      "[  1/300] train_loss: 0.61997 valid_loss: 0.51426 test_loss: 0.52185 \n",
      "Validation loss decreased (inf --> 0.514263).  Saving model ...\n",
      "[  2/300] train_loss: 0.41316 valid_loss: 0.37691 test_loss: 0.38237 \n",
      "Validation loss decreased (0.514263 --> 0.376913).  Saving model ...\n",
      "[  3/300] train_loss: 0.31823 valid_loss: 0.30942 test_loss: 0.31745 \n",
      "Validation loss decreased (0.376913 --> 0.309416).  Saving model ...\n",
      "[  4/300] train_loss: 0.26283 valid_loss: 0.26510 test_loss: 0.27573 \n",
      "Validation loss decreased (0.309416 --> 0.265097).  Saving model ...\n",
      "[  5/300] train_loss: 0.23384 valid_loss: 0.23315 test_loss: 0.24765 \n",
      "Validation loss decreased (0.265097 --> 0.233149).  Saving model ...\n",
      "[  6/300] train_loss: 0.21067 valid_loss: 0.20961 test_loss: 0.22446 \n",
      "Validation loss decreased (0.233149 --> 0.209614).  Saving model ...\n",
      "[  7/300] train_loss: 0.19340 valid_loss: 0.19154 test_loss: 0.20527 \n",
      "Validation loss decreased (0.209614 --> 0.191543).  Saving model ...\n",
      "[  8/300] train_loss: 0.18319 valid_loss: 0.17955 test_loss: 0.19382 \n",
      "Validation loss decreased (0.191543 --> 0.179550).  Saving model ...\n",
      "[  9/300] train_loss: 0.17386 valid_loss: 0.16932 test_loss: 0.18177 \n",
      "Validation loss decreased (0.179550 --> 0.169320).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16371 valid_loss: 0.16225 test_loss: 0.17437 \n",
      "Validation loss decreased (0.169320 --> 0.162245).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15903 valid_loss: 0.15763 test_loss: 0.16847 \n",
      "Validation loss decreased (0.162245 --> 0.157633).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15207 valid_loss: 0.15584 test_loss: 0.16657 \n",
      "Validation loss decreased (0.157633 --> 0.155839).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14650 valid_loss: 0.15238 test_loss: 0.16462 \n",
      "Validation loss decreased (0.155839 --> 0.152376).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14346 valid_loss: 0.14793 test_loss: 0.15957 \n",
      "Validation loss decreased (0.152376 --> 0.147933).  Saving model ...\n",
      "[ 15/300] train_loss: 0.13984 valid_loss: 0.14873 test_loss: 0.16003 \n",
      "[ 16/300] train_loss: 0.13740 valid_loss: 0.14670 test_loss: 0.15649 \n",
      "Validation loss decreased (0.147933 --> 0.146698).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13911 valid_loss: 0.14281 test_loss: 0.15470 \n",
      "Validation loss decreased (0.146698 --> 0.142809).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13517 valid_loss: 0.14495 test_loss: 0.15749 \n",
      "[ 19/300] train_loss: 0.13192 valid_loss: 0.14162 test_loss: 0.15381 \n",
      "Validation loss decreased (0.142809 --> 0.141619).  Saving model ...\n",
      "[ 20/300] train_loss: 0.12793 valid_loss: 0.13911 test_loss: 0.14912 \n",
      "Validation loss decreased (0.141619 --> 0.139110).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12625 valid_loss: 0.13876 test_loss: 0.14836 \n",
      "Validation loss decreased (0.139110 --> 0.138763).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12772 valid_loss: 0.13879 test_loss: 0.14787 \n",
      "[ 23/300] train_loss: 0.12401 valid_loss: 0.13666 test_loss: 0.14814 \n",
      "Validation loss decreased (0.138763 --> 0.136663).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12250 valid_loss: 0.13491 test_loss: 0.14522 \n",
      "Validation loss decreased (0.136663 --> 0.134912).  Saving model ...\n",
      "[ 25/300] train_loss: 0.11834 valid_loss: 0.13464 test_loss: 0.14446 \n",
      "Validation loss decreased (0.134912 --> 0.134642).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12322 valid_loss: 0.13322 test_loss: 0.14277 \n",
      "Validation loss decreased (0.134642 --> 0.133216).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12554 valid_loss: 0.13279 test_loss: 0.14197 \n",
      "Validation loss decreased (0.133216 --> 0.132792).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11839 valid_loss: 0.13185 test_loss: 0.14247 \n",
      "Validation loss decreased (0.132792 --> 0.131845).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11530 valid_loss: 0.12656 test_loss: 0.13795 \n",
      "Validation loss decreased (0.131845 --> 0.126556).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11434 valid_loss: 0.12694 test_loss: 0.13862 \n",
      "[ 31/300] train_loss: 0.11272 valid_loss: 0.12761 test_loss: 0.13912 \n",
      "[ 32/300] train_loss: 0.11761 valid_loss: 0.12601 test_loss: 0.13676 \n",
      "Validation loss decreased (0.126556 --> 0.126011).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11225 valid_loss: 0.12317 test_loss: 0.13467 \n",
      "Validation loss decreased (0.126011 --> 0.123170).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11569 valid_loss: 0.12566 test_loss: 0.13525 \n",
      "[ 35/300] train_loss: 0.11196 valid_loss: 0.12076 test_loss: 0.13195 \n",
      "Validation loss decreased (0.123170 --> 0.120759).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11082 valid_loss: 0.11878 test_loss: 0.13110 \n",
      "Validation loss decreased (0.120759 --> 0.118785).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11093 valid_loss: 0.11852 test_loss: 0.12949 \n",
      "Validation loss decreased (0.118785 --> 0.118517).  Saving model ...\n",
      "[ 38/300] train_loss: 0.10925 valid_loss: 0.12019 test_loss: 0.13036 \n",
      "[ 39/300] train_loss: 0.10680 valid_loss: 0.11831 test_loss: 0.12872 \n",
      "Validation loss decreased (0.118517 --> 0.118305).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11094 valid_loss: 0.11894 test_loss: 0.12945 \n",
      "[ 41/300] train_loss: 0.10596 valid_loss: 0.11719 test_loss: 0.12955 \n",
      "Validation loss decreased (0.118305 --> 0.117195).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10423 valid_loss: 0.11490 test_loss: 0.12723 \n",
      "Validation loss decreased (0.117195 --> 0.114900).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10729 valid_loss: 0.11623 test_loss: 0.12669 \n",
      "[ 44/300] train_loss: 0.10426 valid_loss: 0.11325 test_loss: 0.12636 \n",
      "Validation loss decreased (0.114900 --> 0.113249).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10278 valid_loss: 0.11259 test_loss: 0.12467 \n",
      "Validation loss decreased (0.113249 --> 0.112592).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10456 valid_loss: 0.11233 test_loss: 0.12414 \n",
      "Validation loss decreased (0.112592 --> 0.112335).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10180 valid_loss: 0.11213 test_loss: 0.12439 \n",
      "Validation loss decreased (0.112335 --> 0.112131).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10218 valid_loss: 0.11078 test_loss: 0.12187 \n",
      "Validation loss decreased (0.112131 --> 0.110776).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10361 valid_loss: 0.10948 test_loss: 0.12111 \n",
      "Validation loss decreased (0.110776 --> 0.109479).  Saving model ...\n",
      "[ 50/300] train_loss: 0.09987 valid_loss: 0.11151 test_loss: 0.12284 \n",
      "[ 51/300] train_loss: 0.10001 valid_loss: 0.11021 test_loss: 0.12104 \n",
      "[ 52/300] train_loss: 0.09843 valid_loss: 0.11040 test_loss: 0.12316 \n",
      "[ 53/300] train_loss: 0.10026 valid_loss: 0.10882 test_loss: 0.12064 \n",
      "Validation loss decreased (0.109479 --> 0.108818).  Saving model ...\n",
      "[ 54/300] train_loss: 0.09918 valid_loss: 0.11061 test_loss: 0.12199 \n",
      "[ 55/300] train_loss: 0.09824 valid_loss: 0.11028 test_loss: 0.12123 \n",
      "[ 56/300] train_loss: 0.09945 valid_loss: 0.10868 test_loss: 0.11936 \n",
      "Validation loss decreased (0.108818 --> 0.108679).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09825 valid_loss: 0.10621 test_loss: 0.11777 \n",
      "Validation loss decreased (0.108679 --> 0.106212).  Saving model ...\n",
      "[ 58/300] train_loss: 0.09677 valid_loss: 0.10837 test_loss: 0.11958 \n",
      "[ 59/300] train_loss: 0.09438 valid_loss: 0.10534 test_loss: 0.11734 \n",
      "Validation loss decreased (0.106212 --> 0.105342).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09505 valid_loss: 0.10647 test_loss: 0.11735 \n",
      "[ 61/300] train_loss: 0.09580 valid_loss: 0.10918 test_loss: 0.11883 \n",
      "[ 62/300] train_loss: 0.09525 valid_loss: 0.10491 test_loss: 0.11580 \n",
      "Validation loss decreased (0.105342 --> 0.104908).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09270 valid_loss: 0.10537 test_loss: 0.11703 \n",
      "[ 64/300] train_loss: 0.09199 valid_loss: 0.10283 test_loss: 0.11463 \n",
      "Validation loss decreased (0.104908 --> 0.102830).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09399 valid_loss: 0.10087 test_loss: 0.11434 \n",
      "Validation loss decreased (0.102830 --> 0.100869).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09446 valid_loss: 0.10129 test_loss: 0.11322 \n",
      "[ 67/300] train_loss: 0.09643 valid_loss: 0.10294 test_loss: 0.11505 \n",
      "[ 68/300] train_loss: 0.09239 valid_loss: 0.10157 test_loss: 0.11414 \n",
      "[ 69/300] train_loss: 0.09139 valid_loss: 0.10223 test_loss: 0.11382 \n",
      "[ 70/300] train_loss: 0.09130 valid_loss: 0.10066 test_loss: 0.11300 \n",
      "Validation loss decreased (0.100869 --> 0.100657).  Saving model ...\n",
      "[ 71/300] train_loss: 0.08956 valid_loss: 0.09984 test_loss: 0.11116 \n",
      "Validation loss decreased (0.100657 --> 0.099844).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09197 valid_loss: 0.09966 test_loss: 0.11174 \n",
      "Validation loss decreased (0.099844 --> 0.099660).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09081 valid_loss: 0.09894 test_loss: 0.11073 \n",
      "Validation loss decreased (0.099660 --> 0.098943).  Saving model ...\n",
      "[ 74/300] train_loss: 0.08775 valid_loss: 0.09827 test_loss: 0.11027 \n",
      "Validation loss decreased (0.098943 --> 0.098265).  Saving model ...\n",
      "[ 75/300] train_loss: 0.08916 valid_loss: 0.09850 test_loss: 0.10996 \n",
      "[ 76/300] train_loss: 0.08647 valid_loss: 0.09692 test_loss: 0.10932 \n",
      "Validation loss decreased (0.098265 --> 0.096916).  Saving model ...\n",
      "[ 77/300] train_loss: 0.08984 valid_loss: 0.09913 test_loss: 0.10966 \n",
      "[ 78/300] train_loss: 0.08832 valid_loss: 0.09643 test_loss: 0.10933 \n",
      "Validation loss decreased (0.096916 --> 0.096435).  Saving model ...\n",
      "[ 79/300] train_loss: 0.09086 valid_loss: 0.09682 test_loss: 0.10852 \n",
      "[ 80/300] train_loss: 0.08979 valid_loss: 0.09820 test_loss: 0.10877 \n",
      "[ 81/300] train_loss: 0.08752 valid_loss: 0.09691 test_loss: 0.10905 \n",
      "[ 82/300] train_loss: 0.08832 valid_loss: 0.09691 test_loss: 0.10819 \n",
      "[ 83/300] train_loss: 0.08836 valid_loss: 0.09631 test_loss: 0.10740 \n",
      "Validation loss decreased (0.096435 --> 0.096311).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08956 valid_loss: 0.09699 test_loss: 0.10948 \n",
      "[ 85/300] train_loss: 0.08827 valid_loss: 0.09504 test_loss: 0.10806 \n",
      "Validation loss decreased (0.096311 --> 0.095040).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08345 valid_loss: 0.09488 test_loss: 0.10626 \n",
      "Validation loss decreased (0.095040 --> 0.094881).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08461 valid_loss: 0.09365 test_loss: 0.10571 \n",
      "Validation loss decreased (0.094881 --> 0.093646).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08707 valid_loss: 0.09501 test_loss: 0.10744 \n",
      "[ 89/300] train_loss: 0.08397 valid_loss: 0.09475 test_loss: 0.10621 \n",
      "[ 90/300] train_loss: 0.08646 valid_loss: 0.09340 test_loss: 0.10470 \n",
      "Validation loss decreased (0.093646 --> 0.093398).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08338 valid_loss: 0.09698 test_loss: 0.10810 \n",
      "[ 92/300] train_loss: 0.08428 valid_loss: 0.09574 test_loss: 0.10714 \n",
      "[ 93/300] train_loss: 0.08351 valid_loss: 0.09277 test_loss: 0.10426 \n",
      "Validation loss decreased (0.093398 --> 0.092766).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08260 valid_loss: 0.09248 test_loss: 0.10632 \n",
      "Validation loss decreased (0.092766 --> 0.092481).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08384 valid_loss: 0.09294 test_loss: 0.10472 \n",
      "[ 96/300] train_loss: 0.08205 valid_loss: 0.09297 test_loss: 0.10454 \n",
      "[ 97/300] train_loss: 0.08389 valid_loss: 0.09383 test_loss: 0.10551 \n",
      "[ 98/300] train_loss: 0.08374 valid_loss: 0.09301 test_loss: 0.10521 \n",
      "[ 99/300] train_loss: 0.08275 valid_loss: 0.09150 test_loss: 0.10251 \n",
      "Validation loss decreased (0.092481 --> 0.091502).  Saving model ...\n",
      "[100/300] train_loss: 0.08496 valid_loss: 0.09140 test_loss: 0.10247 \n",
      "Validation loss decreased (0.091502 --> 0.091401).  Saving model ...\n",
      "[101/300] train_loss: 0.08220 valid_loss: 0.09136 test_loss: 0.10260 \n",
      "Validation loss decreased (0.091401 --> 0.091364).  Saving model ...\n",
      "[102/300] train_loss: 0.08419 valid_loss: 0.09165 test_loss: 0.10440 \n",
      "[103/300] train_loss: 0.08182 valid_loss: 0.09360 test_loss: 0.10541 \n",
      "[104/300] train_loss: 0.08383 valid_loss: 0.09089 test_loss: 0.10228 \n",
      "Validation loss decreased (0.091364 --> 0.090894).  Saving model ...\n",
      "[105/300] train_loss: 0.08391 valid_loss: 0.09181 test_loss: 0.10277 \n",
      "[106/300] train_loss: 0.08115 valid_loss: 0.08934 test_loss: 0.10183 \n",
      "Validation loss decreased (0.090894 --> 0.089343).  Saving model ...\n",
      "[107/300] train_loss: 0.07976 valid_loss: 0.08977 test_loss: 0.10178 \n",
      "[108/300] train_loss: 0.07972 valid_loss: 0.09199 test_loss: 0.10348 \n",
      "[109/300] train_loss: 0.07995 valid_loss: 0.09013 test_loss: 0.10251 \n",
      "[110/300] train_loss: 0.07951 valid_loss: 0.08875 test_loss: 0.10107 \n",
      "Validation loss decreased (0.089343 --> 0.088754).  Saving model ...\n",
      "[111/300] train_loss: 0.08124 valid_loss: 0.09033 test_loss: 0.10239 \n",
      "[112/300] train_loss: 0.08274 valid_loss: 0.08885 test_loss: 0.10092 \n",
      "[113/300] train_loss: 0.08160 valid_loss: 0.09130 test_loss: 0.10185 \n",
      "[114/300] train_loss: 0.07912 valid_loss: 0.08940 test_loss: 0.10034 \n",
      "[115/300] train_loss: 0.07970 valid_loss: 0.08926 test_loss: 0.10037 \n",
      "[116/300] train_loss: 0.07979 valid_loss: 0.08836 test_loss: 0.10057 \n",
      "Validation loss decreased (0.088754 --> 0.088364).  Saving model ...\n",
      "[117/300] train_loss: 0.07903 valid_loss: 0.08982 test_loss: 0.10072 \n",
      "[118/300] train_loss: 0.08092 valid_loss: 0.08890 test_loss: 0.10093 \n",
      "[119/300] train_loss: 0.08091 valid_loss: 0.08776 test_loss: 0.10078 \n",
      "Validation loss decreased (0.088364 --> 0.087765).  Saving model ...\n",
      "[120/300] train_loss: 0.08062 valid_loss: 0.08869 test_loss: 0.10072 \n",
      "[121/300] train_loss: 0.07834 valid_loss: 0.08844 test_loss: 0.10184 \n",
      "[122/300] train_loss: 0.07753 valid_loss: 0.08800 test_loss: 0.09980 \n",
      "[123/300] train_loss: 0.07769 valid_loss: 0.08777 test_loss: 0.10029 \n",
      "[124/300] train_loss: 0.07738 valid_loss: 0.08749 test_loss: 0.09934 \n",
      "Validation loss decreased (0.087765 --> 0.087490).  Saving model ...\n",
      "[125/300] train_loss: 0.08019 valid_loss: 0.08843 test_loss: 0.10011 \n",
      "[126/300] train_loss: 0.07665 valid_loss: 0.08997 test_loss: 0.10142 \n",
      "[127/300] train_loss: 0.07698 valid_loss: 0.08904 test_loss: 0.10092 \n",
      "[128/300] train_loss: 0.07597 valid_loss: 0.08832 test_loss: 0.10041 \n",
      "[129/300] train_loss: 0.07505 valid_loss: 0.08710 test_loss: 0.09875 \n",
      "Validation loss decreased (0.087490 --> 0.087100).  Saving model ...\n",
      "[130/300] train_loss: 0.07719 valid_loss: 0.08775 test_loss: 0.09920 \n",
      "[131/300] train_loss: 0.07644 valid_loss: 0.08790 test_loss: 0.09806 \n",
      "[132/300] train_loss: 0.07752 valid_loss: 0.08780 test_loss: 0.09911 \n",
      "[133/300] train_loss: 0.07633 valid_loss: 0.08807 test_loss: 0.09841 \n",
      "[134/300] train_loss: 0.07470 valid_loss: 0.08808 test_loss: 0.09919 \n",
      "[135/300] train_loss: 0.07824 valid_loss: 0.08831 test_loss: 0.09816 \n",
      "[136/300] train_loss: 0.07473 valid_loss: 0.08737 test_loss: 0.09782 \n",
      "[137/300] train_loss: 0.07672 valid_loss: 0.08719 test_loss: 0.09748 \n",
      "[138/300] train_loss: 0.07503 valid_loss: 0.08651 test_loss: 0.09681 \n",
      "Validation loss decreased (0.087100 --> 0.086507).  Saving model ...\n",
      "[139/300] train_loss: 0.07419 valid_loss: 0.08696 test_loss: 0.09905 \n",
      "[140/300] train_loss: 0.07753 valid_loss: 0.08644 test_loss: 0.09856 \n",
      "Validation loss decreased (0.086507 --> 0.086442).  Saving model ...\n",
      "[141/300] train_loss: 0.07641 valid_loss: 0.08565 test_loss: 0.09683 \n",
      "Validation loss decreased (0.086442 --> 0.085650).  Saving model ...\n",
      "[142/300] train_loss: 0.07527 valid_loss: 0.08530 test_loss: 0.09617 \n",
      "Validation loss decreased (0.085650 --> 0.085299).  Saving model ...\n",
      "[143/300] train_loss: 0.07578 valid_loss: 0.08681 test_loss: 0.09758 \n",
      "[144/300] train_loss: 0.07342 valid_loss: 0.08571 test_loss: 0.09726 \n",
      "[145/300] train_loss: 0.07166 valid_loss: 0.08582 test_loss: 0.09675 \n",
      "[146/300] train_loss: 0.07524 valid_loss: 0.08525 test_loss: 0.09604 \n",
      "Validation loss decreased (0.085299 --> 0.085252).  Saving model ...\n",
      "[147/300] train_loss: 0.07465 valid_loss: 0.08513 test_loss: 0.09550 \n",
      "Validation loss decreased (0.085252 --> 0.085128).  Saving model ...\n",
      "[148/300] train_loss: 0.07348 valid_loss: 0.08513 test_loss: 0.09573 \n",
      "[149/300] train_loss: 0.07381 valid_loss: 0.08558 test_loss: 0.09673 \n",
      "[150/300] train_loss: 0.07408 valid_loss: 0.08538 test_loss: 0.09699 \n",
      "[151/300] train_loss: 0.07609 valid_loss: 0.08410 test_loss: 0.09517 \n",
      "Validation loss decreased (0.085128 --> 0.084104).  Saving model ...\n",
      "[152/300] train_loss: 0.07301 valid_loss: 0.08625 test_loss: 0.09654 \n",
      "[153/300] train_loss: 0.07300 valid_loss: 0.08498 test_loss: 0.09587 \n",
      "[154/300] train_loss: 0.07375 valid_loss: 0.08561 test_loss: 0.09647 \n",
      "[155/300] train_loss: 0.07409 valid_loss: 0.08353 test_loss: 0.09386 \n",
      "Validation loss decreased (0.084104 --> 0.083533).  Saving model ...\n",
      "[156/300] train_loss: 0.07383 valid_loss: 0.08322 test_loss: 0.09457 \n",
      "Validation loss decreased (0.083533 --> 0.083218).  Saving model ...\n",
      "[157/300] train_loss: 0.07407 valid_loss: 0.08401 test_loss: 0.09476 \n",
      "[158/300] train_loss: 0.07199 valid_loss: 0.08293 test_loss: 0.09488 \n",
      "Validation loss decreased (0.083218 --> 0.082935).  Saving model ...\n",
      "[159/300] train_loss: 0.07299 valid_loss: 0.08330 test_loss: 0.09507 \n",
      "[160/300] train_loss: 0.07307 valid_loss: 0.08206 test_loss: 0.09320 \n",
      "Validation loss decreased (0.082935 --> 0.082057).  Saving model ...\n",
      "[161/300] train_loss: 0.07214 valid_loss: 0.08264 test_loss: 0.09334 \n",
      "[162/300] train_loss: 0.07406 valid_loss: 0.08351 test_loss: 0.09513 \n",
      "[163/300] train_loss: 0.07314 valid_loss: 0.08260 test_loss: 0.09502 \n",
      "[164/300] train_loss: 0.07263 valid_loss: 0.08165 test_loss: 0.09409 \n",
      "Validation loss decreased (0.082057 --> 0.081646).  Saving model ...\n",
      "[165/300] train_loss: 0.07296 valid_loss: 0.08310 test_loss: 0.09424 \n",
      "[166/300] train_loss: 0.07299 valid_loss: 0.08297 test_loss: 0.09466 \n",
      "[167/300] train_loss: 0.07260 valid_loss: 0.08340 test_loss: 0.09358 \n",
      "[168/300] train_loss: 0.07140 valid_loss: 0.08311 test_loss: 0.09419 \n",
      "[169/300] train_loss: 0.06984 valid_loss: 0.08312 test_loss: 0.09385 \n",
      "[170/300] train_loss: 0.07069 valid_loss: 0.08232 test_loss: 0.09332 \n",
      "[171/300] train_loss: 0.07279 valid_loss: 0.08380 test_loss: 0.09477 \n",
      "[172/300] train_loss: 0.07249 valid_loss: 0.08210 test_loss: 0.09356 \n",
      "[173/300] train_loss: 0.07051 valid_loss: 0.08287 test_loss: 0.09370 \n",
      "[174/300] train_loss: 0.06851 valid_loss: 0.08221 test_loss: 0.09303 \n",
      "[175/300] train_loss: 0.07303 valid_loss: 0.08310 test_loss: 0.09412 \n",
      "[176/300] train_loss: 0.07274 valid_loss: 0.08135 test_loss: 0.09170 \n",
      "Validation loss decreased (0.081646 --> 0.081352).  Saving model ...\n",
      "[177/300] train_loss: 0.06970 valid_loss: 0.08138 test_loss: 0.09234 \n",
      "[178/300] train_loss: 0.07127 valid_loss: 0.08237 test_loss: 0.09439 \n",
      "[179/300] train_loss: 0.06961 valid_loss: 0.08099 test_loss: 0.09296 \n",
      "Validation loss decreased (0.081352 --> 0.080986).  Saving model ...\n",
      "[180/300] train_loss: 0.06948 valid_loss: 0.08277 test_loss: 0.09274 \n",
      "[181/300] train_loss: 0.07011 valid_loss: 0.08246 test_loss: 0.09336 \n",
      "[182/300] train_loss: 0.06819 valid_loss: 0.08182 test_loss: 0.09419 \n",
      "[183/300] train_loss: 0.06863 valid_loss: 0.08473 test_loss: 0.09418 \n",
      "[184/300] train_loss: 0.07131 valid_loss: 0.08142 test_loss: 0.09259 \n",
      "[185/300] train_loss: 0.06858 valid_loss: 0.08221 test_loss: 0.09344 \n",
      "[186/300] train_loss: 0.06990 valid_loss: 0.08145 test_loss: 0.09205 \n",
      "[187/300] train_loss: 0.07004 valid_loss: 0.08155 test_loss: 0.09424 \n",
      "[188/300] train_loss: 0.06867 valid_loss: 0.08124 test_loss: 0.09244 \n",
      "[189/300] train_loss: 0.06977 valid_loss: 0.08033 test_loss: 0.09152 \n",
      "Validation loss decreased (0.080986 --> 0.080326).  Saving model ...\n",
      "[190/300] train_loss: 0.06961 valid_loss: 0.08088 test_loss: 0.09209 \n",
      "[191/300] train_loss: 0.06972 valid_loss: 0.08142 test_loss: 0.09316 \n",
      "[192/300] train_loss: 0.06758 valid_loss: 0.08055 test_loss: 0.09105 \n",
      "[193/300] train_loss: 0.06991 valid_loss: 0.08020 test_loss: 0.09162 \n",
      "Validation loss decreased (0.080326 --> 0.080205).  Saving model ...\n",
      "[194/300] train_loss: 0.06851 valid_loss: 0.08108 test_loss: 0.09197 \n",
      "[195/300] train_loss: 0.07056 valid_loss: 0.08109 test_loss: 0.09113 \n",
      "[196/300] train_loss: 0.06761 valid_loss: 0.08175 test_loss: 0.09137 \n",
      "[197/300] train_loss: 0.07149 valid_loss: 0.08066 test_loss: 0.09033 \n",
      "[198/300] train_loss: 0.06877 valid_loss: 0.08060 test_loss: 0.09120 \n",
      "[199/300] train_loss: 0.06698 valid_loss: 0.08116 test_loss: 0.09158 \n",
      "[200/300] train_loss: 0.06787 valid_loss: 0.08179 test_loss: 0.09197 \n",
      "[201/300] train_loss: 0.06734 valid_loss: 0.08214 test_loss: 0.09092 \n",
      "[202/300] train_loss: 0.06907 valid_loss: 0.08082 test_loss: 0.09104 \n",
      "[203/300] train_loss: 0.06661 valid_loss: 0.08055 test_loss: 0.09112 \n",
      "[204/300] train_loss: 0.06834 valid_loss: 0.08023 test_loss: 0.09056 \n",
      "[205/300] train_loss: 0.06647 valid_loss: 0.08134 test_loss: 0.09202 \n",
      "[206/300] train_loss: 0.06877 valid_loss: 0.07862 test_loss: 0.09039 \n",
      "Validation loss decreased (0.080205 --> 0.078623).  Saving model ...\n",
      "[207/300] train_loss: 0.06784 valid_loss: 0.08120 test_loss: 0.09154 \n",
      "[208/300] train_loss: 0.06583 valid_loss: 0.08020 test_loss: 0.09111 \n",
      "[209/300] train_loss: 0.06482 valid_loss: 0.08017 test_loss: 0.09017 \n",
      "[210/300] train_loss: 0.06502 valid_loss: 0.07976 test_loss: 0.09058 \n",
      "[211/300] train_loss: 0.06517 valid_loss: 0.07987 test_loss: 0.09001 \n",
      "[212/300] train_loss: 0.06779 valid_loss: 0.07894 test_loss: 0.08857 \n",
      "[213/300] train_loss: 0.06911 valid_loss: 0.07997 test_loss: 0.08996 \n",
      "[214/300] train_loss: 0.06634 valid_loss: 0.07991 test_loss: 0.08939 \n",
      "[215/300] train_loss: 0.06625 valid_loss: 0.07908 test_loss: 0.08984 \n",
      "[216/300] train_loss: 0.06634 valid_loss: 0.07918 test_loss: 0.08958 \n",
      "[217/300] train_loss: 0.06717 valid_loss: 0.07846 test_loss: 0.08911 \n",
      "Validation loss decreased (0.078623 --> 0.078461).  Saving model ...\n",
      "[218/300] train_loss: 0.06465 valid_loss: 0.07862 test_loss: 0.08973 \n",
      "[219/300] train_loss: 0.06722 valid_loss: 0.07911 test_loss: 0.09062 \n",
      "[220/300] train_loss: 0.06516 valid_loss: 0.07884 test_loss: 0.09019 \n",
      "[221/300] train_loss: 0.06662 valid_loss: 0.07996 test_loss: 0.08997 \n",
      "[222/300] train_loss: 0.06558 valid_loss: 0.07913 test_loss: 0.08967 \n",
      "[223/300] train_loss: 0.06668 valid_loss: 0.08038 test_loss: 0.08976 \n",
      "[224/300] train_loss: 0.06608 valid_loss: 0.07882 test_loss: 0.08898 \n",
      "[225/300] train_loss: 0.06627 valid_loss: 0.07805 test_loss: 0.08835 \n",
      "Validation loss decreased (0.078461 --> 0.078050).  Saving model ...\n",
      "[226/300] train_loss: 0.06469 valid_loss: 0.07826 test_loss: 0.08904 \n",
      "[227/300] train_loss: 0.06613 valid_loss: 0.07852 test_loss: 0.08819 \n",
      "[228/300] train_loss: 0.06478 valid_loss: 0.07821 test_loss: 0.08880 \n",
      "[229/300] train_loss: 0.06273 valid_loss: 0.07906 test_loss: 0.08967 \n",
      "[230/300] train_loss: 0.06598 valid_loss: 0.07874 test_loss: 0.08843 \n",
      "[231/300] train_loss: 0.06646 valid_loss: 0.07980 test_loss: 0.08965 \n",
      "[232/300] train_loss: 0.06411 valid_loss: 0.07829 test_loss: 0.08843 \n",
      "[233/300] train_loss: 0.06437 valid_loss: 0.07816 test_loss: 0.08851 \n",
      "[234/300] train_loss: 0.06270 valid_loss: 0.07805 test_loss: 0.08800 \n",
      "Validation loss decreased (0.078050 --> 0.078048).  Saving model ...\n",
      "[235/300] train_loss: 0.06592 valid_loss: 0.07737 test_loss: 0.08799 \n",
      "Validation loss decreased (0.078048 --> 0.077366).  Saving model ...\n",
      "[236/300] train_loss: 0.06460 valid_loss: 0.07716 test_loss: 0.08836 \n",
      "Validation loss decreased (0.077366 --> 0.077157).  Saving model ...\n",
      "[237/300] train_loss: 0.06424 valid_loss: 0.07789 test_loss: 0.08847 \n",
      "[238/300] train_loss: 0.06350 valid_loss: 0.07728 test_loss: 0.08779 \n",
      "[239/300] train_loss: 0.06390 valid_loss: 0.07969 test_loss: 0.08922 \n",
      "[240/300] train_loss: 0.06441 valid_loss: 0.07770 test_loss: 0.08927 \n",
      "[241/300] train_loss: 0.06405 valid_loss: 0.07858 test_loss: 0.08874 \n",
      "[242/300] train_loss: 0.06594 valid_loss: 0.07841 test_loss: 0.08863 \n",
      "[243/300] train_loss: 0.06208 valid_loss: 0.07840 test_loss: 0.08860 \n",
      "[244/300] train_loss: 0.06505 valid_loss: 0.07826 test_loss: 0.08834 \n",
      "[245/300] train_loss: 0.06504 valid_loss: 0.07763 test_loss: 0.08786 \n",
      "[246/300] train_loss: 0.06434 valid_loss: 0.07748 test_loss: 0.08823 \n",
      "[247/300] train_loss: 0.06412 valid_loss: 0.07854 test_loss: 0.08933 \n",
      "[248/300] train_loss: 0.06377 valid_loss: 0.07800 test_loss: 0.08895 \n",
      "[249/300] train_loss: 0.06276 valid_loss: 0.07921 test_loss: 0.09001 \n",
      "[250/300] train_loss: 0.06377 valid_loss: 0.07752 test_loss: 0.08643 \n",
      "[251/300] train_loss: 0.06407 valid_loss: 0.07774 test_loss: 0.08866 \n",
      "[252/300] train_loss: 0.06234 valid_loss: 0.07883 test_loss: 0.08901 \n",
      "[253/300] train_loss: 0.06307 valid_loss: 0.07831 test_loss: 0.08818 \n",
      "[254/300] train_loss: 0.06158 valid_loss: 0.07791 test_loss: 0.08775 \n",
      "[255/300] train_loss: 0.06342 valid_loss: 0.07632 test_loss: 0.08770 \n",
      "Validation loss decreased (0.077157 --> 0.076316).  Saving model ...\n",
      "[256/300] train_loss: 0.06342 valid_loss: 0.07580 test_loss: 0.08636 \n",
      "Validation loss decreased (0.076316 --> 0.075804).  Saving model ...\n",
      "[257/300] train_loss: 0.06276 valid_loss: 0.07820 test_loss: 0.08950 \n",
      "[258/300] train_loss: 0.06230 valid_loss: 0.07727 test_loss: 0.08731 \n",
      "[259/300] train_loss: 0.06271 valid_loss: 0.07701 test_loss: 0.08730 \n",
      "[260/300] train_loss: 0.06362 valid_loss: 0.07662 test_loss: 0.08758 \n",
      "[261/300] train_loss: 0.06170 valid_loss: 0.07713 test_loss: 0.08757 \n",
      "[262/300] train_loss: 0.06492 valid_loss: 0.07950 test_loss: 0.08969 \n",
      "[263/300] train_loss: 0.06435 valid_loss: 0.07688 test_loss: 0.08790 \n",
      "[264/300] train_loss: 0.06119 valid_loss: 0.07570 test_loss: 0.08731 \n",
      "Validation loss decreased (0.075804 --> 0.075703).  Saving model ...\n",
      "[265/300] train_loss: 0.06179 valid_loss: 0.07526 test_loss: 0.08700 \n",
      "Validation loss decreased (0.075703 --> 0.075259).  Saving model ...\n",
      "[266/300] train_loss: 0.06186 valid_loss: 0.07607 test_loss: 0.08660 \n",
      "[267/300] train_loss: 0.06315 valid_loss: 0.07823 test_loss: 0.08807 \n",
      "[268/300] train_loss: 0.06264 valid_loss: 0.07793 test_loss: 0.08744 \n",
      "[269/300] train_loss: 0.06060 valid_loss: 0.07669 test_loss: 0.08627 \n",
      "[270/300] train_loss: 0.06220 valid_loss: 0.07524 test_loss: 0.08536 \n",
      "Validation loss decreased (0.075259 --> 0.075238).  Saving model ...\n",
      "[271/300] train_loss: 0.06222 valid_loss: 0.07696 test_loss: 0.08589 \n",
      "[272/300] train_loss: 0.06042 valid_loss: 0.07743 test_loss: 0.08672 \n",
      "[273/300] train_loss: 0.06265 valid_loss: 0.07786 test_loss: 0.08761 \n",
      "[274/300] train_loss: 0.06071 valid_loss: 0.07489 test_loss: 0.08629 \n",
      "Validation loss decreased (0.075238 --> 0.074887).  Saving model ...\n",
      "[275/300] train_loss: 0.06516 valid_loss: 0.07882 test_loss: 0.08783 \n",
      "[276/300] train_loss: 0.06141 valid_loss: 0.07660 test_loss: 0.08636 \n",
      "[277/300] train_loss: 0.06135 valid_loss: 0.07529 test_loss: 0.08646 \n",
      "[278/300] train_loss: 0.06164 valid_loss: 0.07712 test_loss: 0.08795 \n",
      "[279/300] train_loss: 0.06086 valid_loss: 0.07793 test_loss: 0.08700 \n",
      "[280/300] train_loss: 0.06065 valid_loss: 0.07847 test_loss: 0.08720 \n",
      "[281/300] train_loss: 0.06151 valid_loss: 0.07710 test_loss: 0.08659 \n",
      "[282/300] train_loss: 0.06248 valid_loss: 0.07650 test_loss: 0.08655 \n",
      "[283/300] train_loss: 0.06081 valid_loss: 0.07675 test_loss: 0.08731 \n",
      "[284/300] train_loss: 0.06078 valid_loss: 0.07605 test_loss: 0.08812 \n",
      "[285/300] train_loss: 0.06106 valid_loss: 0.07464 test_loss: 0.08578 \n",
      "Validation loss decreased (0.074887 --> 0.074641).  Saving model ...\n",
      "[286/300] train_loss: 0.06115 valid_loss: 0.07526 test_loss: 0.08622 \n",
      "[287/300] train_loss: 0.06109 valid_loss: 0.07554 test_loss: 0.08781 \n",
      "[288/300] train_loss: 0.06157 valid_loss: 0.07681 test_loss: 0.08673 \n",
      "[289/300] train_loss: 0.06149 valid_loss: 0.07628 test_loss: 0.08635 \n",
      "[290/300] train_loss: 0.06218 valid_loss: 0.07590 test_loss: 0.08633 \n",
      "[291/300] train_loss: 0.05992 valid_loss: 0.07540 test_loss: 0.08608 \n",
      "[292/300] train_loss: 0.06217 valid_loss: 0.07552 test_loss: 0.08519 \n",
      "[293/300] train_loss: 0.06236 valid_loss: 0.07540 test_loss: 0.08557 \n",
      "[294/300] train_loss: 0.06075 valid_loss: 0.07630 test_loss: 0.08641 \n",
      "[295/300] train_loss: 0.06088 valid_loss: 0.07652 test_loss: 0.08518 \n",
      "[296/300] train_loss: 0.06009 valid_loss: 0.07518 test_loss: 0.08614 \n",
      "[297/300] train_loss: 0.06029 valid_loss: 0.07951 test_loss: 0.08882 \n",
      "[298/300] train_loss: 0.05928 valid_loss: 0.07573 test_loss: 0.08600 \n",
      "[299/300] train_loss: 0.05960 valid_loss: 0.07669 test_loss: 0.08726 \n",
      "[300/300] train_loss: 0.06186 valid_loss: 0.07761 test_loss: 0.08656 \n",
      "TRAINING MODEL 8\n",
      "[  1/300] train_loss: 0.56284 valid_loss: 0.46964 test_loss: 0.47046 \n",
      "Validation loss decreased (inf --> 0.469640).  Saving model ...\n",
      "[  2/300] train_loss: 0.38064 valid_loss: 0.34254 test_loss: 0.34612 \n",
      "Validation loss decreased (0.469640 --> 0.342539).  Saving model ...\n",
      "[  3/300] train_loss: 0.29151 valid_loss: 0.27357 test_loss: 0.28304 \n",
      "Validation loss decreased (0.342539 --> 0.273572).  Saving model ...\n",
      "[  4/300] train_loss: 0.23578 valid_loss: 0.23096 test_loss: 0.24386 \n",
      "Validation loss decreased (0.273572 --> 0.230956).  Saving model ...\n",
      "[  5/300] train_loss: 0.21479 valid_loss: 0.21049 test_loss: 0.22583 \n",
      "Validation loss decreased (0.230956 --> 0.210491).  Saving model ...\n",
      "[  6/300] train_loss: 0.19316 valid_loss: 0.19724 test_loss: 0.20982 \n",
      "Validation loss decreased (0.210491 --> 0.197242).  Saving model ...\n",
      "[  7/300] train_loss: 0.18628 valid_loss: 0.18664 test_loss: 0.19986 \n",
      "Validation loss decreased (0.197242 --> 0.186645).  Saving model ...\n",
      "[  8/300] train_loss: 0.17974 valid_loss: 0.17557 test_loss: 0.18681 \n",
      "Validation loss decreased (0.186645 --> 0.175566).  Saving model ...\n",
      "[  9/300] train_loss: 0.17257 valid_loss: 0.17401 test_loss: 0.18770 \n",
      "Validation loss decreased (0.175566 --> 0.174010).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16564 valid_loss: 0.16723 test_loss: 0.17850 \n",
      "Validation loss decreased (0.174010 --> 0.167227).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16606 valid_loss: 0.16853 test_loss: 0.17917 \n",
      "[ 12/300] train_loss: 0.16044 valid_loss: 0.16032 test_loss: 0.17125 \n",
      "Validation loss decreased (0.167227 --> 0.160318).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15713 valid_loss: 0.16009 test_loss: 0.17068 \n",
      "Validation loss decreased (0.160318 --> 0.160088).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15253 valid_loss: 0.15818 test_loss: 0.16543 \n",
      "Validation loss decreased (0.160088 --> 0.158178).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14694 valid_loss: 0.15265 test_loss: 0.16119 \n",
      "Validation loss decreased (0.158178 --> 0.152652).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14511 valid_loss: 0.15307 test_loss: 0.16310 \n",
      "[ 17/300] train_loss: 0.14237 valid_loss: 0.15011 test_loss: 0.15851 \n",
      "Validation loss decreased (0.152652 --> 0.150108).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14195 valid_loss: 0.14897 test_loss: 0.15688 \n",
      "Validation loss decreased (0.150108 --> 0.148971).  Saving model ...\n",
      "[ 19/300] train_loss: 0.14154 valid_loss: 0.15427 test_loss: 0.15979 \n",
      "[ 20/300] train_loss: 0.13475 valid_loss: 0.14633 test_loss: 0.15446 \n",
      "Validation loss decreased (0.148971 --> 0.146326).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13543 valid_loss: 0.15141 test_loss: 0.15705 \n",
      "[ 22/300] train_loss: 0.13441 valid_loss: 0.14421 test_loss: 0.15299 \n",
      "Validation loss decreased (0.146326 --> 0.144211).  Saving model ...\n",
      "[ 23/300] train_loss: 0.13367 valid_loss: 0.14226 test_loss: 0.15137 \n",
      "Validation loss decreased (0.144211 --> 0.142255).  Saving model ...\n",
      "[ 24/300] train_loss: 0.13327 valid_loss: 0.14089 test_loss: 0.14955 \n",
      "Validation loss decreased (0.142255 --> 0.140886).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12760 valid_loss: 0.14180 test_loss: 0.14995 \n",
      "[ 26/300] train_loss: 0.12703 valid_loss: 0.13972 test_loss: 0.14877 \n",
      "Validation loss decreased (0.140886 --> 0.139722).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12845 valid_loss: 0.14514 test_loss: 0.15288 \n",
      "[ 28/300] train_loss: 0.12450 valid_loss: 0.13623 test_loss: 0.14718 \n",
      "Validation loss decreased (0.139722 --> 0.136227).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12511 valid_loss: 0.13693 test_loss: 0.14758 \n",
      "[ 30/300] train_loss: 0.12229 valid_loss: 0.13969 test_loss: 0.14747 \n",
      "[ 31/300] train_loss: 0.12295 valid_loss: 0.13410 test_loss: 0.14480 \n",
      "Validation loss decreased (0.136227 --> 0.134100).  Saving model ...\n",
      "[ 32/300] train_loss: 0.12131 valid_loss: 0.13151 test_loss: 0.14236 \n",
      "Validation loss decreased (0.134100 --> 0.131511).  Saving model ...\n",
      "[ 33/300] train_loss: 0.12048 valid_loss: 0.13377 test_loss: 0.14290 \n",
      "[ 34/300] train_loss: 0.11714 valid_loss: 0.12809 test_loss: 0.13829 \n",
      "Validation loss decreased (0.131511 --> 0.128085).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11908 valid_loss: 0.12675 test_loss: 0.13771 \n",
      "Validation loss decreased (0.128085 --> 0.126751).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11766 valid_loss: 0.13257 test_loss: 0.14315 \n",
      "[ 37/300] train_loss: 0.11660 valid_loss: 0.12562 test_loss: 0.13659 \n",
      "Validation loss decreased (0.126751 --> 0.125617).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11580 valid_loss: 0.12768 test_loss: 0.13757 \n",
      "[ 39/300] train_loss: 0.11495 valid_loss: 0.12362 test_loss: 0.13436 \n",
      "Validation loss decreased (0.125617 --> 0.123616).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11192 valid_loss: 0.12678 test_loss: 0.13644 \n",
      "[ 41/300] train_loss: 0.11528 valid_loss: 0.12517 test_loss: 0.13672 \n",
      "[ 42/300] train_loss: 0.11217 valid_loss: 0.11978 test_loss: 0.13120 \n",
      "Validation loss decreased (0.123616 --> 0.119778).  Saving model ...\n",
      "[ 43/300] train_loss: 0.11159 valid_loss: 0.12350 test_loss: 0.13358 \n",
      "[ 44/300] train_loss: 0.10866 valid_loss: 0.12402 test_loss: 0.13370 \n",
      "[ 45/300] train_loss: 0.11146 valid_loss: 0.11875 test_loss: 0.13016 \n",
      "Validation loss decreased (0.119778 --> 0.118746).  Saving model ...\n",
      "[ 46/300] train_loss: 0.11056 valid_loss: 0.12149 test_loss: 0.13174 \n",
      "[ 47/300] train_loss: 0.10713 valid_loss: 0.11798 test_loss: 0.12972 \n",
      "Validation loss decreased (0.118746 --> 0.117979).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10580 valid_loss: 0.11698 test_loss: 0.12875 \n",
      "Validation loss decreased (0.117979 --> 0.116983).  Saving model ...\n",
      "[ 49/300] train_loss: 0.11235 valid_loss: 0.11846 test_loss: 0.13027 \n",
      "[ 50/300] train_loss: 0.10596 valid_loss: 0.11752 test_loss: 0.12915 \n",
      "[ 51/300] train_loss: 0.10481 valid_loss: 0.11251 test_loss: 0.12604 \n",
      "Validation loss decreased (0.116983 --> 0.112511).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10498 valid_loss: 0.11473 test_loss: 0.12635 \n",
      "[ 53/300] train_loss: 0.10242 valid_loss: 0.11323 test_loss: 0.12629 \n",
      "[ 54/300] train_loss: 0.10405 valid_loss: 0.11205 test_loss: 0.12430 \n",
      "Validation loss decreased (0.112511 --> 0.112053).  Saving model ...\n",
      "[ 55/300] train_loss: 0.10272 valid_loss: 0.11327 test_loss: 0.12555 \n",
      "[ 56/300] train_loss: 0.09938 valid_loss: 0.11375 test_loss: 0.12514 \n",
      "[ 57/300] train_loss: 0.09830 valid_loss: 0.11392 test_loss: 0.12355 \n",
      "[ 58/300] train_loss: 0.09949 valid_loss: 0.11820 test_loss: 0.12853 \n",
      "[ 59/300] train_loss: 0.10030 valid_loss: 0.11153 test_loss: 0.12378 \n",
      "Validation loss decreased (0.112053 --> 0.111531).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09961 valid_loss: 0.11303 test_loss: 0.12343 \n",
      "[ 61/300] train_loss: 0.09922 valid_loss: 0.11150 test_loss: 0.12277 \n",
      "Validation loss decreased (0.111531 --> 0.111501).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09962 valid_loss: 0.11078 test_loss: 0.12221 \n",
      "Validation loss decreased (0.111501 --> 0.110782).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09920 valid_loss: 0.10953 test_loss: 0.12202 \n",
      "Validation loss decreased (0.110782 --> 0.109534).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09690 valid_loss: 0.11227 test_loss: 0.12323 \n",
      "[ 65/300] train_loss: 0.09699 valid_loss: 0.11161 test_loss: 0.12223 \n",
      "[ 66/300] train_loss: 0.09625 valid_loss: 0.10798 test_loss: 0.11987 \n",
      "Validation loss decreased (0.109534 --> 0.107983).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09594 valid_loss: 0.10855 test_loss: 0.11902 \n",
      "[ 68/300] train_loss: 0.09624 valid_loss: 0.10636 test_loss: 0.11886 \n",
      "Validation loss decreased (0.107983 --> 0.106360).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09602 valid_loss: 0.10888 test_loss: 0.12020 \n",
      "[ 70/300] train_loss: 0.09673 valid_loss: 0.10611 test_loss: 0.11764 \n",
      "Validation loss decreased (0.106360 --> 0.106111).  Saving model ...\n",
      "[ 71/300] train_loss: 0.09414 valid_loss: 0.10772 test_loss: 0.11818 \n",
      "[ 72/300] train_loss: 0.09768 valid_loss: 0.10459 test_loss: 0.11687 \n",
      "Validation loss decreased (0.106111 --> 0.104588).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09356 valid_loss: 0.10546 test_loss: 0.11738 \n",
      "[ 74/300] train_loss: 0.09292 valid_loss: 0.10380 test_loss: 0.11594 \n",
      "Validation loss decreased (0.104588 --> 0.103802).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09437 valid_loss: 0.10297 test_loss: 0.11500 \n",
      "Validation loss decreased (0.103802 --> 0.102971).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09302 valid_loss: 0.10628 test_loss: 0.11744 \n",
      "[ 77/300] train_loss: 0.09435 valid_loss: 0.10276 test_loss: 0.11434 \n",
      "Validation loss decreased (0.102971 --> 0.102762).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09252 valid_loss: 0.10450 test_loss: 0.11522 \n",
      "[ 79/300] train_loss: 0.09263 valid_loss: 0.10290 test_loss: 0.11382 \n",
      "[ 80/300] train_loss: 0.09419 valid_loss: 0.10210 test_loss: 0.11372 \n",
      "Validation loss decreased (0.102762 --> 0.102098).  Saving model ...\n",
      "[ 81/300] train_loss: 0.09279 valid_loss: 0.10478 test_loss: 0.11553 \n",
      "[ 82/300] train_loss: 0.09246 valid_loss: 0.10512 test_loss: 0.11717 \n",
      "[ 83/300] train_loss: 0.09124 valid_loss: 0.10322 test_loss: 0.11210 \n",
      "[ 84/300] train_loss: 0.08871 valid_loss: 0.10462 test_loss: 0.11453 \n",
      "[ 85/300] train_loss: 0.08916 valid_loss: 0.10135 test_loss: 0.11279 \n",
      "Validation loss decreased (0.102098 --> 0.101348).  Saving model ...\n",
      "[ 86/300] train_loss: 0.09124 valid_loss: 0.09878 test_loss: 0.11100 \n",
      "Validation loss decreased (0.101348 --> 0.098780).  Saving model ...\n",
      "[ 87/300] train_loss: 0.09096 valid_loss: 0.10119 test_loss: 0.11121 \n",
      "[ 88/300] train_loss: 0.08866 valid_loss: 0.10018 test_loss: 0.11110 \n",
      "[ 89/300] train_loss: 0.08804 valid_loss: 0.09806 test_loss: 0.10913 \n",
      "Validation loss decreased (0.098780 --> 0.098057).  Saving model ...\n",
      "[ 90/300] train_loss: 0.08923 valid_loss: 0.10156 test_loss: 0.11079 \n",
      "[ 91/300] train_loss: 0.08879 valid_loss: 0.09875 test_loss: 0.10973 \n",
      "[ 92/300] train_loss: 0.08933 valid_loss: 0.09901 test_loss: 0.11071 \n",
      "[ 93/300] train_loss: 0.08824 valid_loss: 0.10009 test_loss: 0.11087 \n",
      "[ 94/300] train_loss: 0.08929 valid_loss: 0.09749 test_loss: 0.10848 \n",
      "Validation loss decreased (0.098057 --> 0.097489).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08584 valid_loss: 0.09951 test_loss: 0.10833 \n",
      "[ 96/300] train_loss: 0.08779 valid_loss: 0.09680 test_loss: 0.10939 \n",
      "Validation loss decreased (0.097489 --> 0.096801).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08547 valid_loss: 0.09853 test_loss: 0.10926 \n",
      "[ 98/300] train_loss: 0.08852 valid_loss: 0.09817 test_loss: 0.10745 \n",
      "[ 99/300] train_loss: 0.08687 valid_loss: 0.09670 test_loss: 0.10800 \n",
      "Validation loss decreased (0.096801 --> 0.096696).  Saving model ...\n",
      "[100/300] train_loss: 0.08825 valid_loss: 0.09860 test_loss: 0.10925 \n",
      "[101/300] train_loss: 0.08721 valid_loss: 0.09796 test_loss: 0.10776 \n",
      "[102/300] train_loss: 0.08527 valid_loss: 0.09679 test_loss: 0.10704 \n",
      "[103/300] train_loss: 0.08478 valid_loss: 0.09646 test_loss: 0.10583 \n",
      "Validation loss decreased (0.096696 --> 0.096460).  Saving model ...\n",
      "[104/300] train_loss: 0.08645 valid_loss: 0.09582 test_loss: 0.10739 \n",
      "Validation loss decreased (0.096460 --> 0.095821).  Saving model ...\n",
      "[105/300] train_loss: 0.08479 valid_loss: 0.09671 test_loss: 0.10814 \n",
      "[106/300] train_loss: 0.08633 valid_loss: 0.09400 test_loss: 0.10615 \n",
      "Validation loss decreased (0.095821 --> 0.094005).  Saving model ...\n",
      "[107/300] train_loss: 0.08381 valid_loss: 0.09550 test_loss: 0.10524 \n",
      "[108/300] train_loss: 0.08423 valid_loss: 0.09343 test_loss: 0.10589 \n",
      "Validation loss decreased (0.094005 --> 0.093432).  Saving model ...\n",
      "[109/300] train_loss: 0.08513 valid_loss: 0.09417 test_loss: 0.10495 \n",
      "[110/300] train_loss: 0.08348 valid_loss: 0.09366 test_loss: 0.10496 \n",
      "[111/300] train_loss: 0.08388 valid_loss: 0.09330 test_loss: 0.10418 \n",
      "Validation loss decreased (0.093432 --> 0.093300).  Saving model ...\n",
      "[112/300] train_loss: 0.08327 valid_loss: 0.09455 test_loss: 0.10483 \n",
      "[113/300] train_loss: 0.08546 valid_loss: 0.09250 test_loss: 0.10301 \n",
      "Validation loss decreased (0.093300 --> 0.092504).  Saving model ...\n",
      "[114/300] train_loss: 0.08399 valid_loss: 0.09532 test_loss: 0.10627 \n",
      "[115/300] train_loss: 0.08218 valid_loss: 0.09306 test_loss: 0.10279 \n",
      "[116/300] train_loss: 0.08272 valid_loss: 0.09121 test_loss: 0.10290 \n",
      "Validation loss decreased (0.092504 --> 0.091208).  Saving model ...\n",
      "[117/300] train_loss: 0.08369 valid_loss: 0.09207 test_loss: 0.10265 \n",
      "[118/300] train_loss: 0.08188 valid_loss: 0.09188 test_loss: 0.10354 \n",
      "[119/300] train_loss: 0.08398 valid_loss: 0.09300 test_loss: 0.10387 \n",
      "[120/300] train_loss: 0.08473 valid_loss: 0.09325 test_loss: 0.10408 \n",
      "[121/300] train_loss: 0.08402 valid_loss: 0.09093 test_loss: 0.10246 \n",
      "Validation loss decreased (0.091208 --> 0.090929).  Saving model ...\n",
      "[122/300] train_loss: 0.08234 valid_loss: 0.09238 test_loss: 0.10299 \n",
      "[123/300] train_loss: 0.08101 valid_loss: 0.09171 test_loss: 0.10160 \n",
      "[124/300] train_loss: 0.08368 valid_loss: 0.09110 test_loss: 0.10093 \n",
      "[125/300] train_loss: 0.08103 valid_loss: 0.09299 test_loss: 0.10236 \n",
      "[126/300] train_loss: 0.08074 valid_loss: 0.09143 test_loss: 0.10184 \n",
      "[127/300] train_loss: 0.08166 valid_loss: 0.08954 test_loss: 0.10127 \n",
      "Validation loss decreased (0.090929 --> 0.089542).  Saving model ...\n",
      "[128/300] train_loss: 0.08032 valid_loss: 0.08930 test_loss: 0.10027 \n",
      "Validation loss decreased (0.089542 --> 0.089296).  Saving model ...\n",
      "[129/300] train_loss: 0.08223 valid_loss: 0.09038 test_loss: 0.10096 \n",
      "[130/300] train_loss: 0.07985 valid_loss: 0.08995 test_loss: 0.10054 \n",
      "[131/300] train_loss: 0.07989 valid_loss: 0.09182 test_loss: 0.10101 \n",
      "[132/300] train_loss: 0.08050 valid_loss: 0.09075 test_loss: 0.10130 \n",
      "[133/300] train_loss: 0.07971 valid_loss: 0.08896 test_loss: 0.09935 \n",
      "Validation loss decreased (0.089296 --> 0.088959).  Saving model ...\n",
      "[134/300] train_loss: 0.07990 valid_loss: 0.08877 test_loss: 0.10016 \n",
      "Validation loss decreased (0.088959 --> 0.088766).  Saving model ...\n",
      "[135/300] train_loss: 0.07989 valid_loss: 0.08837 test_loss: 0.10016 \n",
      "Validation loss decreased (0.088766 --> 0.088366).  Saving model ...\n",
      "[136/300] train_loss: 0.07927 valid_loss: 0.09077 test_loss: 0.10016 \n",
      "[137/300] train_loss: 0.07850 valid_loss: 0.08838 test_loss: 0.10024 \n",
      "[138/300] train_loss: 0.07814 valid_loss: 0.09240 test_loss: 0.10153 \n",
      "[139/300] train_loss: 0.08360 valid_loss: 0.09013 test_loss: 0.10050 \n",
      "[140/300] train_loss: 0.08161 valid_loss: 0.08885 test_loss: 0.09895 \n",
      "[141/300] train_loss: 0.07822 valid_loss: 0.09015 test_loss: 0.10085 \n",
      "[142/300] train_loss: 0.07674 valid_loss: 0.08969 test_loss: 0.10003 \n",
      "[143/300] train_loss: 0.07737 valid_loss: 0.08849 test_loss: 0.09877 \n",
      "[144/300] train_loss: 0.07541 valid_loss: 0.08767 test_loss: 0.09886 \n",
      "Validation loss decreased (0.088366 --> 0.087671).  Saving model ...\n",
      "[145/300] train_loss: 0.07810 valid_loss: 0.08866 test_loss: 0.09909 \n",
      "[146/300] train_loss: 0.07457 valid_loss: 0.08755 test_loss: 0.09880 \n",
      "Validation loss decreased (0.087671 --> 0.087552).  Saving model ...\n",
      "[147/300] train_loss: 0.07647 valid_loss: 0.08835 test_loss: 0.09927 \n",
      "[148/300] train_loss: 0.07782 valid_loss: 0.08672 test_loss: 0.09837 \n",
      "Validation loss decreased (0.087552 --> 0.086725).  Saving model ...\n",
      "[149/300] train_loss: 0.07784 valid_loss: 0.08821 test_loss: 0.09892 \n",
      "[150/300] train_loss: 0.07653 valid_loss: 0.08915 test_loss: 0.09891 \n",
      "[151/300] train_loss: 0.08005 valid_loss: 0.08806 test_loss: 0.09918 \n",
      "[152/300] train_loss: 0.07771 valid_loss: 0.09017 test_loss: 0.10088 \n",
      "[153/300] train_loss: 0.07497 valid_loss: 0.08687 test_loss: 0.09709 \n",
      "[154/300] train_loss: 0.07529 valid_loss: 0.08763 test_loss: 0.09818 \n",
      "[155/300] train_loss: 0.07597 valid_loss: 0.08839 test_loss: 0.09850 \n",
      "[156/300] train_loss: 0.07796 valid_loss: 0.08674 test_loss: 0.09812 \n",
      "[157/300] train_loss: 0.07384 valid_loss: 0.08786 test_loss: 0.09917 \n",
      "[158/300] train_loss: 0.07606 valid_loss: 0.08728 test_loss: 0.09814 \n",
      "[159/300] train_loss: 0.07373 valid_loss: 0.08568 test_loss: 0.09706 \n",
      "Validation loss decreased (0.086725 --> 0.085675).  Saving model ...\n",
      "[160/300] train_loss: 0.07604 valid_loss: 0.08915 test_loss: 0.10006 \n",
      "[161/300] train_loss: 0.07755 valid_loss: 0.08669 test_loss: 0.09717 \n",
      "[162/300] train_loss: 0.07789 valid_loss: 0.08665 test_loss: 0.09648 \n",
      "[163/300] train_loss: 0.07549 valid_loss: 0.08815 test_loss: 0.09778 \n",
      "[164/300] train_loss: 0.07442 valid_loss: 0.08810 test_loss: 0.09809 \n",
      "[165/300] train_loss: 0.07416 valid_loss: 0.08627 test_loss: 0.09663 \n",
      "[166/300] train_loss: 0.07416 valid_loss: 0.08502 test_loss: 0.09598 \n",
      "Validation loss decreased (0.085675 --> 0.085024).  Saving model ...\n",
      "[167/300] train_loss: 0.07441 valid_loss: 0.08621 test_loss: 0.09617 \n",
      "[168/300] train_loss: 0.07412 valid_loss: 0.08564 test_loss: 0.09598 \n",
      "[169/300] train_loss: 0.07377 valid_loss: 0.08504 test_loss: 0.09635 \n",
      "[170/300] train_loss: 0.07261 valid_loss: 0.08465 test_loss: 0.09581 \n",
      "Validation loss decreased (0.085024 --> 0.084646).  Saving model ...\n",
      "[171/300] train_loss: 0.07589 valid_loss: 0.08645 test_loss: 0.09674 \n",
      "[172/300] train_loss: 0.07224 valid_loss: 0.08465 test_loss: 0.09559 \n",
      "[173/300] train_loss: 0.07470 valid_loss: 0.08658 test_loss: 0.09685 \n",
      "[174/300] train_loss: 0.07385 valid_loss: 0.08480 test_loss: 0.09513 \n",
      "[175/300] train_loss: 0.07154 valid_loss: 0.08612 test_loss: 0.09711 \n",
      "[176/300] train_loss: 0.07402 valid_loss: 0.08620 test_loss: 0.09637 \n",
      "[177/300] train_loss: 0.07170 valid_loss: 0.08536 test_loss: 0.09499 \n",
      "[178/300] train_loss: 0.07185 valid_loss: 0.08522 test_loss: 0.09613 \n",
      "[179/300] train_loss: 0.07261 valid_loss: 0.08484 test_loss: 0.09610 \n",
      "[180/300] train_loss: 0.07370 valid_loss: 0.08447 test_loss: 0.09485 \n",
      "Validation loss decreased (0.084646 --> 0.084466).  Saving model ...\n",
      "[181/300] train_loss: 0.07173 valid_loss: 0.08432 test_loss: 0.09535 \n",
      "Validation loss decreased (0.084466 --> 0.084320).  Saving model ...\n",
      "[182/300] train_loss: 0.07381 valid_loss: 0.08441 test_loss: 0.09467 \n",
      "[183/300] train_loss: 0.07224 valid_loss: 0.08786 test_loss: 0.09706 \n",
      "[184/300] train_loss: 0.07127 valid_loss: 0.08632 test_loss: 0.09542 \n",
      "[185/300] train_loss: 0.07095 valid_loss: 0.08489 test_loss: 0.09412 \n",
      "[186/300] train_loss: 0.07404 valid_loss: 0.08385 test_loss: 0.09422 \n",
      "Validation loss decreased (0.084320 --> 0.083847).  Saving model ...\n",
      "[187/300] train_loss: 0.07361 valid_loss: 0.08430 test_loss: 0.09492 \n",
      "[188/300] train_loss: 0.07289 valid_loss: 0.08391 test_loss: 0.09462 \n",
      "[189/300] train_loss: 0.07283 valid_loss: 0.08460 test_loss: 0.09412 \n",
      "[190/300] train_loss: 0.07130 valid_loss: 0.08370 test_loss: 0.09403 \n",
      "Validation loss decreased (0.083847 --> 0.083702).  Saving model ...\n",
      "[191/300] train_loss: 0.07171 valid_loss: 0.08444 test_loss: 0.09472 \n",
      "[192/300] train_loss: 0.07305 valid_loss: 0.08564 test_loss: 0.09460 \n",
      "[193/300] train_loss: 0.07089 valid_loss: 0.08395 test_loss: 0.09296 \n",
      "[194/300] train_loss: 0.07001 valid_loss: 0.08582 test_loss: 0.09512 \n",
      "[195/300] train_loss: 0.07129 valid_loss: 0.08288 test_loss: 0.09374 \n",
      "Validation loss decreased (0.083702 --> 0.082884).  Saving model ...\n",
      "[196/300] train_loss: 0.07194 valid_loss: 0.08408 test_loss: 0.09377 \n",
      "[197/300] train_loss: 0.06960 valid_loss: 0.08425 test_loss: 0.09409 \n",
      "[198/300] train_loss: 0.07149 valid_loss: 0.08304 test_loss: 0.09285 \n",
      "[199/300] train_loss: 0.07078 valid_loss: 0.08273 test_loss: 0.09334 \n",
      "Validation loss decreased (0.082884 --> 0.082733).  Saving model ...\n",
      "[200/300] train_loss: 0.06921 valid_loss: 0.08133 test_loss: 0.09231 \n",
      "Validation loss decreased (0.082733 --> 0.081326).  Saving model ...\n",
      "[201/300] train_loss: 0.07027 valid_loss: 0.08326 test_loss: 0.09393 \n",
      "[202/300] train_loss: 0.07125 valid_loss: 0.08165 test_loss: 0.09264 \n",
      "[203/300] train_loss: 0.07148 valid_loss: 0.08184 test_loss: 0.09233 \n",
      "[204/300] train_loss: 0.06744 valid_loss: 0.08216 test_loss: 0.09242 \n",
      "[205/300] train_loss: 0.07081 valid_loss: 0.08183 test_loss: 0.09335 \n",
      "[206/300] train_loss: 0.07224 valid_loss: 0.08278 test_loss: 0.09281 \n",
      "[207/300] train_loss: 0.06845 valid_loss: 0.08301 test_loss: 0.09296 \n",
      "[208/300] train_loss: 0.06931 valid_loss: 0.08359 test_loss: 0.09295 \n",
      "[209/300] train_loss: 0.07051 valid_loss: 0.08216 test_loss: 0.09230 \n",
      "[210/300] train_loss: 0.07002 valid_loss: 0.08283 test_loss: 0.09310 \n",
      "[211/300] train_loss: 0.06977 valid_loss: 0.08348 test_loss: 0.09239 \n",
      "[212/300] train_loss: 0.07017 valid_loss: 0.08164 test_loss: 0.09281 \n",
      "[213/300] train_loss: 0.06764 valid_loss: 0.08117 test_loss: 0.09196 \n",
      "Validation loss decreased (0.081326 --> 0.081167).  Saving model ...\n",
      "[214/300] train_loss: 0.06852 valid_loss: 0.08103 test_loss: 0.09128 \n",
      "Validation loss decreased (0.081167 --> 0.081033).  Saving model ...\n",
      "[215/300] train_loss: 0.06876 valid_loss: 0.08352 test_loss: 0.09284 \n",
      "[216/300] train_loss: 0.06743 valid_loss: 0.08212 test_loss: 0.09189 \n",
      "[217/300] train_loss: 0.06917 valid_loss: 0.08177 test_loss: 0.09193 \n",
      "[218/300] train_loss: 0.06713 valid_loss: 0.08268 test_loss: 0.09303 \n",
      "[219/300] train_loss: 0.06795 valid_loss: 0.08141 test_loss: 0.09183 \n",
      "[220/300] train_loss: 0.06913 valid_loss: 0.08128 test_loss: 0.09249 \n",
      "[221/300] train_loss: 0.06802 valid_loss: 0.08198 test_loss: 0.09209 \n",
      "[222/300] train_loss: 0.06954 valid_loss: 0.08261 test_loss: 0.09220 \n",
      "[223/300] train_loss: 0.06934 valid_loss: 0.08291 test_loss: 0.09266 \n",
      "[224/300] train_loss: 0.06733 valid_loss: 0.08220 test_loss: 0.09228 \n",
      "[225/300] train_loss: 0.06848 valid_loss: 0.08047 test_loss: 0.09153 \n",
      "Validation loss decreased (0.081033 --> 0.080467).  Saving model ...\n",
      "[226/300] train_loss: 0.06909 valid_loss: 0.08135 test_loss: 0.09212 \n",
      "[227/300] train_loss: 0.06896 valid_loss: 0.08070 test_loss: 0.09185 \n",
      "[228/300] train_loss: 0.06703 valid_loss: 0.08220 test_loss: 0.09201 \n",
      "[229/300] train_loss: 0.06672 valid_loss: 0.08309 test_loss: 0.09205 \n",
      "[230/300] train_loss: 0.06580 valid_loss: 0.08109 test_loss: 0.09109 \n",
      "[231/300] train_loss: 0.06783 valid_loss: 0.08054 test_loss: 0.09069 \n",
      "[232/300] train_loss: 0.06523 valid_loss: 0.08033 test_loss: 0.09056 \n",
      "Validation loss decreased (0.080467 --> 0.080334).  Saving model ...\n",
      "[233/300] train_loss: 0.06753 valid_loss: 0.08278 test_loss: 0.09367 \n",
      "[234/300] train_loss: 0.06862 valid_loss: 0.08124 test_loss: 0.09172 \n",
      "[235/300] train_loss: 0.06809 valid_loss: 0.08194 test_loss: 0.09193 \n",
      "[236/300] train_loss: 0.06709 valid_loss: 0.08325 test_loss: 0.09294 \n",
      "[237/300] train_loss: 0.06687 valid_loss: 0.08297 test_loss: 0.09242 \n",
      "[238/300] train_loss: 0.06617 valid_loss: 0.08076 test_loss: 0.09179 \n",
      "[239/300] train_loss: 0.06714 valid_loss: 0.08054 test_loss: 0.09104 \n",
      "[240/300] train_loss: 0.06643 valid_loss: 0.08057 test_loss: 0.09083 \n",
      "[241/300] train_loss: 0.06623 valid_loss: 0.08299 test_loss: 0.09047 \n",
      "[242/300] train_loss: 0.06727 valid_loss: 0.08028 test_loss: 0.09077 \n",
      "Validation loss decreased (0.080334 --> 0.080277).  Saving model ...\n",
      "[243/300] train_loss: 0.06567 valid_loss: 0.07983 test_loss: 0.09110 \n",
      "Validation loss decreased (0.080277 --> 0.079826).  Saving model ...\n",
      "[244/300] train_loss: 0.06602 valid_loss: 0.08105 test_loss: 0.09112 \n",
      "[245/300] train_loss: 0.06646 valid_loss: 0.08055 test_loss: 0.09138 \n",
      "[246/300] train_loss: 0.06555 valid_loss: 0.08196 test_loss: 0.09048 \n",
      "[247/300] train_loss: 0.06693 valid_loss: 0.07970 test_loss: 0.09141 \n",
      "Validation loss decreased (0.079826 --> 0.079700).  Saving model ...\n",
      "[248/300] train_loss: 0.06522 valid_loss: 0.08101 test_loss: 0.09149 \n",
      "[249/300] train_loss: 0.06398 valid_loss: 0.07807 test_loss: 0.09040 \n",
      "Validation loss decreased (0.079700 --> 0.078074).  Saving model ...\n",
      "[250/300] train_loss: 0.06502 valid_loss: 0.08011 test_loss: 0.09101 \n",
      "[251/300] train_loss: 0.06623 valid_loss: 0.08002 test_loss: 0.09000 \n",
      "[252/300] train_loss: 0.06500 valid_loss: 0.08013 test_loss: 0.09075 \n",
      "[253/300] train_loss: 0.06618 valid_loss: 0.07998 test_loss: 0.09084 \n",
      "[254/300] train_loss: 0.06664 valid_loss: 0.07913 test_loss: 0.09050 \n",
      "[255/300] train_loss: 0.06535 valid_loss: 0.07893 test_loss: 0.09087 \n",
      "[256/300] train_loss: 0.06527 valid_loss: 0.07842 test_loss: 0.08981 \n",
      "[257/300] train_loss: 0.06707 valid_loss: 0.07977 test_loss: 0.09094 \n",
      "[258/300] train_loss: 0.06527 valid_loss: 0.07894 test_loss: 0.08992 \n",
      "[259/300] train_loss: 0.06647 valid_loss: 0.07853 test_loss: 0.09005 \n",
      "[260/300] train_loss: 0.06497 valid_loss: 0.07892 test_loss: 0.09098 \n",
      "[261/300] train_loss: 0.06507 valid_loss: 0.08127 test_loss: 0.09167 \n",
      "[262/300] train_loss: 0.06412 valid_loss: 0.07858 test_loss: 0.08893 \n",
      "[263/300] train_loss: 0.06584 valid_loss: 0.07968 test_loss: 0.09003 \n",
      "[264/300] train_loss: 0.06652 valid_loss: 0.07894 test_loss: 0.08993 \n",
      "[265/300] train_loss: 0.06451 valid_loss: 0.08073 test_loss: 0.08988 \n",
      "[266/300] train_loss: 0.06467 valid_loss: 0.08001 test_loss: 0.09000 \n",
      "[267/300] train_loss: 0.06487 valid_loss: 0.08044 test_loss: 0.08949 \n",
      "[268/300] train_loss: 0.06328 valid_loss: 0.07916 test_loss: 0.08938 \n",
      "[269/300] train_loss: 0.06329 valid_loss: 0.07897 test_loss: 0.08832 \n",
      "[270/300] train_loss: 0.06537 valid_loss: 0.07945 test_loss: 0.09004 \n",
      "[271/300] train_loss: 0.06384 valid_loss: 0.07849 test_loss: 0.08851 \n",
      "[272/300] train_loss: 0.06249 valid_loss: 0.07915 test_loss: 0.09012 \n",
      "[273/300] train_loss: 0.06496 valid_loss: 0.07988 test_loss: 0.08926 \n",
      "[274/300] train_loss: 0.06272 valid_loss: 0.08013 test_loss: 0.09062 \n",
      "[275/300] train_loss: 0.06369 valid_loss: 0.07862 test_loss: 0.08855 \n",
      "[276/300] train_loss: 0.06427 valid_loss: 0.07983 test_loss: 0.08941 \n",
      "[277/300] train_loss: 0.06661 valid_loss: 0.07867 test_loss: 0.08721 \n",
      "[278/300] train_loss: 0.06595 valid_loss: 0.08042 test_loss: 0.08923 \n",
      "[279/300] train_loss: 0.06356 valid_loss: 0.08060 test_loss: 0.09025 \n",
      "[280/300] train_loss: 0.06534 valid_loss: 0.07872 test_loss: 0.08847 \n",
      "[281/300] train_loss: 0.06011 valid_loss: 0.07819 test_loss: 0.08875 \n",
      "[282/300] train_loss: 0.06164 valid_loss: 0.07927 test_loss: 0.08830 \n",
      "[283/300] train_loss: 0.06452 valid_loss: 0.07865 test_loss: 0.08787 \n",
      "[284/300] train_loss: 0.06442 valid_loss: 0.07929 test_loss: 0.08827 \n",
      "[285/300] train_loss: 0.06343 valid_loss: 0.07962 test_loss: 0.08835 \n",
      "[286/300] train_loss: 0.06482 valid_loss: 0.07674 test_loss: 0.08797 \n",
      "Validation loss decreased (0.078074 --> 0.076735).  Saving model ...\n",
      "[287/300] train_loss: 0.06542 valid_loss: 0.07781 test_loss: 0.08891 \n",
      "[288/300] train_loss: 0.06257 valid_loss: 0.07810 test_loss: 0.08843 \n",
      "[289/300] train_loss: 0.06353 valid_loss: 0.07772 test_loss: 0.08756 \n",
      "[290/300] train_loss: 0.06482 valid_loss: 0.07894 test_loss: 0.08989 \n",
      "[291/300] train_loss: 0.06288 valid_loss: 0.07742 test_loss: 0.08906 \n",
      "[292/300] train_loss: 0.06209 valid_loss: 0.07798 test_loss: 0.08798 \n",
      "[293/300] train_loss: 0.06221 valid_loss: 0.07821 test_loss: 0.08834 \n",
      "[294/300] train_loss: 0.06424 valid_loss: 0.07837 test_loss: 0.08875 \n",
      "[295/300] train_loss: 0.06116 valid_loss: 0.07804 test_loss: 0.08761 \n",
      "[296/300] train_loss: 0.06429 valid_loss: 0.07753 test_loss: 0.08720 \n",
      "[297/300] train_loss: 0.06145 valid_loss: 0.07847 test_loss: 0.08876 \n",
      "[298/300] train_loss: 0.06175 valid_loss: 0.08000 test_loss: 0.08924 \n",
      "[299/300] train_loss: 0.06111 valid_loss: 0.07823 test_loss: 0.08942 \n",
      "[300/300] train_loss: 0.06269 valid_loss: 0.07823 test_loss: 0.08843 \n",
      "TRAINING MODEL 9\n",
      "[  1/300] train_loss: 0.60414 valid_loss: 0.50811 test_loss: 0.51228 \n",
      "Validation loss decreased (inf --> 0.508107).  Saving model ...\n",
      "[  2/300] train_loss: 0.41161 valid_loss: 0.36585 test_loss: 0.37276 \n",
      "Validation loss decreased (0.508107 --> 0.365848).  Saving model ...\n",
      "[  3/300] train_loss: 0.30493 valid_loss: 0.29048 test_loss: 0.30112 \n",
      "Validation loss decreased (0.365848 --> 0.290482).  Saving model ...\n",
      "[  4/300] train_loss: 0.24803 valid_loss: 0.24771 test_loss: 0.26122 \n",
      "Validation loss decreased (0.290482 --> 0.247712).  Saving model ...\n",
      "[  5/300] train_loss: 0.22132 valid_loss: 0.21816 test_loss: 0.23346 \n",
      "Validation loss decreased (0.247712 --> 0.218156).  Saving model ...\n",
      "[  6/300] train_loss: 0.19783 valid_loss: 0.20009 test_loss: 0.21803 \n",
      "Validation loss decreased (0.218156 --> 0.200095).  Saving model ...\n",
      "[  7/300] train_loss: 0.18693 valid_loss: 0.18658 test_loss: 0.20141 \n",
      "Validation loss decreased (0.200095 --> 0.186575).  Saving model ...\n",
      "[  8/300] train_loss: 0.17673 valid_loss: 0.17500 test_loss: 0.18952 \n",
      "Validation loss decreased (0.186575 --> 0.174997).  Saving model ...\n",
      "[  9/300] train_loss: 0.16764 valid_loss: 0.16801 test_loss: 0.18313 \n",
      "Validation loss decreased (0.174997 --> 0.168007).  Saving model ...\n",
      "[ 10/300] train_loss: 0.15892 valid_loss: 0.16219 test_loss: 0.17672 \n",
      "Validation loss decreased (0.168007 --> 0.162186).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15994 valid_loss: 0.15862 test_loss: 0.17105 \n",
      "Validation loss decreased (0.162186 --> 0.158623).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15095 valid_loss: 0.15581 test_loss: 0.16867 \n",
      "Validation loss decreased (0.158623 --> 0.155806).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14705 valid_loss: 0.14998 test_loss: 0.16445 \n",
      "Validation loss decreased (0.155806 --> 0.149985).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14404 valid_loss: 0.14764 test_loss: 0.16137 \n",
      "Validation loss decreased (0.149985 --> 0.147638).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14206 valid_loss: 0.14737 test_loss: 0.15926 \n",
      "Validation loss decreased (0.147638 --> 0.147367).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14050 valid_loss: 0.14279 test_loss: 0.15454 \n",
      "Validation loss decreased (0.147367 --> 0.142791).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13671 valid_loss: 0.14274 test_loss: 0.15596 \n",
      "Validation loss decreased (0.142791 --> 0.142744).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13350 valid_loss: 0.14100 test_loss: 0.15439 \n",
      "Validation loss decreased (0.142744 --> 0.140997).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13160 valid_loss: 0.14200 test_loss: 0.15563 \n",
      "[ 20/300] train_loss: 0.13310 valid_loss: 0.13580 test_loss: 0.14828 \n",
      "Validation loss decreased (0.140997 --> 0.135799).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12677 valid_loss: 0.13651 test_loss: 0.15109 \n",
      "[ 22/300] train_loss: 0.12771 valid_loss: 0.13410 test_loss: 0.14871 \n",
      "Validation loss decreased (0.135799 --> 0.134102).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12215 valid_loss: 0.13152 test_loss: 0.14474 \n",
      "Validation loss decreased (0.134102 --> 0.131520).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12038 valid_loss: 0.13210 test_loss: 0.14527 \n",
      "[ 25/300] train_loss: 0.12321 valid_loss: 0.12728 test_loss: 0.14217 \n",
      "Validation loss decreased (0.131520 --> 0.127283).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12028 valid_loss: 0.12686 test_loss: 0.14096 \n",
      "Validation loss decreased (0.127283 --> 0.126857).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11859 valid_loss: 0.12596 test_loss: 0.14028 \n",
      "Validation loss decreased (0.126857 --> 0.125962).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11637 valid_loss: 0.12609 test_loss: 0.13970 \n",
      "[ 29/300] train_loss: 0.12052 valid_loss: 0.12484 test_loss: 0.13868 \n",
      "Validation loss decreased (0.125962 --> 0.124840).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11356 valid_loss: 0.12306 test_loss: 0.13861 \n",
      "Validation loss decreased (0.124840 --> 0.123060).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11255 valid_loss: 0.12075 test_loss: 0.13681 \n",
      "Validation loss decreased (0.123060 --> 0.120751).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11521 valid_loss: 0.12190 test_loss: 0.13370 \n",
      "[ 33/300] train_loss: 0.11490 valid_loss: 0.11966 test_loss: 0.13192 \n",
      "Validation loss decreased (0.120751 --> 0.119660).  Saving model ...\n",
      "[ 34/300] train_loss: 0.10940 valid_loss: 0.11902 test_loss: 0.13126 \n",
      "Validation loss decreased (0.119660 --> 0.119024).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11071 valid_loss: 0.11995 test_loss: 0.13342 \n",
      "[ 36/300] train_loss: 0.10832 valid_loss: 0.11685 test_loss: 0.13035 \n",
      "Validation loss decreased (0.119024 --> 0.116848).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11066 valid_loss: 0.11942 test_loss: 0.13171 \n",
      "[ 38/300] train_loss: 0.10905 valid_loss: 0.11747 test_loss: 0.13038 \n",
      "[ 39/300] train_loss: 0.10931 valid_loss: 0.11523 test_loss: 0.12815 \n",
      "Validation loss decreased (0.116848 --> 0.115232).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10734 valid_loss: 0.11426 test_loss: 0.12696 \n",
      "Validation loss decreased (0.115232 --> 0.114260).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10852 valid_loss: 0.11309 test_loss: 0.12626 \n",
      "Validation loss decreased (0.114260 --> 0.113094).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10575 valid_loss: 0.11325 test_loss: 0.12709 \n",
      "[ 43/300] train_loss: 0.10816 valid_loss: 0.11447 test_loss: 0.12674 \n",
      "[ 44/300] train_loss: 0.10607 valid_loss: 0.11354 test_loss: 0.12607 \n",
      "[ 45/300] train_loss: 0.10197 valid_loss: 0.11014 test_loss: 0.12369 \n",
      "Validation loss decreased (0.113094 --> 0.110144).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10055 valid_loss: 0.11211 test_loss: 0.12386 \n",
      "[ 47/300] train_loss: 0.10420 valid_loss: 0.10981 test_loss: 0.12199 \n",
      "Validation loss decreased (0.110144 --> 0.109810).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10107 valid_loss: 0.10964 test_loss: 0.12222 \n",
      "Validation loss decreased (0.109810 --> 0.109635).  Saving model ...\n",
      "[ 49/300] train_loss: 0.09980 valid_loss: 0.10855 test_loss: 0.12134 \n",
      "Validation loss decreased (0.109635 --> 0.108549).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10010 valid_loss: 0.10850 test_loss: 0.12163 \n",
      "Validation loss decreased (0.108549 --> 0.108497).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10097 valid_loss: 0.10819 test_loss: 0.12155 \n",
      "Validation loss decreased (0.108497 --> 0.108192).  Saving model ...\n",
      "[ 52/300] train_loss: 0.09881 valid_loss: 0.10933 test_loss: 0.12244 \n",
      "[ 53/300] train_loss: 0.10042 valid_loss: 0.10580 test_loss: 0.11897 \n",
      "Validation loss decreased (0.108192 --> 0.105796).  Saving model ...\n",
      "[ 54/300] train_loss: 0.09859 valid_loss: 0.10595 test_loss: 0.11880 \n",
      "[ 55/300] train_loss: 0.09502 valid_loss: 0.10518 test_loss: 0.11883 \n",
      "Validation loss decreased (0.105796 --> 0.105182).  Saving model ...\n",
      "[ 56/300] train_loss: 0.09721 valid_loss: 0.10536 test_loss: 0.11875 \n",
      "[ 57/300] train_loss: 0.09787 valid_loss: 0.10601 test_loss: 0.11904 \n",
      "[ 58/300] train_loss: 0.09591 valid_loss: 0.10485 test_loss: 0.11748 \n",
      "Validation loss decreased (0.105182 --> 0.104854).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09410 valid_loss: 0.10528 test_loss: 0.11895 \n",
      "[ 60/300] train_loss: 0.09517 valid_loss: 0.10796 test_loss: 0.11945 \n",
      "[ 61/300] train_loss: 0.09695 valid_loss: 0.10434 test_loss: 0.11637 \n",
      "Validation loss decreased (0.104854 --> 0.104342).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09470 valid_loss: 0.10246 test_loss: 0.11511 \n",
      "Validation loss decreased (0.104342 --> 0.102457).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09460 valid_loss: 0.10563 test_loss: 0.11746 \n",
      "[ 64/300] train_loss: 0.09577 valid_loss: 0.10109 test_loss: 0.11415 \n",
      "Validation loss decreased (0.102457 --> 0.101094).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09502 valid_loss: 0.09938 test_loss: 0.11223 \n",
      "Validation loss decreased (0.101094 --> 0.099376).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09419 valid_loss: 0.10058 test_loss: 0.11325 \n",
      "[ 67/300] train_loss: 0.09423 valid_loss: 0.09971 test_loss: 0.11318 \n",
      "[ 68/300] train_loss: 0.09256 valid_loss: 0.10129 test_loss: 0.11355 \n",
      "[ 69/300] train_loss: 0.09317 valid_loss: 0.09908 test_loss: 0.11234 \n",
      "Validation loss decreased (0.099376 --> 0.099083).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09173 valid_loss: 0.10154 test_loss: 0.11353 \n",
      "[ 71/300] train_loss: 0.09249 valid_loss: 0.09926 test_loss: 0.11082 \n",
      "[ 72/300] train_loss: 0.09239 valid_loss: 0.10126 test_loss: 0.11107 \n",
      "[ 73/300] train_loss: 0.09026 valid_loss: 0.09761 test_loss: 0.10993 \n",
      "Validation loss decreased (0.099083 --> 0.097608).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09172 valid_loss: 0.09867 test_loss: 0.10979 \n",
      "[ 75/300] train_loss: 0.08855 valid_loss: 0.09929 test_loss: 0.11038 \n",
      "[ 76/300] train_loss: 0.08954 valid_loss: 0.09789 test_loss: 0.10924 \n",
      "[ 77/300] train_loss: 0.08902 valid_loss: 0.09876 test_loss: 0.11098 \n",
      "[ 78/300] train_loss: 0.09047 valid_loss: 0.09850 test_loss: 0.10942 \n",
      "[ 79/300] train_loss: 0.08856 valid_loss: 0.09666 test_loss: 0.10799 \n",
      "Validation loss decreased (0.097608 --> 0.096664).  Saving model ...\n",
      "[ 80/300] train_loss: 0.08728 valid_loss: 0.09745 test_loss: 0.10908 \n",
      "[ 81/300] train_loss: 0.08693 valid_loss: 0.09649 test_loss: 0.10852 \n",
      "Validation loss decreased (0.096664 --> 0.096493).  Saving model ...\n",
      "[ 82/300] train_loss: 0.08737 valid_loss: 0.09717 test_loss: 0.10764 \n",
      "[ 83/300] train_loss: 0.08602 valid_loss: 0.09551 test_loss: 0.10662 \n",
      "Validation loss decreased (0.096493 --> 0.095506).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08545 valid_loss: 0.09520 test_loss: 0.10742 \n",
      "Validation loss decreased (0.095506 --> 0.095198).  Saving model ...\n",
      "[ 85/300] train_loss: 0.08791 valid_loss: 0.09453 test_loss: 0.10573 \n",
      "Validation loss decreased (0.095198 --> 0.094533).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08715 valid_loss: 0.09710 test_loss: 0.10846 \n",
      "[ 87/300] train_loss: 0.08412 valid_loss: 0.09348 test_loss: 0.10622 \n",
      "Validation loss decreased (0.094533 --> 0.093483).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08456 valid_loss: 0.09539 test_loss: 0.10836 \n",
      "[ 89/300] train_loss: 0.08611 valid_loss: 0.09408 test_loss: 0.10600 \n",
      "[ 90/300] train_loss: 0.08644 valid_loss: 0.09316 test_loss: 0.10501 \n",
      "Validation loss decreased (0.093483 --> 0.093161).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08801 valid_loss: 0.09305 test_loss: 0.10469 \n",
      "Validation loss decreased (0.093161 --> 0.093048).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08649 valid_loss: 0.09568 test_loss: 0.10704 \n",
      "[ 93/300] train_loss: 0.08755 valid_loss: 0.09392 test_loss: 0.10592 \n",
      "[ 94/300] train_loss: 0.08268 valid_loss: 0.09596 test_loss: 0.10673 \n",
      "[ 95/300] train_loss: 0.08532 valid_loss: 0.09314 test_loss: 0.10490 \n",
      "[ 96/300] train_loss: 0.08343 valid_loss: 0.09244 test_loss: 0.10394 \n",
      "Validation loss decreased (0.093048 --> 0.092437).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08497 valid_loss: 0.09300 test_loss: 0.10407 \n",
      "[ 98/300] train_loss: 0.08380 valid_loss: 0.09217 test_loss: 0.10448 \n",
      "Validation loss decreased (0.092437 --> 0.092166).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08444 valid_loss: 0.09090 test_loss: 0.10258 \n",
      "Validation loss decreased (0.092166 --> 0.090900).  Saving model ...\n",
      "[100/300] train_loss: 0.08643 valid_loss: 0.09316 test_loss: 0.10410 \n",
      "[101/300] train_loss: 0.08548 valid_loss: 0.09204 test_loss: 0.10333 \n",
      "[102/300] train_loss: 0.08507 valid_loss: 0.09309 test_loss: 0.10272 \n",
      "[103/300] train_loss: 0.08382 valid_loss: 0.09235 test_loss: 0.10223 \n",
      "[104/300] train_loss: 0.08397 valid_loss: 0.09225 test_loss: 0.10330 \n",
      "[105/300] train_loss: 0.08342 valid_loss: 0.09278 test_loss: 0.10173 \n",
      "[106/300] train_loss: 0.08355 valid_loss: 0.09370 test_loss: 0.10401 \n",
      "[107/300] train_loss: 0.08215 valid_loss: 0.09168 test_loss: 0.10204 \n",
      "[108/300] train_loss: 0.08281 valid_loss: 0.09173 test_loss: 0.10239 \n",
      "[109/300] train_loss: 0.08087 valid_loss: 0.09049 test_loss: 0.10242 \n",
      "Validation loss decreased (0.090900 --> 0.090487).  Saving model ...\n",
      "[110/300] train_loss: 0.07951 valid_loss: 0.09054 test_loss: 0.10197 \n",
      "[111/300] train_loss: 0.08076 valid_loss: 0.09128 test_loss: 0.10257 \n",
      "[112/300] train_loss: 0.08338 valid_loss: 0.08887 test_loss: 0.10073 \n",
      "Validation loss decreased (0.090487 --> 0.088870).  Saving model ...\n",
      "[113/300] train_loss: 0.08122 valid_loss: 0.08912 test_loss: 0.10082 \n",
      "[114/300] train_loss: 0.08288 valid_loss: 0.09002 test_loss: 0.10117 \n",
      "[115/300] train_loss: 0.08075 valid_loss: 0.09062 test_loss: 0.10208 \n",
      "[116/300] train_loss: 0.07881 valid_loss: 0.08871 test_loss: 0.10102 \n",
      "Validation loss decreased (0.088870 --> 0.088709).  Saving model ...\n",
      "[117/300] train_loss: 0.07875 valid_loss: 0.09066 test_loss: 0.10294 \n",
      "[118/300] train_loss: 0.08142 valid_loss: 0.08970 test_loss: 0.10071 \n",
      "[119/300] train_loss: 0.07943 valid_loss: 0.09121 test_loss: 0.10229 \n",
      "[120/300] train_loss: 0.08106 valid_loss: 0.08814 test_loss: 0.09937 \n",
      "Validation loss decreased (0.088709 --> 0.088145).  Saving model ...\n",
      "[121/300] train_loss: 0.07813 valid_loss: 0.09202 test_loss: 0.10239 \n",
      "[122/300] train_loss: 0.08069 valid_loss: 0.08833 test_loss: 0.09957 \n",
      "[123/300] train_loss: 0.07771 valid_loss: 0.08740 test_loss: 0.09942 \n",
      "Validation loss decreased (0.088145 --> 0.087397).  Saving model ...\n",
      "[124/300] train_loss: 0.07646 valid_loss: 0.08829 test_loss: 0.09919 \n",
      "[125/300] train_loss: 0.07880 valid_loss: 0.08755 test_loss: 0.09881 \n",
      "[126/300] train_loss: 0.07727 valid_loss: 0.08713 test_loss: 0.10007 \n",
      "Validation loss decreased (0.087397 --> 0.087126).  Saving model ...\n",
      "[127/300] train_loss: 0.07945 valid_loss: 0.08750 test_loss: 0.09947 \n",
      "[128/300] train_loss: 0.07999 valid_loss: 0.08927 test_loss: 0.09973 \n",
      "[129/300] train_loss: 0.07756 valid_loss: 0.08790 test_loss: 0.09937 \n",
      "[130/300] train_loss: 0.07813 valid_loss: 0.08787 test_loss: 0.09860 \n",
      "[131/300] train_loss: 0.07814 valid_loss: 0.08867 test_loss: 0.09939 \n",
      "[132/300] train_loss: 0.07648 valid_loss: 0.08783 test_loss: 0.09830 \n",
      "[133/300] train_loss: 0.07844 valid_loss: 0.08643 test_loss: 0.09869 \n",
      "Validation loss decreased (0.087126 --> 0.086426).  Saving model ...\n",
      "[134/300] train_loss: 0.07820 valid_loss: 0.08916 test_loss: 0.09870 \n",
      "[135/300] train_loss: 0.07821 valid_loss: 0.08625 test_loss: 0.09707 \n",
      "Validation loss decreased (0.086426 --> 0.086254).  Saving model ...\n",
      "[136/300] train_loss: 0.07445 valid_loss: 0.08693 test_loss: 0.09887 \n",
      "[137/300] train_loss: 0.07596 valid_loss: 0.08742 test_loss: 0.09801 \n",
      "[138/300] train_loss: 0.07592 valid_loss: 0.08486 test_loss: 0.09664 \n",
      "Validation loss decreased (0.086254 --> 0.084856).  Saving model ...\n",
      "[139/300] train_loss: 0.07543 valid_loss: 0.08595 test_loss: 0.09666 \n",
      "[140/300] train_loss: 0.07459 valid_loss: 0.08506 test_loss: 0.09693 \n",
      "[141/300] train_loss: 0.07612 valid_loss: 0.08688 test_loss: 0.09738 \n",
      "[142/300] train_loss: 0.07475 valid_loss: 0.08695 test_loss: 0.09851 \n",
      "[143/300] train_loss: 0.07422 valid_loss: 0.08624 test_loss: 0.09734 \n",
      "[144/300] train_loss: 0.07428 valid_loss: 0.08574 test_loss: 0.09655 \n",
      "[145/300] train_loss: 0.07467 valid_loss: 0.08695 test_loss: 0.09643 \n",
      "[146/300] train_loss: 0.07716 valid_loss: 0.08709 test_loss: 0.09705 \n",
      "[147/300] train_loss: 0.07481 valid_loss: 0.08523 test_loss: 0.09602 \n",
      "[148/300] train_loss: 0.07231 valid_loss: 0.08757 test_loss: 0.09768 \n",
      "[149/300] train_loss: 0.07633 valid_loss: 0.08517 test_loss: 0.09650 \n",
      "[150/300] train_loss: 0.07438 valid_loss: 0.08530 test_loss: 0.09604 \n",
      "[151/300] train_loss: 0.07429 valid_loss: 0.08584 test_loss: 0.09737 \n",
      "[152/300] train_loss: 0.07472 valid_loss: 0.08372 test_loss: 0.09632 \n",
      "Validation loss decreased (0.084856 --> 0.083722).  Saving model ...\n",
      "[153/300] train_loss: 0.07299 valid_loss: 0.08471 test_loss: 0.09521 \n",
      "[154/300] train_loss: 0.07540 valid_loss: 0.08457 test_loss: 0.09527 \n",
      "[155/300] train_loss: 0.07418 valid_loss: 0.08547 test_loss: 0.09624 \n",
      "[156/300] train_loss: 0.07431 valid_loss: 0.08512 test_loss: 0.09531 \n",
      "[157/300] train_loss: 0.07527 valid_loss: 0.08492 test_loss: 0.09519 \n",
      "[158/300] train_loss: 0.07162 valid_loss: 0.08528 test_loss: 0.09614 \n",
      "[159/300] train_loss: 0.07314 valid_loss: 0.08261 test_loss: 0.09398 \n",
      "Validation loss decreased (0.083722 --> 0.082610).  Saving model ...\n",
      "[160/300] train_loss: 0.07380 valid_loss: 0.08295 test_loss: 0.09447 \n",
      "[161/300] train_loss: 0.07048 valid_loss: 0.08412 test_loss: 0.09483 \n",
      "[162/300] train_loss: 0.07139 valid_loss: 0.08451 test_loss: 0.09373 \n",
      "[163/300] train_loss: 0.07377 valid_loss: 0.08312 test_loss: 0.09496 \n",
      "[164/300] train_loss: 0.07309 valid_loss: 0.08296 test_loss: 0.09519 \n",
      "[165/300] train_loss: 0.07290 valid_loss: 0.08374 test_loss: 0.09425 \n",
      "[166/300] train_loss: 0.07398 valid_loss: 0.08352 test_loss: 0.09436 \n",
      "[167/300] train_loss: 0.07193 valid_loss: 0.08334 test_loss: 0.09308 \n",
      "[168/300] train_loss: 0.07164 valid_loss: 0.08486 test_loss: 0.09503 \n",
      "[169/300] train_loss: 0.07397 valid_loss: 0.08524 test_loss: 0.09543 \n",
      "[170/300] train_loss: 0.07234 valid_loss: 0.08377 test_loss: 0.09349 \n",
      "[171/300] train_loss: 0.07294 valid_loss: 0.08331 test_loss: 0.09432 \n",
      "[172/300] train_loss: 0.07386 valid_loss: 0.08234 test_loss: 0.09410 \n",
      "Validation loss decreased (0.082610 --> 0.082344).  Saving model ...\n",
      "[173/300] train_loss: 0.07119 valid_loss: 0.08299 test_loss: 0.09448 \n",
      "[174/300] train_loss: 0.07184 valid_loss: 0.08292 test_loss: 0.09464 \n",
      "[175/300] train_loss: 0.07166 valid_loss: 0.08287 test_loss: 0.09251 \n",
      "[176/300] train_loss: 0.06982 valid_loss: 0.08225 test_loss: 0.09293 \n",
      "Validation loss decreased (0.082344 --> 0.082253).  Saving model ...\n",
      "[177/300] train_loss: 0.06970 valid_loss: 0.08199 test_loss: 0.09340 \n",
      "Validation loss decreased (0.082253 --> 0.081992).  Saving model ...\n",
      "[178/300] train_loss: 0.07286 valid_loss: 0.08331 test_loss: 0.09384 \n",
      "[179/300] train_loss: 0.07033 valid_loss: 0.08403 test_loss: 0.09517 \n",
      "[180/300] train_loss: 0.07207 valid_loss: 0.08357 test_loss: 0.09417 \n",
      "[181/300] train_loss: 0.07206 valid_loss: 0.08501 test_loss: 0.09447 \n",
      "[182/300] train_loss: 0.06848 valid_loss: 0.08372 test_loss: 0.09558 \n",
      "[183/300] train_loss: 0.07139 valid_loss: 0.08173 test_loss: 0.09236 \n",
      "Validation loss decreased (0.081992 --> 0.081733).  Saving model ...\n",
      "[184/300] train_loss: 0.06930 valid_loss: 0.08328 test_loss: 0.09394 \n",
      "[185/300] train_loss: 0.07129 valid_loss: 0.08253 test_loss: 0.09393 \n",
      "[186/300] train_loss: 0.07017 valid_loss: 0.08280 test_loss: 0.09355 \n",
      "[187/300] train_loss: 0.06940 valid_loss: 0.08162 test_loss: 0.09195 \n",
      "Validation loss decreased (0.081733 --> 0.081617).  Saving model ...\n",
      "[188/300] train_loss: 0.06841 valid_loss: 0.08246 test_loss: 0.09346 \n",
      "[189/300] train_loss: 0.06693 valid_loss: 0.08109 test_loss: 0.09327 \n",
      "Validation loss decreased (0.081617 --> 0.081090).  Saving model ...\n",
      "[190/300] train_loss: 0.07098 valid_loss: 0.08148 test_loss: 0.09365 \n",
      "[191/300] train_loss: 0.06945 valid_loss: 0.08132 test_loss: 0.09332 \n",
      "[192/300] train_loss: 0.06800 valid_loss: 0.08156 test_loss: 0.09165 \n",
      "[193/300] train_loss: 0.06873 valid_loss: 0.08069 test_loss: 0.09234 \n",
      "Validation loss decreased (0.081090 --> 0.080687).  Saving model ...\n",
      "[194/300] train_loss: 0.06864 valid_loss: 0.08223 test_loss: 0.09344 \n",
      "[195/300] train_loss: 0.06718 valid_loss: 0.08168 test_loss: 0.09270 \n",
      "[196/300] train_loss: 0.06969 valid_loss: 0.08163 test_loss: 0.09202 \n",
      "[197/300] train_loss: 0.06832 valid_loss: 0.08121 test_loss: 0.09200 \n",
      "[198/300] train_loss: 0.06572 valid_loss: 0.08006 test_loss: 0.09145 \n",
      "Validation loss decreased (0.080687 --> 0.080059).  Saving model ...\n",
      "[199/300] train_loss: 0.06770 valid_loss: 0.08255 test_loss: 0.09322 \n",
      "[200/300] train_loss: 0.06730 valid_loss: 0.08039 test_loss: 0.09124 \n",
      "[201/300] train_loss: 0.06823 valid_loss: 0.08209 test_loss: 0.09252 \n",
      "[202/300] train_loss: 0.06706 valid_loss: 0.08274 test_loss: 0.09347 \n",
      "[203/300] train_loss: 0.06914 valid_loss: 0.07979 test_loss: 0.09169 \n",
      "Validation loss decreased (0.080059 --> 0.079790).  Saving model ...\n",
      "[204/300] train_loss: 0.07056 valid_loss: 0.08047 test_loss: 0.09337 \n",
      "[205/300] train_loss: 0.06674 valid_loss: 0.08110 test_loss: 0.09240 \n",
      "[206/300] train_loss: 0.06976 valid_loss: 0.07923 test_loss: 0.09122 \n",
      "Validation loss decreased (0.079790 --> 0.079227).  Saving model ...\n",
      "[207/300] train_loss: 0.06821 valid_loss: 0.08079 test_loss: 0.09131 \n",
      "[208/300] train_loss: 0.06771 valid_loss: 0.08011 test_loss: 0.09117 \n",
      "[209/300] train_loss: 0.06891 valid_loss: 0.08022 test_loss: 0.09253 \n",
      "[210/300] train_loss: 0.06628 valid_loss: 0.08198 test_loss: 0.09188 \n",
      "[211/300] train_loss: 0.06797 valid_loss: 0.07881 test_loss: 0.09105 \n",
      "Validation loss decreased (0.079227 --> 0.078815).  Saving model ...\n",
      "[212/300] train_loss: 0.06790 valid_loss: 0.08133 test_loss: 0.09241 \n",
      "[213/300] train_loss: 0.06682 valid_loss: 0.08010 test_loss: 0.09122 \n",
      "[214/300] train_loss: 0.06643 valid_loss: 0.07814 test_loss: 0.08958 \n",
      "Validation loss decreased (0.078815 --> 0.078142).  Saving model ...\n",
      "[215/300] train_loss: 0.06773 valid_loss: 0.07955 test_loss: 0.09087 \n",
      "[216/300] train_loss: 0.06655 valid_loss: 0.07861 test_loss: 0.08980 \n",
      "[217/300] train_loss: 0.06515 valid_loss: 0.08017 test_loss: 0.09235 \n",
      "[218/300] train_loss: 0.06732 valid_loss: 0.07858 test_loss: 0.09089 \n",
      "[219/300] train_loss: 0.06828 valid_loss: 0.07895 test_loss: 0.09108 \n",
      "[220/300] train_loss: 0.06772 valid_loss: 0.07944 test_loss: 0.09139 \n",
      "[221/300] train_loss: 0.06675 valid_loss: 0.07999 test_loss: 0.09331 \n",
      "[222/300] train_loss: 0.06658 valid_loss: 0.07936 test_loss: 0.09125 \n",
      "[223/300] train_loss: 0.06654 valid_loss: 0.07965 test_loss: 0.09053 \n",
      "[224/300] train_loss: 0.06511 valid_loss: 0.07787 test_loss: 0.09083 \n",
      "Validation loss decreased (0.078142 --> 0.077875).  Saving model ...\n",
      "[225/300] train_loss: 0.06576 valid_loss: 0.07964 test_loss: 0.09130 \n",
      "[226/300] train_loss: 0.06653 valid_loss: 0.07747 test_loss: 0.08936 \n",
      "Validation loss decreased (0.077875 --> 0.077469).  Saving model ...\n",
      "[227/300] train_loss: 0.06437 valid_loss: 0.07796 test_loss: 0.09016 \n",
      "[228/300] train_loss: 0.06727 valid_loss: 0.07855 test_loss: 0.08980 \n",
      "[229/300] train_loss: 0.06487 valid_loss: 0.08033 test_loss: 0.09048 \n",
      "[230/300] train_loss: 0.06609 valid_loss: 0.07993 test_loss: 0.09025 \n",
      "[231/300] train_loss: 0.06656 valid_loss: 0.07889 test_loss: 0.08969 \n",
      "[232/300] train_loss: 0.06623 valid_loss: 0.07874 test_loss: 0.09070 \n",
      "[233/300] train_loss: 0.06709 valid_loss: 0.07710 test_loss: 0.08896 \n",
      "Validation loss decreased (0.077469 --> 0.077100).  Saving model ...\n",
      "[234/300] train_loss: 0.06634 valid_loss: 0.07740 test_loss: 0.09077 \n",
      "[235/300] train_loss: 0.06657 valid_loss: 0.07793 test_loss: 0.09046 \n",
      "[236/300] train_loss: 0.06596 valid_loss: 0.07955 test_loss: 0.09041 \n",
      "[237/300] train_loss: 0.06421 valid_loss: 0.07807 test_loss: 0.09111 \n",
      "[238/300] train_loss: 0.06578 valid_loss: 0.07811 test_loss: 0.09016 \n",
      "[239/300] train_loss: 0.06576 valid_loss: 0.07972 test_loss: 0.09206 \n",
      "[240/300] train_loss: 0.06613 valid_loss: 0.07884 test_loss: 0.08910 \n",
      "[241/300] train_loss: 0.06493 valid_loss: 0.07828 test_loss: 0.08881 \n",
      "[242/300] train_loss: 0.06618 valid_loss: 0.07783 test_loss: 0.08923 \n",
      "[243/300] train_loss: 0.06520 valid_loss: 0.07869 test_loss: 0.08970 \n",
      "[244/300] train_loss: 0.06676 valid_loss: 0.07832 test_loss: 0.08872 \n",
      "[245/300] train_loss: 0.06463 valid_loss: 0.07799 test_loss: 0.09048 \n",
      "[246/300] train_loss: 0.06179 valid_loss: 0.07947 test_loss: 0.09076 \n",
      "[247/300] train_loss: 0.06560 valid_loss: 0.07865 test_loss: 0.09125 \n",
      "[248/300] train_loss: 0.06539 valid_loss: 0.07751 test_loss: 0.08952 \n",
      "[249/300] train_loss: 0.06429 valid_loss: 0.07750 test_loss: 0.09031 \n",
      "[250/300] train_loss: 0.06451 valid_loss: 0.07743 test_loss: 0.08909 \n",
      "[251/300] train_loss: 0.06474 valid_loss: 0.07672 test_loss: 0.08863 \n",
      "Validation loss decreased (0.077100 --> 0.076721).  Saving model ...\n",
      "[252/300] train_loss: 0.06560 valid_loss: 0.07807 test_loss: 0.08974 \n",
      "[253/300] train_loss: 0.06419 valid_loss: 0.07725 test_loss: 0.08896 \n",
      "[254/300] train_loss: 0.06539 valid_loss: 0.07893 test_loss: 0.09150 \n",
      "[255/300] train_loss: 0.06543 valid_loss: 0.07607 test_loss: 0.09047 \n",
      "Validation loss decreased (0.076721 --> 0.076074).  Saving model ...\n",
      "[256/300] train_loss: 0.06370 valid_loss: 0.07776 test_loss: 0.09047 \n",
      "[257/300] train_loss: 0.06451 valid_loss: 0.07709 test_loss: 0.08929 \n",
      "[258/300] train_loss: 0.06295 valid_loss: 0.07709 test_loss: 0.09077 \n",
      "[259/300] train_loss: 0.06303 valid_loss: 0.07930 test_loss: 0.08953 \n",
      "[260/300] train_loss: 0.06280 valid_loss: 0.07841 test_loss: 0.09007 \n",
      "[261/300] train_loss: 0.06288 valid_loss: 0.07773 test_loss: 0.08864 \n",
      "[262/300] train_loss: 0.06560 valid_loss: 0.07718 test_loss: 0.08823 \n",
      "[263/300] train_loss: 0.06253 valid_loss: 0.07766 test_loss: 0.09107 \n",
      "[264/300] train_loss: 0.06322 valid_loss: 0.07633 test_loss: 0.08911 \n",
      "[265/300] train_loss: 0.06185 valid_loss: 0.07753 test_loss: 0.08905 \n",
      "[266/300] train_loss: 0.06569 valid_loss: 0.07806 test_loss: 0.08851 \n",
      "[267/300] train_loss: 0.06181 valid_loss: 0.07761 test_loss: 0.08933 \n",
      "[268/300] train_loss: 0.06668 valid_loss: 0.07664 test_loss: 0.08818 \n",
      "[269/300] train_loss: 0.06337 valid_loss: 0.07802 test_loss: 0.08957 \n",
      "[270/300] train_loss: 0.06365 valid_loss: 0.07727 test_loss: 0.08871 \n",
      "[271/300] train_loss: 0.06202 valid_loss: 0.07691 test_loss: 0.08917 \n",
      "[272/300] train_loss: 0.06215 valid_loss: 0.07534 test_loss: 0.08894 \n",
      "Validation loss decreased (0.076074 --> 0.075342).  Saving model ...\n",
      "[273/300] train_loss: 0.06368 valid_loss: 0.07657 test_loss: 0.08897 \n",
      "[274/300] train_loss: 0.06303 valid_loss: 0.07545 test_loss: 0.08776 \n",
      "[275/300] train_loss: 0.06281 valid_loss: 0.07587 test_loss: 0.08864 \n",
      "[276/300] train_loss: 0.06582 valid_loss: 0.07649 test_loss: 0.08755 \n",
      "[277/300] train_loss: 0.06287 valid_loss: 0.07562 test_loss: 0.08746 \n",
      "[278/300] train_loss: 0.06097 valid_loss: 0.07537 test_loss: 0.08828 \n",
      "[279/300] train_loss: 0.06362 valid_loss: 0.07517 test_loss: 0.08808 \n",
      "Validation loss decreased (0.075342 --> 0.075168).  Saving model ...\n",
      "[280/300] train_loss: 0.06219 valid_loss: 0.07556 test_loss: 0.08812 \n",
      "[281/300] train_loss: 0.06270 valid_loss: 0.07552 test_loss: 0.08666 \n",
      "[282/300] train_loss: 0.06167 valid_loss: 0.07523 test_loss: 0.08790 \n",
      "[283/300] train_loss: 0.06246 valid_loss: 0.07733 test_loss: 0.08865 \n",
      "[284/300] train_loss: 0.06163 valid_loss: 0.07689 test_loss: 0.08968 \n",
      "[285/300] train_loss: 0.06373 valid_loss: 0.07663 test_loss: 0.08897 \n",
      "[286/300] train_loss: 0.06214 valid_loss: 0.07608 test_loss: 0.08753 \n",
      "[287/300] train_loss: 0.06343 valid_loss: 0.07578 test_loss: 0.08763 \n",
      "[288/300] train_loss: 0.06326 valid_loss: 0.07549 test_loss: 0.08749 \n",
      "[289/300] train_loss: 0.06140 valid_loss: 0.07620 test_loss: 0.08700 \n",
      "[290/300] train_loss: 0.06090 valid_loss: 0.07814 test_loss: 0.08953 \n",
      "[291/300] train_loss: 0.05962 valid_loss: 0.07610 test_loss: 0.08786 \n",
      "[292/300] train_loss: 0.06326 valid_loss: 0.07657 test_loss: 0.08793 \n",
      "[293/300] train_loss: 0.06109 valid_loss: 0.07695 test_loss: 0.08844 \n",
      "[294/300] train_loss: 0.06162 valid_loss: 0.07514 test_loss: 0.08700 \n",
      "Validation loss decreased (0.075168 --> 0.075138).  Saving model ...\n",
      "[295/300] train_loss: 0.06206 valid_loss: 0.07501 test_loss: 0.08652 \n",
      "Validation loss decreased (0.075138 --> 0.075014).  Saving model ...\n",
      "[296/300] train_loss: 0.06298 valid_loss: 0.07682 test_loss: 0.08811 \n",
      "[297/300] train_loss: 0.06183 valid_loss: 0.07617 test_loss: 0.08678 \n",
      "[298/300] train_loss: 0.06211 valid_loss: 0.07593 test_loss: 0.08760 \n",
      "[299/300] train_loss: 0.06068 valid_loss: 0.07614 test_loss: 0.08750 \n",
      "[300/300] train_loss: 0.05994 valid_loss: 0.07699 test_loss: 0.08735 \n",
      "TRAINING MODEL 10\n",
      "[  1/300] train_loss: 0.65074 valid_loss: 0.56328 test_loss: 0.56458 \n",
      "Validation loss decreased (inf --> 0.563277).  Saving model ...\n",
      "[  2/300] train_loss: 0.46119 valid_loss: 0.41379 test_loss: 0.41683 \n",
      "Validation loss decreased (0.563277 --> 0.413792).  Saving model ...\n",
      "[  3/300] train_loss: 0.34584 valid_loss: 0.33543 test_loss: 0.34342 \n",
      "Validation loss decreased (0.413792 --> 0.335427).  Saving model ...\n",
      "[  4/300] train_loss: 0.28146 valid_loss: 0.28319 test_loss: 0.29521 \n",
      "Validation loss decreased (0.335427 --> 0.283191).  Saving model ...\n",
      "[  5/300] train_loss: 0.24263 valid_loss: 0.24927 test_loss: 0.26603 \n",
      "Validation loss decreased (0.283191 --> 0.249271).  Saving model ...\n",
      "[  6/300] train_loss: 0.22338 valid_loss: 0.22233 test_loss: 0.23769 \n",
      "Validation loss decreased (0.249271 --> 0.222326).  Saving model ...\n",
      "[  7/300] train_loss: 0.20478 valid_loss: 0.20935 test_loss: 0.22595 \n",
      "Validation loss decreased (0.222326 --> 0.209347).  Saving model ...\n",
      "[  8/300] train_loss: 0.19485 valid_loss: 0.19835 test_loss: 0.21467 \n",
      "Validation loss decreased (0.209347 --> 0.198348).  Saving model ...\n",
      "[  9/300] train_loss: 0.18457 valid_loss: 0.18438 test_loss: 0.20012 \n",
      "Validation loss decreased (0.198348 --> 0.184377).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17781 valid_loss: 0.17950 test_loss: 0.19651 \n",
      "Validation loss decreased (0.184377 --> 0.179497).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16930 valid_loss: 0.17490 test_loss: 0.18994 \n",
      "Validation loss decreased (0.179497 --> 0.174903).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16392 valid_loss: 0.16574 test_loss: 0.17974 \n",
      "Validation loss decreased (0.174903 --> 0.165738).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15670 valid_loss: 0.16435 test_loss: 0.17965 \n",
      "Validation loss decreased (0.165738 --> 0.164348).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15740 valid_loss: 0.16024 test_loss: 0.17109 \n",
      "Validation loss decreased (0.164348 --> 0.160237).  Saving model ...\n",
      "[ 15/300] train_loss: 0.15543 valid_loss: 0.15746 test_loss: 0.16935 \n",
      "Validation loss decreased (0.160237 --> 0.157462).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14840 valid_loss: 0.15202 test_loss: 0.16215 \n",
      "Validation loss decreased (0.157462 --> 0.152017).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14441 valid_loss: 0.14666 test_loss: 0.15712 \n",
      "Validation loss decreased (0.152017 --> 0.146662).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14156 valid_loss: 0.14653 test_loss: 0.15611 \n",
      "Validation loss decreased (0.146662 --> 0.146528).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13940 valid_loss: 0.14356 test_loss: 0.15673 \n",
      "Validation loss decreased (0.146528 --> 0.143564).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13656 valid_loss: 0.13857 test_loss: 0.14974 \n",
      "Validation loss decreased (0.143564 --> 0.138566).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13532 valid_loss: 0.13993 test_loss: 0.15113 \n",
      "[ 22/300] train_loss: 0.13016 valid_loss: 0.13886 test_loss: 0.14855 \n",
      "[ 23/300] train_loss: 0.12974 valid_loss: 0.13502 test_loss: 0.14529 \n",
      "Validation loss decreased (0.138566 --> 0.135022).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12781 valid_loss: 0.13537 test_loss: 0.14667 \n",
      "[ 25/300] train_loss: 0.12837 valid_loss: 0.13321 test_loss: 0.14559 \n",
      "Validation loss decreased (0.135022 --> 0.133212).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12611 valid_loss: 0.12899 test_loss: 0.14152 \n",
      "Validation loss decreased (0.133212 --> 0.128993).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12419 valid_loss: 0.12709 test_loss: 0.13988 \n",
      "Validation loss decreased (0.128993 --> 0.127094).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12030 valid_loss: 0.12740 test_loss: 0.13881 \n",
      "[ 29/300] train_loss: 0.12377 valid_loss: 0.12804 test_loss: 0.14094 \n",
      "[ 30/300] train_loss: 0.11936 valid_loss: 0.12430 test_loss: 0.13666 \n",
      "Validation loss decreased (0.127094 --> 0.124303).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11865 valid_loss: 0.12614 test_loss: 0.13791 \n",
      "[ 32/300] train_loss: 0.11580 valid_loss: 0.12109 test_loss: 0.13481 \n",
      "Validation loss decreased (0.124303 --> 0.121089).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11440 valid_loss: 0.12042 test_loss: 0.13335 \n",
      "Validation loss decreased (0.121089 --> 0.120420).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11656 valid_loss: 0.11940 test_loss: 0.13122 \n",
      "Validation loss decreased (0.120420 --> 0.119402).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11649 valid_loss: 0.12003 test_loss: 0.13142 \n",
      "[ 36/300] train_loss: 0.11361 valid_loss: 0.12085 test_loss: 0.13284 \n",
      "[ 37/300] train_loss: 0.10722 valid_loss: 0.11992 test_loss: 0.13252 \n",
      "[ 38/300] train_loss: 0.11445 valid_loss: 0.11621 test_loss: 0.12894 \n",
      "Validation loss decreased (0.119402 --> 0.116213).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11076 valid_loss: 0.11955 test_loss: 0.13015 \n",
      "[ 40/300] train_loss: 0.11183 valid_loss: 0.11432 test_loss: 0.12746 \n",
      "Validation loss decreased (0.116213 --> 0.114322).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10899 valid_loss: 0.11708 test_loss: 0.12917 \n",
      "[ 42/300] train_loss: 0.10737 valid_loss: 0.11498 test_loss: 0.12679 \n",
      "[ 43/300] train_loss: 0.11111 valid_loss: 0.11388 test_loss: 0.12778 \n",
      "Validation loss decreased (0.114322 --> 0.113876).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10591 valid_loss: 0.11254 test_loss: 0.12654 \n",
      "Validation loss decreased (0.113876 --> 0.112545).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10846 valid_loss: 0.11118 test_loss: 0.12446 \n",
      "Validation loss decreased (0.112545 --> 0.111179).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10452 valid_loss: 0.11013 test_loss: 0.12501 \n",
      "Validation loss decreased (0.111179 --> 0.110128).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10183 valid_loss: 0.11113 test_loss: 0.12419 \n",
      "[ 48/300] train_loss: 0.10514 valid_loss: 0.11081 test_loss: 0.12407 \n",
      "[ 49/300] train_loss: 0.10237 valid_loss: 0.10815 test_loss: 0.12309 \n",
      "Validation loss decreased (0.110128 --> 0.108150).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10250 valid_loss: 0.10866 test_loss: 0.12274 \n",
      "[ 51/300] train_loss: 0.10314 valid_loss: 0.10716 test_loss: 0.12168 \n",
      "Validation loss decreased (0.108150 --> 0.107165).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10147 valid_loss: 0.10994 test_loss: 0.12251 \n",
      "[ 53/300] train_loss: 0.10029 valid_loss: 0.10835 test_loss: 0.12128 \n",
      "[ 54/300] train_loss: 0.10068 valid_loss: 0.10668 test_loss: 0.11941 \n",
      "Validation loss decreased (0.107165 --> 0.106681).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09964 valid_loss: 0.10597 test_loss: 0.12004 \n",
      "Validation loss decreased (0.106681 --> 0.105968).  Saving model ...\n",
      "[ 56/300] train_loss: 0.10349 valid_loss: 0.10706 test_loss: 0.11980 \n",
      "[ 57/300] train_loss: 0.10068 valid_loss: 0.10551 test_loss: 0.11833 \n",
      "Validation loss decreased (0.105968 --> 0.105507).  Saving model ...\n",
      "[ 58/300] train_loss: 0.09758 valid_loss: 0.10767 test_loss: 0.12001 \n",
      "[ 59/300] train_loss: 0.09587 valid_loss: 0.10749 test_loss: 0.11764 \n",
      "[ 60/300] train_loss: 0.09904 valid_loss: 0.10377 test_loss: 0.11549 \n",
      "Validation loss decreased (0.105507 --> 0.103770).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09721 valid_loss: 0.10343 test_loss: 0.11702 \n",
      "Validation loss decreased (0.103770 --> 0.103426).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09883 valid_loss: 0.10267 test_loss: 0.11571 \n",
      "Validation loss decreased (0.103426 --> 0.102667).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09624 valid_loss: 0.10416 test_loss: 0.11523 \n",
      "[ 64/300] train_loss: 0.09656 valid_loss: 0.10321 test_loss: 0.11596 \n",
      "[ 65/300] train_loss: 0.09685 valid_loss: 0.10176 test_loss: 0.11411 \n",
      "Validation loss decreased (0.102667 --> 0.101755).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09603 valid_loss: 0.10373 test_loss: 0.11547 \n",
      "[ 67/300] train_loss: 0.09457 valid_loss: 0.10087 test_loss: 0.11496 \n",
      "Validation loss decreased (0.101755 --> 0.100867).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09513 valid_loss: 0.10349 test_loss: 0.11495 \n",
      "[ 69/300] train_loss: 0.09704 valid_loss: 0.10328 test_loss: 0.11471 \n",
      "[ 70/300] train_loss: 0.09405 valid_loss: 0.10178 test_loss: 0.11343 \n",
      "[ 71/300] train_loss: 0.09324 valid_loss: 0.10217 test_loss: 0.11352 \n",
      "[ 72/300] train_loss: 0.09369 valid_loss: 0.10010 test_loss: 0.11185 \n",
      "Validation loss decreased (0.100867 --> 0.100104).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09262 valid_loss: 0.09908 test_loss: 0.11274 \n",
      "Validation loss decreased (0.100104 --> 0.099076).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09210 valid_loss: 0.09971 test_loss: 0.11301 \n",
      "[ 75/300] train_loss: 0.09469 valid_loss: 0.09902 test_loss: 0.11113 \n",
      "Validation loss decreased (0.099076 --> 0.099020).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09027 valid_loss: 0.09768 test_loss: 0.11019 \n",
      "Validation loss decreased (0.099020 --> 0.097679).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09532 valid_loss: 0.09910 test_loss: 0.11112 \n",
      "[ 78/300] train_loss: 0.09068 valid_loss: 0.09892 test_loss: 0.11249 \n",
      "[ 79/300] train_loss: 0.09016 valid_loss: 0.09635 test_loss: 0.10921 \n",
      "Validation loss decreased (0.097679 --> 0.096348).  Saving model ...\n",
      "[ 80/300] train_loss: 0.09340 valid_loss: 0.09758 test_loss: 0.10965 \n",
      "[ 81/300] train_loss: 0.09145 valid_loss: 0.09696 test_loss: 0.11053 \n",
      "[ 82/300] train_loss: 0.08882 valid_loss: 0.09700 test_loss: 0.10930 \n",
      "[ 83/300] train_loss: 0.09007 valid_loss: 0.09527 test_loss: 0.10806 \n",
      "Validation loss decreased (0.096348 --> 0.095273).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08855 valid_loss: 0.09493 test_loss: 0.10786 \n",
      "Validation loss decreased (0.095273 --> 0.094930).  Saving model ...\n",
      "[ 85/300] train_loss: 0.08876 valid_loss: 0.09607 test_loss: 0.10897 \n",
      "[ 86/300] train_loss: 0.08953 valid_loss: 0.09742 test_loss: 0.10828 \n",
      "[ 87/300] train_loss: 0.08824 valid_loss: 0.09584 test_loss: 0.10710 \n",
      "[ 88/300] train_loss: 0.08745 valid_loss: 0.09714 test_loss: 0.10832 \n",
      "[ 89/300] train_loss: 0.08901 valid_loss: 0.09505 test_loss: 0.10646 \n",
      "[ 90/300] train_loss: 0.08835 valid_loss: 0.09293 test_loss: 0.10689 \n",
      "Validation loss decreased (0.094930 --> 0.092928).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08898 valid_loss: 0.09559 test_loss: 0.10592 \n",
      "[ 92/300] train_loss: 0.08626 valid_loss: 0.09471 test_loss: 0.10591 \n",
      "[ 93/300] train_loss: 0.08531 valid_loss: 0.09776 test_loss: 0.10736 \n",
      "[ 94/300] train_loss: 0.08518 valid_loss: 0.09509 test_loss: 0.10703 \n",
      "[ 95/300] train_loss: 0.08712 valid_loss: 0.09404 test_loss: 0.10599 \n",
      "[ 96/300] train_loss: 0.08590 valid_loss: 0.09207 test_loss: 0.10489 \n",
      "Validation loss decreased (0.092928 --> 0.092068).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08549 valid_loss: 0.09659 test_loss: 0.10530 \n",
      "[ 98/300] train_loss: 0.08329 valid_loss: 0.09421 test_loss: 0.10617 \n",
      "[ 99/300] train_loss: 0.08415 valid_loss: 0.09386 test_loss: 0.10578 \n",
      "[100/300] train_loss: 0.08602 valid_loss: 0.09320 test_loss: 0.10316 \n",
      "[101/300] train_loss: 0.08685 valid_loss: 0.09221 test_loss: 0.10325 \n",
      "[102/300] train_loss: 0.08762 valid_loss: 0.09411 test_loss: 0.10516 \n",
      "[103/300] train_loss: 0.08415 valid_loss: 0.09188 test_loss: 0.10336 \n",
      "Validation loss decreased (0.092068 --> 0.091882).  Saving model ...\n",
      "[104/300] train_loss: 0.08291 valid_loss: 0.09300 test_loss: 0.10365 \n",
      "[105/300] train_loss: 0.08525 valid_loss: 0.09339 test_loss: 0.10433 \n",
      "[106/300] train_loss: 0.08485 valid_loss: 0.09357 test_loss: 0.10414 \n",
      "[107/300] train_loss: 0.08337 valid_loss: 0.09004 test_loss: 0.10267 \n",
      "Validation loss decreased (0.091882 --> 0.090043).  Saving model ...\n",
      "[108/300] train_loss: 0.08575 valid_loss: 0.09237 test_loss: 0.10300 \n",
      "[109/300] train_loss: 0.08320 valid_loss: 0.09208 test_loss: 0.10203 \n",
      "[110/300] train_loss: 0.08294 valid_loss: 0.09065 test_loss: 0.10139 \n",
      "[111/300] train_loss: 0.08039 valid_loss: 0.08950 test_loss: 0.10117 \n",
      "Validation loss decreased (0.090043 --> 0.089503).  Saving model ...\n",
      "[112/300] train_loss: 0.08263 valid_loss: 0.09002 test_loss: 0.10003 \n",
      "[113/300] train_loss: 0.08198 valid_loss: 0.09008 test_loss: 0.10054 \n",
      "[114/300] train_loss: 0.08275 valid_loss: 0.08985 test_loss: 0.10112 \n",
      "[115/300] train_loss: 0.08108 valid_loss: 0.09003 test_loss: 0.10111 \n",
      "[116/300] train_loss: 0.08299 valid_loss: 0.09136 test_loss: 0.10257 \n",
      "[117/300] train_loss: 0.08147 valid_loss: 0.08999 test_loss: 0.10087 \n",
      "[118/300] train_loss: 0.08180 valid_loss: 0.08940 test_loss: 0.09937 \n",
      "Validation loss decreased (0.089503 --> 0.089400).  Saving model ...\n",
      "[119/300] train_loss: 0.08066 valid_loss: 0.08822 test_loss: 0.09970 \n",
      "Validation loss decreased (0.089400 --> 0.088220).  Saving model ...\n",
      "[120/300] train_loss: 0.08119 valid_loss: 0.09130 test_loss: 0.10054 \n",
      "[121/300] train_loss: 0.08339 valid_loss: 0.09034 test_loss: 0.09989 \n",
      "[122/300] train_loss: 0.08120 valid_loss: 0.08818 test_loss: 0.09963 \n",
      "Validation loss decreased (0.088220 --> 0.088182).  Saving model ...\n",
      "[123/300] train_loss: 0.07924 valid_loss: 0.08926 test_loss: 0.09773 \n",
      "[124/300] train_loss: 0.07997 valid_loss: 0.08826 test_loss: 0.09952 \n",
      "[125/300] train_loss: 0.07949 valid_loss: 0.08861 test_loss: 0.09913 \n",
      "[126/300] train_loss: 0.07978 valid_loss: 0.08961 test_loss: 0.09913 \n",
      "[127/300] train_loss: 0.07928 valid_loss: 0.08683 test_loss: 0.09716 \n",
      "Validation loss decreased (0.088182 --> 0.086832).  Saving model ...\n",
      "[128/300] train_loss: 0.07775 valid_loss: 0.08719 test_loss: 0.09676 \n",
      "[129/300] train_loss: 0.08003 valid_loss: 0.08767 test_loss: 0.09818 \n",
      "[130/300] train_loss: 0.08075 valid_loss: 0.08873 test_loss: 0.09845 \n",
      "[131/300] train_loss: 0.07845 valid_loss: 0.08663 test_loss: 0.09822 \n",
      "Validation loss decreased (0.086832 --> 0.086627).  Saving model ...\n",
      "[132/300] train_loss: 0.07822 valid_loss: 0.08797 test_loss: 0.09815 \n",
      "[133/300] train_loss: 0.07659 valid_loss: 0.08916 test_loss: 0.09793 \n",
      "[134/300] train_loss: 0.07822 valid_loss: 0.08581 test_loss: 0.09687 \n",
      "Validation loss decreased (0.086627 --> 0.085810).  Saving model ...\n",
      "[135/300] train_loss: 0.07997 valid_loss: 0.08647 test_loss: 0.09670 \n",
      "[136/300] train_loss: 0.07869 valid_loss: 0.08677 test_loss: 0.09820 \n",
      "[137/300] train_loss: 0.07532 valid_loss: 0.08468 test_loss: 0.09566 \n",
      "Validation loss decreased (0.085810 --> 0.084677).  Saving model ...\n",
      "[138/300] train_loss: 0.07539 valid_loss: 0.08582 test_loss: 0.09639 \n",
      "[139/300] train_loss: 0.07705 valid_loss: 0.08712 test_loss: 0.09835 \n",
      "[140/300] train_loss: 0.07713 valid_loss: 0.08632 test_loss: 0.09601 \n",
      "[141/300] train_loss: 0.07625 valid_loss: 0.08529 test_loss: 0.09528 \n",
      "[142/300] train_loss: 0.07571 valid_loss: 0.08645 test_loss: 0.09579 \n",
      "[143/300] train_loss: 0.07593 valid_loss: 0.08555 test_loss: 0.09462 \n",
      "[144/300] train_loss: 0.07768 valid_loss: 0.08361 test_loss: 0.09529 \n",
      "Validation loss decreased (0.084677 --> 0.083613).  Saving model ...\n",
      "[145/300] train_loss: 0.07508 valid_loss: 0.08609 test_loss: 0.09517 \n",
      "[146/300] train_loss: 0.07499 valid_loss: 0.08681 test_loss: 0.09588 \n",
      "[147/300] train_loss: 0.07634 valid_loss: 0.08539 test_loss: 0.09392 \n",
      "[148/300] train_loss: 0.07283 valid_loss: 0.08460 test_loss: 0.09499 \n",
      "[149/300] train_loss: 0.07581 valid_loss: 0.08448 test_loss: 0.09606 \n",
      "[150/300] train_loss: 0.07573 valid_loss: 0.08373 test_loss: 0.09447 \n",
      "[151/300] train_loss: 0.07579 valid_loss: 0.08552 test_loss: 0.09519 \n",
      "[152/300] train_loss: 0.07578 valid_loss: 0.08542 test_loss: 0.09561 \n",
      "[153/300] train_loss: 0.07698 valid_loss: 0.08511 test_loss: 0.09381 \n",
      "[154/300] train_loss: 0.07433 valid_loss: 0.08397 test_loss: 0.09449 \n",
      "[155/300] train_loss: 0.07356 valid_loss: 0.08502 test_loss: 0.09534 \n",
      "[156/300] train_loss: 0.07415 valid_loss: 0.08285 test_loss: 0.09480 \n",
      "Validation loss decreased (0.083613 --> 0.082854).  Saving model ...\n",
      "[157/300] train_loss: 0.07515 valid_loss: 0.08452 test_loss: 0.09544 \n",
      "[158/300] train_loss: 0.07548 valid_loss: 0.08627 test_loss: 0.09461 \n",
      "[159/300] train_loss: 0.07399 valid_loss: 0.08754 test_loss: 0.09507 \n",
      "[160/300] train_loss: 0.07329 valid_loss: 0.08388 test_loss: 0.09446 \n",
      "[161/300] train_loss: 0.07407 valid_loss: 0.08432 test_loss: 0.09381 \n",
      "[162/300] train_loss: 0.07472 valid_loss: 0.08432 test_loss: 0.09338 \n",
      "[163/300] train_loss: 0.07465 valid_loss: 0.08609 test_loss: 0.09567 \n",
      "[164/300] train_loss: 0.07477 valid_loss: 0.08537 test_loss: 0.09458 \n",
      "[165/300] train_loss: 0.07388 valid_loss: 0.08535 test_loss: 0.09366 \n",
      "[166/300] train_loss: 0.07461 valid_loss: 0.08428 test_loss: 0.09487 \n",
      "[167/300] train_loss: 0.07319 valid_loss: 0.08515 test_loss: 0.09455 \n",
      "[168/300] train_loss: 0.07530 valid_loss: 0.08538 test_loss: 0.09442 \n",
      "[169/300] train_loss: 0.07351 valid_loss: 0.08325 test_loss: 0.09451 \n",
      "[170/300] train_loss: 0.07208 valid_loss: 0.08491 test_loss: 0.09384 \n",
      "[171/300] train_loss: 0.07512 valid_loss: 0.08521 test_loss: 0.09384 \n",
      "[172/300] train_loss: 0.07143 valid_loss: 0.08418 test_loss: 0.09430 \n",
      "[173/300] train_loss: 0.07152 valid_loss: 0.08275 test_loss: 0.09290 \n",
      "Validation loss decreased (0.082854 --> 0.082749).  Saving model ...\n",
      "[174/300] train_loss: 0.07149 valid_loss: 0.08328 test_loss: 0.09228 \n",
      "[175/300] train_loss: 0.07337 valid_loss: 0.08245 test_loss: 0.09244 \n",
      "Validation loss decreased (0.082749 --> 0.082453).  Saving model ...\n",
      "[176/300] train_loss: 0.07069 valid_loss: 0.08285 test_loss: 0.09148 \n",
      "[177/300] train_loss: 0.07322 valid_loss: 0.08095 test_loss: 0.09150 \n",
      "Validation loss decreased (0.082453 --> 0.080946).  Saving model ...\n",
      "[178/300] train_loss: 0.07311 valid_loss: 0.08239 test_loss: 0.09276 \n",
      "[179/300] train_loss: 0.07115 valid_loss: 0.08202 test_loss: 0.09306 \n",
      "[180/300] train_loss: 0.07185 valid_loss: 0.08181 test_loss: 0.09287 \n",
      "[181/300] train_loss: 0.07195 valid_loss: 0.08320 test_loss: 0.09224 \n",
      "[182/300] train_loss: 0.07102 valid_loss: 0.08312 test_loss: 0.09147 \n",
      "[183/300] train_loss: 0.07162 valid_loss: 0.08139 test_loss: 0.09229 \n",
      "[184/300] train_loss: 0.07083 valid_loss: 0.08173 test_loss: 0.09135 \n",
      "[185/300] train_loss: 0.07117 valid_loss: 0.07941 test_loss: 0.09043 \n",
      "Validation loss decreased (0.080946 --> 0.079414).  Saving model ...\n",
      "[186/300] train_loss: 0.07051 valid_loss: 0.08158 test_loss: 0.09195 \n",
      "[187/300] train_loss: 0.07112 valid_loss: 0.08224 test_loss: 0.09200 \n",
      "[188/300] train_loss: 0.07077 valid_loss: 0.08193 test_loss: 0.09323 \n",
      "[189/300] train_loss: 0.07089 valid_loss: 0.08173 test_loss: 0.09113 \n",
      "[190/300] train_loss: 0.07031 valid_loss: 0.08017 test_loss: 0.09094 \n",
      "[191/300] train_loss: 0.07029 valid_loss: 0.08065 test_loss: 0.09033 \n",
      "[192/300] train_loss: 0.07010 valid_loss: 0.08046 test_loss: 0.09099 \n",
      "[193/300] train_loss: 0.06907 valid_loss: 0.08147 test_loss: 0.09149 \n",
      "[194/300] train_loss: 0.07052 valid_loss: 0.08183 test_loss: 0.09283 \n",
      "[195/300] train_loss: 0.06922 valid_loss: 0.08119 test_loss: 0.09078 \n",
      "[196/300] train_loss: 0.06889 valid_loss: 0.07970 test_loss: 0.09039 \n",
      "[197/300] train_loss: 0.07210 valid_loss: 0.08013 test_loss: 0.09090 \n",
      "[198/300] train_loss: 0.06848 valid_loss: 0.08021 test_loss: 0.09288 \n",
      "[199/300] train_loss: 0.06972 valid_loss: 0.08295 test_loss: 0.09256 \n",
      "[200/300] train_loss: 0.06759 valid_loss: 0.08152 test_loss: 0.09244 \n",
      "[201/300] train_loss: 0.06988 valid_loss: 0.08056 test_loss: 0.09047 \n",
      "[202/300] train_loss: 0.06873 valid_loss: 0.08329 test_loss: 0.09193 \n",
      "[203/300] train_loss: 0.07008 valid_loss: 0.08060 test_loss: 0.09060 \n",
      "[204/300] train_loss: 0.06791 valid_loss: 0.07942 test_loss: 0.08960 \n",
      "[205/300] train_loss: 0.07175 valid_loss: 0.07973 test_loss: 0.08975 \n",
      "[206/300] train_loss: 0.06645 valid_loss: 0.07976 test_loss: 0.08933 \n",
      "[207/300] train_loss: 0.06986 valid_loss: 0.07954 test_loss: 0.09049 \n",
      "[208/300] train_loss: 0.06754 valid_loss: 0.08048 test_loss: 0.08927 \n",
      "[209/300] train_loss: 0.06753 valid_loss: 0.07980 test_loss: 0.08885 \n",
      "[210/300] train_loss: 0.06753 valid_loss: 0.08068 test_loss: 0.09074 \n",
      "[211/300] train_loss: 0.06636 valid_loss: 0.08188 test_loss: 0.08921 \n",
      "[212/300] train_loss: 0.06669 valid_loss: 0.08074 test_loss: 0.08925 \n",
      "[213/300] train_loss: 0.06785 valid_loss: 0.07899 test_loss: 0.08966 \n",
      "Validation loss decreased (0.079414 --> 0.078994).  Saving model ...\n",
      "[214/300] train_loss: 0.06756 valid_loss: 0.07946 test_loss: 0.08964 \n",
      "[215/300] train_loss: 0.06602 valid_loss: 0.08022 test_loss: 0.08986 \n",
      "[216/300] train_loss: 0.06945 valid_loss: 0.08009 test_loss: 0.09090 \n",
      "[217/300] train_loss: 0.06628 valid_loss: 0.08003 test_loss: 0.08936 \n",
      "[218/300] train_loss: 0.06713 valid_loss: 0.08026 test_loss: 0.08866 \n",
      "[219/300] train_loss: 0.06772 valid_loss: 0.07966 test_loss: 0.08854 \n",
      "[220/300] train_loss: 0.06663 valid_loss: 0.07895 test_loss: 0.08809 \n",
      "Validation loss decreased (0.078994 --> 0.078954).  Saving model ...\n",
      "[221/300] train_loss: 0.06517 valid_loss: 0.08033 test_loss: 0.08843 \n",
      "[222/300] train_loss: 0.06765 valid_loss: 0.07982 test_loss: 0.08973 \n",
      "[223/300] train_loss: 0.06645 valid_loss: 0.07867 test_loss: 0.08912 \n",
      "Validation loss decreased (0.078954 --> 0.078672).  Saving model ...\n",
      "[224/300] train_loss: 0.06786 valid_loss: 0.07949 test_loss: 0.08971 \n",
      "[225/300] train_loss: 0.06842 valid_loss: 0.07976 test_loss: 0.08842 \n",
      "[226/300] train_loss: 0.06795 valid_loss: 0.08018 test_loss: 0.08920 \n",
      "[227/300] train_loss: 0.06652 valid_loss: 0.08169 test_loss: 0.08963 \n",
      "[228/300] train_loss: 0.06914 valid_loss: 0.07943 test_loss: 0.08912 \n",
      "[229/300] train_loss: 0.06655 valid_loss: 0.07819 test_loss: 0.08870 \n",
      "Validation loss decreased (0.078672 --> 0.078194).  Saving model ...\n",
      "[230/300] train_loss: 0.06607 valid_loss: 0.07913 test_loss: 0.08967 \n",
      "[231/300] train_loss: 0.06791 valid_loss: 0.07853 test_loss: 0.08833 \n",
      "[232/300] train_loss: 0.06718 valid_loss: 0.07959 test_loss: 0.08766 \n",
      "[233/300] train_loss: 0.06353 valid_loss: 0.07839 test_loss: 0.08849 \n",
      "[234/300] train_loss: 0.06790 valid_loss: 0.07913 test_loss: 0.08787 \n",
      "[235/300] train_loss: 0.06645 valid_loss: 0.07700 test_loss: 0.08808 \n",
      "Validation loss decreased (0.078194 --> 0.076999).  Saving model ...\n",
      "[236/300] train_loss: 0.06599 valid_loss: 0.07808 test_loss: 0.08652 \n",
      "[237/300] train_loss: 0.06512 valid_loss: 0.07745 test_loss: 0.08826 \n",
      "[238/300] train_loss: 0.06554 valid_loss: 0.07783 test_loss: 0.08749 \n",
      "[239/300] train_loss: 0.06546 valid_loss: 0.07852 test_loss: 0.08606 \n",
      "[240/300] train_loss: 0.06705 valid_loss: 0.07826 test_loss: 0.08701 \n",
      "[241/300] train_loss: 0.06544 valid_loss: 0.07890 test_loss: 0.08722 \n",
      "[242/300] train_loss: 0.06440 valid_loss: 0.08043 test_loss: 0.08828 \n",
      "[243/300] train_loss: 0.06382 valid_loss: 0.07997 test_loss: 0.08793 \n",
      "[244/300] train_loss: 0.06427 valid_loss: 0.07900 test_loss: 0.08824 \n",
      "[245/300] train_loss: 0.06577 valid_loss: 0.07738 test_loss: 0.08797 \n",
      "[246/300] train_loss: 0.06567 valid_loss: 0.07839 test_loss: 0.08671 \n",
      "[247/300] train_loss: 0.06439 valid_loss: 0.07744 test_loss: 0.08672 \n",
      "[248/300] train_loss: 0.06519 valid_loss: 0.07727 test_loss: 0.08685 \n",
      "[249/300] train_loss: 0.06635 valid_loss: 0.07767 test_loss: 0.08719 \n",
      "[250/300] train_loss: 0.06486 valid_loss: 0.07847 test_loss: 0.08743 \n",
      "[251/300] train_loss: 0.06427 valid_loss: 0.07734 test_loss: 0.08579 \n",
      "[252/300] train_loss: 0.06632 valid_loss: 0.07995 test_loss: 0.08755 \n",
      "[253/300] train_loss: 0.06352 valid_loss: 0.07817 test_loss: 0.08787 \n",
      "[254/300] train_loss: 0.06285 valid_loss: 0.07712 test_loss: 0.08711 \n",
      "[255/300] train_loss: 0.06370 valid_loss: 0.07605 test_loss: 0.08630 \n",
      "Validation loss decreased (0.076999 --> 0.076046).  Saving model ...\n",
      "[256/300] train_loss: 0.06490 valid_loss: 0.07668 test_loss: 0.08704 \n",
      "[257/300] train_loss: 0.06419 valid_loss: 0.07619 test_loss: 0.08574 \n",
      "[258/300] train_loss: 0.06146 valid_loss: 0.07803 test_loss: 0.08798 \n",
      "[259/300] train_loss: 0.06536 valid_loss: 0.07805 test_loss: 0.08752 \n",
      "[260/300] train_loss: 0.06346 valid_loss: 0.07679 test_loss: 0.08751 \n",
      "[261/300] train_loss: 0.06198 valid_loss: 0.07725 test_loss: 0.08712 \n",
      "[262/300] train_loss: 0.06459 valid_loss: 0.07594 test_loss: 0.08698 \n",
      "Validation loss decreased (0.076046 --> 0.075944).  Saving model ...\n",
      "[263/300] train_loss: 0.06523 valid_loss: 0.07756 test_loss: 0.08752 \n",
      "[264/300] train_loss: 0.06493 valid_loss: 0.07611 test_loss: 0.08491 \n",
      "[265/300] train_loss: 0.06427 valid_loss: 0.07593 test_loss: 0.08647 \n",
      "Validation loss decreased (0.075944 --> 0.075935).  Saving model ...\n",
      "[266/300] train_loss: 0.06353 valid_loss: 0.07645 test_loss: 0.08643 \n",
      "[267/300] train_loss: 0.06286 valid_loss: 0.07655 test_loss: 0.08704 \n",
      "[268/300] train_loss: 0.06193 valid_loss: 0.07657 test_loss: 0.08635 \n",
      "[269/300] train_loss: 0.06280 valid_loss: 0.07558 test_loss: 0.08564 \n",
      "Validation loss decreased (0.075935 --> 0.075583).  Saving model ...\n",
      "[270/300] train_loss: 0.06413 valid_loss: 0.07700 test_loss: 0.08686 \n",
      "[271/300] train_loss: 0.06370 valid_loss: 0.07696 test_loss: 0.08652 \n",
      "[272/300] train_loss: 0.06359 valid_loss: 0.07693 test_loss: 0.08681 \n",
      "[273/300] train_loss: 0.06225 valid_loss: 0.07709 test_loss: 0.08695 \n",
      "[274/300] train_loss: 0.06361 valid_loss: 0.07704 test_loss: 0.08611 \n",
      "[275/300] train_loss: 0.06223 valid_loss: 0.07764 test_loss: 0.08733 \n",
      "[276/300] train_loss: 0.06151 valid_loss: 0.07829 test_loss: 0.08590 \n",
      "[277/300] train_loss: 0.06301 valid_loss: 0.07632 test_loss: 0.08697 \n",
      "[278/300] train_loss: 0.06275 valid_loss: 0.07688 test_loss: 0.08625 \n",
      "[279/300] train_loss: 0.06410 valid_loss: 0.07692 test_loss: 0.08577 \n",
      "[280/300] train_loss: 0.06229 valid_loss: 0.07705 test_loss: 0.08608 \n",
      "[281/300] train_loss: 0.06274 valid_loss: 0.07784 test_loss: 0.08616 \n",
      "[282/300] train_loss: 0.06132 valid_loss: 0.07795 test_loss: 0.08630 \n",
      "[283/300] train_loss: 0.06150 valid_loss: 0.07663 test_loss: 0.08694 \n",
      "[284/300] train_loss: 0.06411 valid_loss: 0.07698 test_loss: 0.08603 \n",
      "[285/300] train_loss: 0.06163 valid_loss: 0.07601 test_loss: 0.08621 \n",
      "[286/300] train_loss: 0.06154 valid_loss: 0.07626 test_loss: 0.08662 \n",
      "[287/300] train_loss: 0.06103 valid_loss: 0.07693 test_loss: 0.08649 \n",
      "[288/300] train_loss: 0.06275 valid_loss: 0.07535 test_loss: 0.08670 \n",
      "Validation loss decreased (0.075583 --> 0.075354).  Saving model ...\n",
      "[289/300] train_loss: 0.06415 valid_loss: 0.07690 test_loss: 0.08626 \n",
      "[290/300] train_loss: 0.06296 valid_loss: 0.07522 test_loss: 0.08571 \n",
      "Validation loss decreased (0.075354 --> 0.075221).  Saving model ...\n",
      "[291/300] train_loss: 0.06235 valid_loss: 0.07758 test_loss: 0.08728 \n",
      "[292/300] train_loss: 0.06159 valid_loss: 0.07653 test_loss: 0.08681 \n",
      "[293/300] train_loss: 0.06059 valid_loss: 0.07597 test_loss: 0.08621 \n",
      "[294/300] train_loss: 0.06262 valid_loss: 0.07756 test_loss: 0.08617 \n",
      "[295/300] train_loss: 0.06068 valid_loss: 0.07641 test_loss: 0.08547 \n",
      "[296/300] train_loss: 0.06269 valid_loss: 0.07696 test_loss: 0.08522 \n",
      "[297/300] train_loss: 0.06283 valid_loss: 0.07548 test_loss: 0.08490 \n",
      "[298/300] train_loss: 0.06234 valid_loss: 0.07820 test_loss: 0.08572 \n",
      "[299/300] train_loss: 0.06031 valid_loss: 0.07787 test_loss: 0.08582 \n",
      "[300/300] train_loss: 0.06102 valid_loss: 0.07568 test_loss: 0.08552 \n",
      "TRAINING MODEL 11\n",
      "[  1/300] train_loss: 0.64499 valid_loss: 0.53365 test_loss: 0.54125 \n",
      "Validation loss decreased (inf --> 0.533650).  Saving model ...\n",
      "[  2/300] train_loss: 0.42957 valid_loss: 0.38794 test_loss: 0.39543 \n",
      "Validation loss decreased (0.533650 --> 0.387936).  Saving model ...\n",
      "[  3/300] train_loss: 0.31855 valid_loss: 0.31292 test_loss: 0.32185 \n",
      "Validation loss decreased (0.387936 --> 0.312925).  Saving model ...\n",
      "[  4/300] train_loss: 0.26287 valid_loss: 0.27135 test_loss: 0.28338 \n",
      "Validation loss decreased (0.312925 --> 0.271345).  Saving model ...\n",
      "[  5/300] train_loss: 0.22936 valid_loss: 0.23875 test_loss: 0.25240 \n",
      "Validation loss decreased (0.271345 --> 0.238754).  Saving model ...\n",
      "[  6/300] train_loss: 0.20742 valid_loss: 0.21985 test_loss: 0.23538 \n",
      "Validation loss decreased (0.238754 --> 0.219849).  Saving model ...\n",
      "[  7/300] train_loss: 0.19662 valid_loss: 0.20198 test_loss: 0.21517 \n",
      "Validation loss decreased (0.219849 --> 0.201976).  Saving model ...\n",
      "[  8/300] train_loss: 0.18211 valid_loss: 0.19092 test_loss: 0.20563 \n",
      "Validation loss decreased (0.201976 --> 0.190920).  Saving model ...\n",
      "[  9/300] train_loss: 0.17485 valid_loss: 0.18961 test_loss: 0.20539 \n",
      "Validation loss decreased (0.190920 --> 0.189608).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17573 valid_loss: 0.17778 test_loss: 0.19367 \n",
      "Validation loss decreased (0.189608 --> 0.177776).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16772 valid_loss: 0.17162 test_loss: 0.18744 \n",
      "Validation loss decreased (0.177776 --> 0.171617).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16416 valid_loss: 0.16462 test_loss: 0.17800 \n",
      "Validation loss decreased (0.171617 --> 0.164620).  Saving model ...\n",
      "[ 13/300] train_loss: 0.16032 valid_loss: 0.16320 test_loss: 0.17752 \n",
      "Validation loss decreased (0.164620 --> 0.163197).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15648 valid_loss: 0.15992 test_loss: 0.17249 \n",
      "Validation loss decreased (0.163197 --> 0.159917).  Saving model ...\n",
      "[ 15/300] train_loss: 0.15173 valid_loss: 0.15658 test_loss: 0.16624 \n",
      "Validation loss decreased (0.159917 --> 0.156576).  Saving model ...\n",
      "[ 16/300] train_loss: 0.15155 valid_loss: 0.15327 test_loss: 0.16281 \n",
      "Validation loss decreased (0.156576 --> 0.153270).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14726 valid_loss: 0.15044 test_loss: 0.16397 \n",
      "Validation loss decreased (0.153270 --> 0.150439).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14507 valid_loss: 0.14768 test_loss: 0.16082 \n",
      "Validation loss decreased (0.150439 --> 0.147677).  Saving model ...\n",
      "[ 19/300] train_loss: 0.14079 valid_loss: 0.14456 test_loss: 0.15625 \n",
      "Validation loss decreased (0.147677 --> 0.144557).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13687 valid_loss: 0.14292 test_loss: 0.15581 \n",
      "Validation loss decreased (0.144557 --> 0.142915).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13381 valid_loss: 0.14003 test_loss: 0.15147 \n",
      "Validation loss decreased (0.142915 --> 0.140026).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13408 valid_loss: 0.14133 test_loss: 0.15763 \n",
      "[ 23/300] train_loss: 0.13259 valid_loss: 0.13870 test_loss: 0.15186 \n",
      "Validation loss decreased (0.140026 --> 0.138703).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12936 valid_loss: 0.13675 test_loss: 0.14823 \n",
      "Validation loss decreased (0.138703 --> 0.136746).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12536 valid_loss: 0.13607 test_loss: 0.14859 \n",
      "Validation loss decreased (0.136746 --> 0.136073).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12764 valid_loss: 0.13361 test_loss: 0.14450 \n",
      "Validation loss decreased (0.136073 --> 0.133608).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12073 valid_loss: 0.13163 test_loss: 0.14336 \n",
      "Validation loss decreased (0.133608 --> 0.131628).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12266 valid_loss: 0.13136 test_loss: 0.14432 \n",
      "Validation loss decreased (0.131628 --> 0.131361).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12219 valid_loss: 0.12911 test_loss: 0.14224 \n",
      "Validation loss decreased (0.131361 --> 0.129106).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11793 valid_loss: 0.12717 test_loss: 0.13902 \n",
      "Validation loss decreased (0.129106 --> 0.127165).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11673 valid_loss: 0.12555 test_loss: 0.13860 \n",
      "Validation loss decreased (0.127165 --> 0.125549).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11588 valid_loss: 0.12577 test_loss: 0.14023 \n",
      "[ 33/300] train_loss: 0.11781 valid_loss: 0.12613 test_loss: 0.13723 \n",
      "[ 34/300] train_loss: 0.11502 valid_loss: 0.12655 test_loss: 0.13691 \n",
      "[ 35/300] train_loss: 0.11384 valid_loss: 0.12272 test_loss: 0.13428 \n",
      "Validation loss decreased (0.125549 --> 0.122718).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11204 valid_loss: 0.12136 test_loss: 0.13389 \n",
      "Validation loss decreased (0.122718 --> 0.121359).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11525 valid_loss: 0.11952 test_loss: 0.13147 \n",
      "Validation loss decreased (0.121359 --> 0.119516).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11244 valid_loss: 0.12096 test_loss: 0.13369 \n",
      "[ 39/300] train_loss: 0.11228 valid_loss: 0.11756 test_loss: 0.13218 \n",
      "Validation loss decreased (0.119516 --> 0.117562).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10923 valid_loss: 0.11854 test_loss: 0.13107 \n",
      "[ 41/300] train_loss: 0.10851 valid_loss: 0.11744 test_loss: 0.13014 \n",
      "Validation loss decreased (0.117562 --> 0.117436).  Saving model ...\n",
      "[ 42/300] train_loss: 0.11003 valid_loss: 0.11326 test_loss: 0.12674 \n",
      "Validation loss decreased (0.117436 --> 0.113258).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10949 valid_loss: 0.11787 test_loss: 0.13167 \n",
      "[ 44/300] train_loss: 0.10966 valid_loss: 0.11214 test_loss: 0.12709 \n",
      "Validation loss decreased (0.113258 --> 0.112138).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10929 valid_loss: 0.11746 test_loss: 0.12851 \n",
      "[ 46/300] train_loss: 0.10728 valid_loss: 0.11347 test_loss: 0.12567 \n",
      "[ 47/300] train_loss: 0.10391 valid_loss: 0.11049 test_loss: 0.12364 \n",
      "Validation loss decreased (0.112138 --> 0.110492).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10579 valid_loss: 0.11202 test_loss: 0.12475 \n",
      "[ 49/300] train_loss: 0.10716 valid_loss: 0.11147 test_loss: 0.12302 \n",
      "[ 50/300] train_loss: 0.10432 valid_loss: 0.11326 test_loss: 0.12408 \n",
      "[ 51/300] train_loss: 0.10227 valid_loss: 0.11091 test_loss: 0.12337 \n",
      "[ 52/300] train_loss: 0.10447 valid_loss: 0.10958 test_loss: 0.12138 \n",
      "Validation loss decreased (0.110492 --> 0.109575).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10148 valid_loss: 0.10928 test_loss: 0.12236 \n",
      "Validation loss decreased (0.109575 --> 0.109283).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10171 valid_loss: 0.10736 test_loss: 0.12031 \n",
      "Validation loss decreased (0.109283 --> 0.107364).  Saving model ...\n",
      "[ 55/300] train_loss: 0.10123 valid_loss: 0.10791 test_loss: 0.11985 \n",
      "[ 56/300] train_loss: 0.10057 valid_loss: 0.10798 test_loss: 0.12144 \n",
      "[ 57/300] train_loss: 0.09775 valid_loss: 0.10757 test_loss: 0.11980 \n",
      "[ 58/300] train_loss: 0.09814 valid_loss: 0.10676 test_loss: 0.11933 \n",
      "Validation loss decreased (0.107364 --> 0.106763).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09822 valid_loss: 0.10938 test_loss: 0.12034 \n",
      "[ 60/300] train_loss: 0.09963 valid_loss: 0.10689 test_loss: 0.12043 \n",
      "[ 61/300] train_loss: 0.09866 valid_loss: 0.10675 test_loss: 0.11849 \n",
      "Validation loss decreased (0.106763 --> 0.106745).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09755 valid_loss: 0.10699 test_loss: 0.11881 \n",
      "[ 63/300] train_loss: 0.10026 valid_loss: 0.11162 test_loss: 0.12319 \n",
      "[ 64/300] train_loss: 0.09592 valid_loss: 0.10346 test_loss: 0.11588 \n",
      "Validation loss decreased (0.106745 --> 0.103457).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09705 valid_loss: 0.10379 test_loss: 0.11660 \n",
      "[ 66/300] train_loss: 0.09787 valid_loss: 0.10365 test_loss: 0.11722 \n",
      "[ 67/300] train_loss: 0.09771 valid_loss: 0.10437 test_loss: 0.11540 \n",
      "[ 68/300] train_loss: 0.09406 valid_loss: 0.10444 test_loss: 0.11660 \n",
      "[ 69/300] train_loss: 0.09413 valid_loss: 0.10367 test_loss: 0.11488 \n",
      "[ 70/300] train_loss: 0.09559 valid_loss: 0.10248 test_loss: 0.11359 \n",
      "Validation loss decreased (0.103457 --> 0.102479).  Saving model ...\n",
      "[ 71/300] train_loss: 0.09259 valid_loss: 0.10018 test_loss: 0.11356 \n",
      "Validation loss decreased (0.102479 --> 0.100179).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09451 valid_loss: 0.10196 test_loss: 0.11461 \n",
      "[ 73/300] train_loss: 0.09242 valid_loss: 0.10389 test_loss: 0.11637 \n",
      "[ 74/300] train_loss: 0.09253 valid_loss: 0.10350 test_loss: 0.11383 \n",
      "[ 75/300] train_loss: 0.08910 valid_loss: 0.10144 test_loss: 0.11352 \n",
      "[ 76/300] train_loss: 0.08952 valid_loss: 0.10344 test_loss: 0.11516 \n",
      "[ 77/300] train_loss: 0.09108 valid_loss: 0.09956 test_loss: 0.11150 \n",
      "Validation loss decreased (0.100179 --> 0.099558).  Saving model ...\n",
      "[ 78/300] train_loss: 0.08884 valid_loss: 0.09796 test_loss: 0.11146 \n",
      "Validation loss decreased (0.099558 --> 0.097959).  Saving model ...\n",
      "[ 79/300] train_loss: 0.08959 valid_loss: 0.09988 test_loss: 0.11184 \n",
      "[ 80/300] train_loss: 0.09184 valid_loss: 0.09975 test_loss: 0.11169 \n",
      "[ 81/300] train_loss: 0.09101 valid_loss: 0.10040 test_loss: 0.11180 \n",
      "[ 82/300] train_loss: 0.09029 valid_loss: 0.09802 test_loss: 0.11030 \n",
      "[ 83/300] train_loss: 0.08794 valid_loss: 0.09830 test_loss: 0.11160 \n",
      "[ 84/300] train_loss: 0.08824 valid_loss: 0.09960 test_loss: 0.11159 \n",
      "[ 85/300] train_loss: 0.08785 valid_loss: 0.09875 test_loss: 0.11022 \n",
      "[ 86/300] train_loss: 0.08977 valid_loss: 0.09733 test_loss: 0.10938 \n",
      "Validation loss decreased (0.097959 --> 0.097335).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08886 valid_loss: 0.09601 test_loss: 0.10773 \n",
      "Validation loss decreased (0.097335 --> 0.096007).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08783 valid_loss: 0.09842 test_loss: 0.11034 \n",
      "[ 89/300] train_loss: 0.08878 valid_loss: 0.10161 test_loss: 0.11257 \n",
      "[ 90/300] train_loss: 0.08645 valid_loss: 0.09514 test_loss: 0.10711 \n",
      "Validation loss decreased (0.096007 --> 0.095143).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08861 valid_loss: 0.09630 test_loss: 0.10854 \n",
      "[ 92/300] train_loss: 0.08824 valid_loss: 0.09766 test_loss: 0.10918 \n",
      "[ 93/300] train_loss: 0.08825 valid_loss: 0.09298 test_loss: 0.10594 \n",
      "Validation loss decreased (0.095143 --> 0.092982).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08929 valid_loss: 0.09388 test_loss: 0.10688 \n",
      "[ 95/300] train_loss: 0.08305 valid_loss: 0.09585 test_loss: 0.10837 \n",
      "[ 96/300] train_loss: 0.08632 valid_loss: 0.09587 test_loss: 0.10664 \n",
      "[ 97/300] train_loss: 0.08435 valid_loss: 0.09602 test_loss: 0.10705 \n",
      "[ 98/300] train_loss: 0.08497 valid_loss: 0.09647 test_loss: 0.10870 \n",
      "[ 99/300] train_loss: 0.08307 valid_loss: 0.09697 test_loss: 0.10826 \n",
      "[100/300] train_loss: 0.08494 valid_loss: 0.09323 test_loss: 0.10536 \n",
      "[101/300] train_loss: 0.08597 valid_loss: 0.09589 test_loss: 0.10791 \n",
      "[102/300] train_loss: 0.08459 valid_loss: 0.09550 test_loss: 0.10796 \n",
      "[103/300] train_loss: 0.08300 valid_loss: 0.09417 test_loss: 0.10778 \n",
      "[104/300] train_loss: 0.08383 valid_loss: 0.09184 test_loss: 0.10475 \n",
      "Validation loss decreased (0.092982 --> 0.091841).  Saving model ...\n",
      "[105/300] train_loss: 0.08285 valid_loss: 0.09526 test_loss: 0.10724 \n",
      "[106/300] train_loss: 0.08024 valid_loss: 0.09425 test_loss: 0.10670 \n",
      "[107/300] train_loss: 0.08575 valid_loss: 0.09689 test_loss: 0.10741 \n",
      "[108/300] train_loss: 0.08425 valid_loss: 0.09593 test_loss: 0.10723 \n",
      "[109/300] train_loss: 0.08388 valid_loss: 0.09324 test_loss: 0.10590 \n",
      "[110/300] train_loss: 0.08301 valid_loss: 0.09330 test_loss: 0.10501 \n",
      "[111/300] train_loss: 0.08301 valid_loss: 0.09052 test_loss: 0.10226 \n",
      "Validation loss decreased (0.091841 --> 0.090521).  Saving model ...\n",
      "[112/300] train_loss: 0.08015 valid_loss: 0.09125 test_loss: 0.10444 \n",
      "[113/300] train_loss: 0.08076 valid_loss: 0.09296 test_loss: 0.10457 \n",
      "[114/300] train_loss: 0.08271 valid_loss: 0.09034 test_loss: 0.10217 \n",
      "Validation loss decreased (0.090521 --> 0.090339).  Saving model ...\n",
      "[115/300] train_loss: 0.08042 valid_loss: 0.09631 test_loss: 0.10707 \n",
      "[116/300] train_loss: 0.07885 valid_loss: 0.09253 test_loss: 0.10470 \n",
      "[117/300] train_loss: 0.08295 valid_loss: 0.09399 test_loss: 0.10481 \n",
      "[118/300] train_loss: 0.07965 valid_loss: 0.09203 test_loss: 0.10451 \n",
      "[119/300] train_loss: 0.08015 valid_loss: 0.09202 test_loss: 0.10332 \n",
      "[120/300] train_loss: 0.08067 valid_loss: 0.09020 test_loss: 0.10174 \n",
      "Validation loss decreased (0.090339 --> 0.090195).  Saving model ...\n",
      "[121/300] train_loss: 0.07829 valid_loss: 0.08865 test_loss: 0.10185 \n",
      "Validation loss decreased (0.090195 --> 0.088650).  Saving model ...\n",
      "[122/300] train_loss: 0.08103 valid_loss: 0.09274 test_loss: 0.10417 \n",
      "[123/300] train_loss: 0.07996 valid_loss: 0.08980 test_loss: 0.10108 \n",
      "[124/300] train_loss: 0.08074 valid_loss: 0.09207 test_loss: 0.10382 \n",
      "[125/300] train_loss: 0.07864 valid_loss: 0.08881 test_loss: 0.10148 \n",
      "[126/300] train_loss: 0.07827 valid_loss: 0.08992 test_loss: 0.10155 \n",
      "[127/300] train_loss: 0.07850 valid_loss: 0.09110 test_loss: 0.10155 \n",
      "[128/300] train_loss: 0.07994 valid_loss: 0.08655 test_loss: 0.10048 \n",
      "Validation loss decreased (0.088650 --> 0.086550).  Saving model ...\n",
      "[129/300] train_loss: 0.07721 valid_loss: 0.08889 test_loss: 0.10176 \n",
      "[130/300] train_loss: 0.07991 valid_loss: 0.09071 test_loss: 0.10151 \n",
      "[131/300] train_loss: 0.07551 valid_loss: 0.08831 test_loss: 0.10092 \n",
      "[132/300] train_loss: 0.07596 valid_loss: 0.09178 test_loss: 0.10195 \n",
      "[133/300] train_loss: 0.07883 valid_loss: 0.09066 test_loss: 0.10266 \n",
      "[134/300] train_loss: 0.07672 valid_loss: 0.09124 test_loss: 0.10193 \n",
      "[135/300] train_loss: 0.07841 valid_loss: 0.08905 test_loss: 0.10018 \n",
      "[136/300] train_loss: 0.07605 valid_loss: 0.09031 test_loss: 0.10111 \n",
      "[137/300] train_loss: 0.07751 valid_loss: 0.08930 test_loss: 0.10061 \n",
      "[138/300] train_loss: 0.07910 valid_loss: 0.08915 test_loss: 0.10070 \n",
      "[139/300] train_loss: 0.07746 valid_loss: 0.08879 test_loss: 0.10092 \n",
      "[140/300] train_loss: 0.07498 valid_loss: 0.09207 test_loss: 0.10321 \n",
      "[141/300] train_loss: 0.07753 valid_loss: 0.08690 test_loss: 0.09992 \n",
      "[142/300] train_loss: 0.07706 valid_loss: 0.08682 test_loss: 0.09862 \n",
      "[143/300] train_loss: 0.07561 valid_loss: 0.08782 test_loss: 0.10046 \n",
      "[144/300] train_loss: 0.07697 valid_loss: 0.08891 test_loss: 0.10145 \n",
      "[145/300] train_loss: 0.07558 valid_loss: 0.08762 test_loss: 0.09944 \n",
      "[146/300] train_loss: 0.07433 valid_loss: 0.08836 test_loss: 0.10041 \n",
      "[147/300] train_loss: 0.07594 valid_loss: 0.08516 test_loss: 0.09857 \n",
      "Validation loss decreased (0.086550 --> 0.085163).  Saving model ...\n",
      "[148/300] train_loss: 0.07581 valid_loss: 0.08820 test_loss: 0.10115 \n",
      "[149/300] train_loss: 0.07650 valid_loss: 0.08550 test_loss: 0.09727 \n",
      "[150/300] train_loss: 0.07706 valid_loss: 0.09006 test_loss: 0.10160 \n",
      "[151/300] train_loss: 0.07348 valid_loss: 0.08603 test_loss: 0.09836 \n",
      "[152/300] train_loss: 0.07428 valid_loss: 0.08750 test_loss: 0.09876 \n",
      "[153/300] train_loss: 0.07534 valid_loss: 0.08564 test_loss: 0.09795 \n",
      "[154/300] train_loss: 0.07422 valid_loss: 0.08657 test_loss: 0.09818 \n",
      "[155/300] train_loss: 0.07647 valid_loss: 0.09062 test_loss: 0.10208 \n",
      "[156/300] train_loss: 0.07493 valid_loss: 0.08469 test_loss: 0.09825 \n",
      "Validation loss decreased (0.085163 --> 0.084690).  Saving model ...\n",
      "[157/300] train_loss: 0.07353 valid_loss: 0.08580 test_loss: 0.09909 \n",
      "[158/300] train_loss: 0.07413 valid_loss: 0.08520 test_loss: 0.09880 \n",
      "[159/300] train_loss: 0.07380 valid_loss: 0.08533 test_loss: 0.09711 \n",
      "[160/300] train_loss: 0.07294 valid_loss: 0.08412 test_loss: 0.09680 \n",
      "Validation loss decreased (0.084690 --> 0.084119).  Saving model ...\n",
      "[161/300] train_loss: 0.07340 valid_loss: 0.08749 test_loss: 0.09922 \n",
      "[162/300] train_loss: 0.07323 valid_loss: 0.08338 test_loss: 0.09558 \n",
      "Validation loss decreased (0.084119 --> 0.083380).  Saving model ...\n",
      "[163/300] train_loss: 0.07269 valid_loss: 0.08449 test_loss: 0.09755 \n",
      "[164/300] train_loss: 0.07489 valid_loss: 0.08623 test_loss: 0.09880 \n",
      "[165/300] train_loss: 0.07301 valid_loss: 0.08470 test_loss: 0.09683 \n",
      "[166/300] train_loss: 0.07231 valid_loss: 0.08570 test_loss: 0.09787 \n",
      "[167/300] train_loss: 0.07352 valid_loss: 0.08389 test_loss: 0.09649 \n",
      "[168/300] train_loss: 0.07500 valid_loss: 0.08304 test_loss: 0.09602 \n",
      "Validation loss decreased (0.083380 --> 0.083045).  Saving model ...\n",
      "[169/300] train_loss: 0.07206 valid_loss: 0.08356 test_loss: 0.09709 \n",
      "[170/300] train_loss: 0.07383 valid_loss: 0.08471 test_loss: 0.09789 \n",
      "[171/300] train_loss: 0.07285 valid_loss: 0.08460 test_loss: 0.09738 \n",
      "[172/300] train_loss: 0.07232 valid_loss: 0.08476 test_loss: 0.09727 \n",
      "[173/300] train_loss: 0.07404 valid_loss: 0.08397 test_loss: 0.09739 \n",
      "[174/300] train_loss: 0.07369 valid_loss: 0.08457 test_loss: 0.09722 \n",
      "[175/300] train_loss: 0.06984 valid_loss: 0.08363 test_loss: 0.09782 \n",
      "[176/300] train_loss: 0.07112 valid_loss: 0.08209 test_loss: 0.09502 \n",
      "Validation loss decreased (0.083045 --> 0.082089).  Saving model ...\n",
      "[177/300] train_loss: 0.07058 valid_loss: 0.08395 test_loss: 0.09671 \n",
      "[178/300] train_loss: 0.06974 valid_loss: 0.08299 test_loss: 0.09538 \n",
      "[179/300] train_loss: 0.07117 valid_loss: 0.08420 test_loss: 0.09618 \n",
      "[180/300] train_loss: 0.06964 valid_loss: 0.08446 test_loss: 0.09682 \n",
      "[181/300] train_loss: 0.07267 valid_loss: 0.08325 test_loss: 0.09595 \n",
      "[182/300] train_loss: 0.07033 valid_loss: 0.08356 test_loss: 0.09553 \n",
      "[183/300] train_loss: 0.07103 valid_loss: 0.08492 test_loss: 0.09667 \n",
      "[184/300] train_loss: 0.07036 valid_loss: 0.08392 test_loss: 0.09678 \n",
      "[185/300] train_loss: 0.06989 valid_loss: 0.08456 test_loss: 0.09769 \n",
      "[186/300] train_loss: 0.07025 valid_loss: 0.08141 test_loss: 0.09470 \n",
      "Validation loss decreased (0.082089 --> 0.081408).  Saving model ...\n",
      "[187/300] train_loss: 0.06967 valid_loss: 0.08350 test_loss: 0.09548 \n",
      "[188/300] train_loss: 0.07036 valid_loss: 0.08122 test_loss: 0.09456 \n",
      "Validation loss decreased (0.081408 --> 0.081216).  Saving model ...\n",
      "[189/300] train_loss: 0.07135 valid_loss: 0.08283 test_loss: 0.09486 \n",
      "[190/300] train_loss: 0.07008 valid_loss: 0.08237 test_loss: 0.09544 \n",
      "[191/300] train_loss: 0.07192 valid_loss: 0.08272 test_loss: 0.09583 \n",
      "[192/300] train_loss: 0.06857 valid_loss: 0.08116 test_loss: 0.09443 \n",
      "Validation loss decreased (0.081216 --> 0.081157).  Saving model ...\n",
      "[193/300] train_loss: 0.06928 valid_loss: 0.08446 test_loss: 0.09645 \n",
      "[194/300] train_loss: 0.07091 valid_loss: 0.08337 test_loss: 0.09474 \n",
      "[195/300] train_loss: 0.06987 valid_loss: 0.08270 test_loss: 0.09581 \n",
      "[196/300] train_loss: 0.07102 valid_loss: 0.08308 test_loss: 0.09484 \n",
      "[197/300] train_loss: 0.07018 valid_loss: 0.08268 test_loss: 0.09513 \n",
      "[198/300] train_loss: 0.06881 valid_loss: 0.08058 test_loss: 0.09267 \n",
      "Validation loss decreased (0.081157 --> 0.080576).  Saving model ...\n",
      "[199/300] train_loss: 0.07104 valid_loss: 0.08158 test_loss: 0.09321 \n",
      "[200/300] train_loss: 0.06920 valid_loss: 0.08133 test_loss: 0.09345 \n",
      "[201/300] train_loss: 0.06890 valid_loss: 0.08018 test_loss: 0.09375 \n",
      "Validation loss decreased (0.080576 --> 0.080183).  Saving model ...\n",
      "[202/300] train_loss: 0.06848 valid_loss: 0.08031 test_loss: 0.09317 \n",
      "[203/300] train_loss: 0.06552 valid_loss: 0.08105 test_loss: 0.09400 \n",
      "[204/300] train_loss: 0.06680 valid_loss: 0.08153 test_loss: 0.09490 \n",
      "[205/300] train_loss: 0.06665 valid_loss: 0.08216 test_loss: 0.09435 \n",
      "[206/300] train_loss: 0.06866 valid_loss: 0.08185 test_loss: 0.09522 \n",
      "[207/300] train_loss: 0.06907 valid_loss: 0.08103 test_loss: 0.09337 \n",
      "[208/300] train_loss: 0.06849 valid_loss: 0.08073 test_loss: 0.09364 \n",
      "[209/300] train_loss: 0.06666 valid_loss: 0.07965 test_loss: 0.09290 \n",
      "Validation loss decreased (0.080183 --> 0.079650).  Saving model ...\n",
      "[210/300] train_loss: 0.06807 valid_loss: 0.07934 test_loss: 0.09209 \n",
      "Validation loss decreased (0.079650 --> 0.079338).  Saving model ...\n",
      "[211/300] train_loss: 0.06872 valid_loss: 0.08359 test_loss: 0.09510 \n",
      "[212/300] train_loss: 0.06655 valid_loss: 0.08012 test_loss: 0.09226 \n",
      "[213/300] train_loss: 0.06951 valid_loss: 0.08041 test_loss: 0.09431 \n",
      "[214/300] train_loss: 0.06974 valid_loss: 0.08126 test_loss: 0.09446 \n",
      "[215/300] train_loss: 0.06689 valid_loss: 0.08147 test_loss: 0.09320 \n",
      "[216/300] train_loss: 0.06664 valid_loss: 0.08106 test_loss: 0.09211 \n",
      "[217/300] train_loss: 0.06630 valid_loss: 0.07984 test_loss: 0.09299 \n",
      "[218/300] train_loss: 0.06665 valid_loss: 0.08198 test_loss: 0.09355 \n",
      "[219/300] train_loss: 0.06684 valid_loss: 0.08138 test_loss: 0.09331 \n",
      "[220/300] train_loss: 0.06615 valid_loss: 0.08097 test_loss: 0.09326 \n",
      "[221/300] train_loss: 0.06815 valid_loss: 0.07995 test_loss: 0.09236 \n",
      "[222/300] train_loss: 0.06874 valid_loss: 0.08078 test_loss: 0.09311 \n",
      "[223/300] train_loss: 0.06709 valid_loss: 0.08054 test_loss: 0.09332 \n",
      "[224/300] train_loss: 0.06757 valid_loss: 0.08034 test_loss: 0.09247 \n",
      "[225/300] train_loss: 0.06561 valid_loss: 0.08038 test_loss: 0.09353 \n",
      "[226/300] train_loss: 0.06882 valid_loss: 0.07936 test_loss: 0.09252 \n",
      "[227/300] train_loss: 0.06612 valid_loss: 0.07938 test_loss: 0.09193 \n",
      "[228/300] train_loss: 0.06592 valid_loss: 0.07798 test_loss: 0.09166 \n",
      "Validation loss decreased (0.079338 --> 0.077981).  Saving model ...\n",
      "[229/300] train_loss: 0.06344 valid_loss: 0.08165 test_loss: 0.09256 \n",
      "[230/300] train_loss: 0.06650 valid_loss: 0.07885 test_loss: 0.09088 \n",
      "[231/300] train_loss: 0.06741 valid_loss: 0.07953 test_loss: 0.09045 \n",
      "[232/300] train_loss: 0.06663 valid_loss: 0.08110 test_loss: 0.09365 \n",
      "[233/300] train_loss: 0.06467 valid_loss: 0.07917 test_loss: 0.09229 \n",
      "[234/300] train_loss: 0.06481 valid_loss: 0.07927 test_loss: 0.09264 \n",
      "[235/300] train_loss: 0.06531 valid_loss: 0.07862 test_loss: 0.09218 \n",
      "[236/300] train_loss: 0.06606 valid_loss: 0.08010 test_loss: 0.09187 \n",
      "[237/300] train_loss: 0.06617 valid_loss: 0.07971 test_loss: 0.09320 \n",
      "[238/300] train_loss: 0.06579 valid_loss: 0.07821 test_loss: 0.09016 \n",
      "[239/300] train_loss: 0.06505 valid_loss: 0.07812 test_loss: 0.09170 \n",
      "[240/300] train_loss: 0.06518 valid_loss: 0.07909 test_loss: 0.09086 \n",
      "[241/300] train_loss: 0.06395 valid_loss: 0.07977 test_loss: 0.09245 \n",
      "[242/300] train_loss: 0.06459 valid_loss: 0.07975 test_loss: 0.09294 \n",
      "[243/300] train_loss: 0.06454 valid_loss: 0.08033 test_loss: 0.09276 \n",
      "[244/300] train_loss: 0.06535 valid_loss: 0.07788 test_loss: 0.09118 \n",
      "Validation loss decreased (0.077981 --> 0.077884).  Saving model ...\n",
      "[245/300] train_loss: 0.06486 valid_loss: 0.07967 test_loss: 0.09090 \n",
      "[246/300] train_loss: 0.06480 valid_loss: 0.07869 test_loss: 0.09015 \n",
      "[247/300] train_loss: 0.06551 valid_loss: 0.07865 test_loss: 0.09101 \n",
      "[248/300] train_loss: 0.06456 valid_loss: 0.07885 test_loss: 0.09103 \n",
      "[249/300] train_loss: 0.06416 valid_loss: 0.07934 test_loss: 0.08966 \n",
      "[250/300] train_loss: 0.06440 valid_loss: 0.07734 test_loss: 0.08924 \n",
      "Validation loss decreased (0.077884 --> 0.077338).  Saving model ...\n",
      "[251/300] train_loss: 0.06419 valid_loss: 0.07954 test_loss: 0.09184 \n",
      "[252/300] train_loss: 0.06438 valid_loss: 0.08002 test_loss: 0.09223 \n",
      "[253/300] train_loss: 0.06278 valid_loss: 0.07781 test_loss: 0.09062 \n",
      "[254/300] train_loss: 0.06488 valid_loss: 0.07672 test_loss: 0.08904 \n",
      "Validation loss decreased (0.077338 --> 0.076720).  Saving model ...\n",
      "[255/300] train_loss: 0.06508 valid_loss: 0.07878 test_loss: 0.09148 \n",
      "[256/300] train_loss: 0.06207 valid_loss: 0.07784 test_loss: 0.09098 \n",
      "[257/300] train_loss: 0.06216 valid_loss: 0.07796 test_loss: 0.09073 \n",
      "[258/300] train_loss: 0.06371 valid_loss: 0.07706 test_loss: 0.08940 \n",
      "[259/300] train_loss: 0.06326 valid_loss: 0.07622 test_loss: 0.08998 \n",
      "Validation loss decreased (0.076720 --> 0.076222).  Saving model ...\n",
      "[260/300] train_loss: 0.06309 valid_loss: 0.07710 test_loss: 0.09071 \n",
      "[261/300] train_loss: 0.06385 valid_loss: 0.07820 test_loss: 0.09147 \n",
      "[262/300] train_loss: 0.06565 valid_loss: 0.07679 test_loss: 0.09103 \n",
      "[263/300] train_loss: 0.06378 valid_loss: 0.07869 test_loss: 0.09187 \n",
      "[264/300] train_loss: 0.06180 valid_loss: 0.07785 test_loss: 0.09117 \n",
      "[265/300] train_loss: 0.06268 valid_loss: 0.07662 test_loss: 0.08948 \n",
      "[266/300] train_loss: 0.06378 valid_loss: 0.07692 test_loss: 0.08954 \n",
      "[267/300] train_loss: 0.06380 valid_loss: 0.07732 test_loss: 0.09039 \n",
      "[268/300] train_loss: 0.06289 valid_loss: 0.07610 test_loss: 0.08951 \n",
      "Validation loss decreased (0.076222 --> 0.076103).  Saving model ...\n",
      "[269/300] train_loss: 0.06206 valid_loss: 0.07682 test_loss: 0.09020 \n",
      "[270/300] train_loss: 0.06391 valid_loss: 0.07584 test_loss: 0.08931 \n",
      "Validation loss decreased (0.076103 --> 0.075845).  Saving model ...\n",
      "[271/300] train_loss: 0.06279 valid_loss: 0.07732 test_loss: 0.08964 \n",
      "[272/300] train_loss: 0.06240 valid_loss: 0.07912 test_loss: 0.09003 \n",
      "[273/300] train_loss: 0.06405 valid_loss: 0.07592 test_loss: 0.08911 \n",
      "[274/300] train_loss: 0.06160 valid_loss: 0.07870 test_loss: 0.08996 \n",
      "[275/300] train_loss: 0.06340 valid_loss: 0.07722 test_loss: 0.08905 \n",
      "[276/300] train_loss: 0.06282 valid_loss: 0.07824 test_loss: 0.09048 \n",
      "[277/300] train_loss: 0.06199 valid_loss: 0.07673 test_loss: 0.08937 \n",
      "[278/300] train_loss: 0.06403 valid_loss: 0.07725 test_loss: 0.08891 \n",
      "[279/300] train_loss: 0.06281 valid_loss: 0.07605 test_loss: 0.08912 \n",
      "[280/300] train_loss: 0.06010 valid_loss: 0.07729 test_loss: 0.08972 \n",
      "[281/300] train_loss: 0.06302 valid_loss: 0.07511 test_loss: 0.08859 \n",
      "Validation loss decreased (0.075845 --> 0.075112).  Saving model ...\n",
      "[282/300] train_loss: 0.06169 valid_loss: 0.07506 test_loss: 0.08761 \n",
      "Validation loss decreased (0.075112 --> 0.075062).  Saving model ...\n",
      "[283/300] train_loss: 0.06117 valid_loss: 0.07691 test_loss: 0.09015 \n",
      "[284/300] train_loss: 0.06148 valid_loss: 0.07756 test_loss: 0.08996 \n",
      "[285/300] train_loss: 0.06248 valid_loss: 0.07726 test_loss: 0.08945 \n",
      "[286/300] train_loss: 0.06377 valid_loss: 0.07628 test_loss: 0.08921 \n",
      "[287/300] train_loss: 0.06046 valid_loss: 0.07797 test_loss: 0.09012 \n",
      "[288/300] train_loss: 0.06131 valid_loss: 0.07659 test_loss: 0.08812 \n",
      "[289/300] train_loss: 0.06174 valid_loss: 0.07642 test_loss: 0.08875 \n",
      "[290/300] train_loss: 0.06444 valid_loss: 0.07637 test_loss: 0.08836 \n",
      "[291/300] train_loss: 0.06392 valid_loss: 0.07556 test_loss: 0.08878 \n",
      "[292/300] train_loss: 0.05998 valid_loss: 0.07621 test_loss: 0.08743 \n",
      "[293/300] train_loss: 0.06321 valid_loss: 0.07760 test_loss: 0.08878 \n",
      "[294/300] train_loss: 0.06165 valid_loss: 0.07623 test_loss: 0.08843 \n",
      "[295/300] train_loss: 0.06052 valid_loss: 0.07525 test_loss: 0.08868 \n",
      "[296/300] train_loss: 0.06101 valid_loss: 0.07591 test_loss: 0.08929 \n",
      "[297/300] train_loss: 0.06138 valid_loss: 0.07657 test_loss: 0.08842 \n",
      "[298/300] train_loss: 0.06282 valid_loss: 0.07622 test_loss: 0.08909 \n",
      "[299/300] train_loss: 0.06259 valid_loss: 0.07790 test_loss: 0.09012 \n",
      "[300/300] train_loss: 0.06293 valid_loss: 0.07472 test_loss: 0.08900 \n",
      "Validation loss decreased (0.075062 --> 0.074723).  Saving model ...\n",
      "TRAINING MODEL 12\n",
      "[  1/300] train_loss: 0.54778 valid_loss: 0.47063 test_loss: 0.47516 \n",
      "Validation loss decreased (inf --> 0.470626).  Saving model ...\n",
      "[  2/300] train_loss: 0.37361 valid_loss: 0.35019 test_loss: 0.35763 \n",
      "Validation loss decreased (0.470626 --> 0.350187).  Saving model ...\n",
      "[  3/300] train_loss: 0.28541 valid_loss: 0.28162 test_loss: 0.29209 \n",
      "Validation loss decreased (0.350187 --> 0.281624).  Saving model ...\n",
      "[  4/300] train_loss: 0.24193 valid_loss: 0.24254 test_loss: 0.25622 \n",
      "Validation loss decreased (0.281624 --> 0.242535).  Saving model ...\n",
      "[  5/300] train_loss: 0.21304 valid_loss: 0.21982 test_loss: 0.23595 \n",
      "Validation loss decreased (0.242535 --> 0.219816).  Saving model ...\n",
      "[  6/300] train_loss: 0.19889 valid_loss: 0.19979 test_loss: 0.21453 \n",
      "Validation loss decreased (0.219816 --> 0.199786).  Saving model ...\n",
      "[  7/300] train_loss: 0.18169 valid_loss: 0.18636 test_loss: 0.20250 \n",
      "Validation loss decreased (0.199786 --> 0.186357).  Saving model ...\n",
      "[  8/300] train_loss: 0.18131 valid_loss: 0.17869 test_loss: 0.19116 \n",
      "Validation loss decreased (0.186357 --> 0.178694).  Saving model ...\n",
      "[  9/300] train_loss: 0.17271 valid_loss: 0.17104 test_loss: 0.18373 \n",
      "Validation loss decreased (0.178694 --> 0.171037).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16299 valid_loss: 0.16324 test_loss: 0.17562 \n",
      "Validation loss decreased (0.171037 --> 0.163241).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15653 valid_loss: 0.15766 test_loss: 0.17019 \n",
      "Validation loss decreased (0.163241 --> 0.157663).  Saving model ...\n",
      "[ 12/300] train_loss: 0.14966 valid_loss: 0.15056 test_loss: 0.16496 \n",
      "Validation loss decreased (0.157663 --> 0.150562).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14549 valid_loss: 0.14711 test_loss: 0.16126 \n",
      "Validation loss decreased (0.150562 --> 0.147105).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14157 valid_loss: 0.14668 test_loss: 0.16217 \n",
      "Validation loss decreased (0.147105 --> 0.146677).  Saving model ...\n",
      "[ 15/300] train_loss: 0.13869 valid_loss: 0.14166 test_loss: 0.15439 \n",
      "Validation loss decreased (0.146677 --> 0.141664).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13425 valid_loss: 0.13811 test_loss: 0.15476 \n",
      "Validation loss decreased (0.141664 --> 0.138109).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13211 valid_loss: 0.13764 test_loss: 0.15420 \n",
      "Validation loss decreased (0.138109 --> 0.137637).  Saving model ...\n",
      "[ 18/300] train_loss: 0.12791 valid_loss: 0.13307 test_loss: 0.14711 \n",
      "Validation loss decreased (0.137637 --> 0.133075).  Saving model ...\n",
      "[ 19/300] train_loss: 0.12789 valid_loss: 0.13067 test_loss: 0.14531 \n",
      "Validation loss decreased (0.133075 --> 0.130675).  Saving model ...\n",
      "[ 20/300] train_loss: 0.12517 valid_loss: 0.12760 test_loss: 0.14317 \n",
      "Validation loss decreased (0.130675 --> 0.127596).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12492 valid_loss: 0.12444 test_loss: 0.13984 \n",
      "Validation loss decreased (0.127596 --> 0.124443).  Saving model ...\n",
      "[ 22/300] train_loss: 0.11916 valid_loss: 0.12498 test_loss: 0.14046 \n",
      "[ 23/300] train_loss: 0.12301 valid_loss: 0.12304 test_loss: 0.13815 \n",
      "Validation loss decreased (0.124443 --> 0.123043).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12058 valid_loss: 0.12462 test_loss: 0.13839 \n",
      "[ 25/300] train_loss: 0.12024 valid_loss: 0.12448 test_loss: 0.13813 \n",
      "[ 26/300] train_loss: 0.11607 valid_loss: 0.11917 test_loss: 0.13292 \n",
      "Validation loss decreased (0.123043 --> 0.119173).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11445 valid_loss: 0.12125 test_loss: 0.13605 \n",
      "[ 28/300] train_loss: 0.11363 valid_loss: 0.11767 test_loss: 0.13240 \n",
      "Validation loss decreased (0.119173 --> 0.117667).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11483 valid_loss: 0.11746 test_loss: 0.13077 \n",
      "Validation loss decreased (0.117667 --> 0.117460).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11103 valid_loss: 0.11507 test_loss: 0.12962 \n",
      "Validation loss decreased (0.117460 --> 0.115070).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11383 valid_loss: 0.11495 test_loss: 0.12918 \n",
      "Validation loss decreased (0.115070 --> 0.114953).  Saving model ...\n",
      "[ 32/300] train_loss: 0.10912 valid_loss: 0.11323 test_loss: 0.12716 \n",
      "Validation loss decreased (0.114953 --> 0.113229).  Saving model ...\n",
      "[ 33/300] train_loss: 0.10689 valid_loss: 0.11475 test_loss: 0.12702 \n",
      "[ 34/300] train_loss: 0.10749 valid_loss: 0.11202 test_loss: 0.12516 \n",
      "Validation loss decreased (0.113229 --> 0.112024).  Saving model ...\n",
      "[ 35/300] train_loss: 0.10817 valid_loss: 0.11513 test_loss: 0.12674 \n",
      "[ 36/300] train_loss: 0.10600 valid_loss: 0.11391 test_loss: 0.12686 \n",
      "[ 37/300] train_loss: 0.10589 valid_loss: 0.11277 test_loss: 0.12511 \n",
      "[ 38/300] train_loss: 0.10510 valid_loss: 0.11354 test_loss: 0.12676 \n",
      "[ 39/300] train_loss: 0.10613 valid_loss: 0.10780 test_loss: 0.12169 \n",
      "Validation loss decreased (0.112024 --> 0.107802).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10554 valid_loss: 0.10962 test_loss: 0.12343 \n",
      "[ 41/300] train_loss: 0.10412 valid_loss: 0.11153 test_loss: 0.12277 \n",
      "[ 42/300] train_loss: 0.10224 valid_loss: 0.10791 test_loss: 0.12132 \n",
      "[ 43/300] train_loss: 0.10193 valid_loss: 0.10948 test_loss: 0.12245 \n",
      "[ 44/300] train_loss: 0.10163 valid_loss: 0.10678 test_loss: 0.12054 \n",
      "Validation loss decreased (0.107802 --> 0.106781).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10215 valid_loss: 0.11056 test_loss: 0.12122 \n",
      "[ 46/300] train_loss: 0.10167 valid_loss: 0.10474 test_loss: 0.11818 \n",
      "Validation loss decreased (0.106781 --> 0.104739).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10083 valid_loss: 0.10593 test_loss: 0.11810 \n",
      "[ 48/300] train_loss: 0.09943 valid_loss: 0.10550 test_loss: 0.11820 \n",
      "[ 49/300] train_loss: 0.09882 valid_loss: 0.10402 test_loss: 0.11730 \n",
      "Validation loss decreased (0.104739 --> 0.104023).  Saving model ...\n",
      "[ 50/300] train_loss: 0.09899 valid_loss: 0.10406 test_loss: 0.11718 \n",
      "[ 51/300] train_loss: 0.09705 valid_loss: 0.10340 test_loss: 0.11670 \n",
      "Validation loss decreased (0.104023 --> 0.103396).  Saving model ...\n",
      "[ 52/300] train_loss: 0.09741 valid_loss: 0.10340 test_loss: 0.11668 \n",
      "[ 53/300] train_loss: 0.09767 valid_loss: 0.10545 test_loss: 0.11612 \n",
      "[ 54/300] train_loss: 0.09863 valid_loss: 0.10307 test_loss: 0.11579 \n",
      "Validation loss decreased (0.103396 --> 0.103073).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09454 valid_loss: 0.10229 test_loss: 0.11625 \n",
      "Validation loss decreased (0.103073 --> 0.102291).  Saving model ...\n",
      "[ 56/300] train_loss: 0.09201 valid_loss: 0.10123 test_loss: 0.11474 \n",
      "Validation loss decreased (0.102291 --> 0.101229).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09493 valid_loss: 0.10330 test_loss: 0.11614 \n",
      "[ 58/300] train_loss: 0.09591 valid_loss: 0.10062 test_loss: 0.11333 \n",
      "Validation loss decreased (0.101229 --> 0.100625).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09658 valid_loss: 0.10094 test_loss: 0.11377 \n",
      "[ 60/300] train_loss: 0.09467 valid_loss: 0.10184 test_loss: 0.11230 \n",
      "[ 61/300] train_loss: 0.09262 valid_loss: 0.10348 test_loss: 0.11402 \n",
      "[ 62/300] train_loss: 0.09509 valid_loss: 0.10132 test_loss: 0.11265 \n",
      "[ 63/300] train_loss: 0.09426 valid_loss: 0.09947 test_loss: 0.10991 \n",
      "Validation loss decreased (0.100625 --> 0.099468).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09038 valid_loss: 0.10044 test_loss: 0.11293 \n",
      "[ 65/300] train_loss: 0.09212 valid_loss: 0.09885 test_loss: 0.11198 \n",
      "Validation loss decreased (0.099468 --> 0.098847).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09205 valid_loss: 0.09966 test_loss: 0.11157 \n",
      "[ 67/300] train_loss: 0.09144 valid_loss: 0.09836 test_loss: 0.11096 \n",
      "Validation loss decreased (0.098847 --> 0.098357).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09019 valid_loss: 0.09829 test_loss: 0.11149 \n",
      "Validation loss decreased (0.098357 --> 0.098290).  Saving model ...\n",
      "[ 69/300] train_loss: 0.08949 valid_loss: 0.09911 test_loss: 0.11064 \n",
      "[ 70/300] train_loss: 0.08938 valid_loss: 0.09816 test_loss: 0.11165 \n",
      "Validation loss decreased (0.098290 --> 0.098155).  Saving model ...\n",
      "[ 71/300] train_loss: 0.08951 valid_loss: 0.09897 test_loss: 0.10971 \n",
      "[ 72/300] train_loss: 0.08891 valid_loss: 0.09725 test_loss: 0.10964 \n",
      "Validation loss decreased (0.098155 --> 0.097251).  Saving model ...\n",
      "[ 73/300] train_loss: 0.08939 valid_loss: 0.09526 test_loss: 0.10694 \n",
      "Validation loss decreased (0.097251 --> 0.095261).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09031 valid_loss: 0.09842 test_loss: 0.10981 \n",
      "[ 75/300] train_loss: 0.09025 valid_loss: 0.09831 test_loss: 0.10948 \n",
      "[ 76/300] train_loss: 0.08895 valid_loss: 0.09750 test_loss: 0.10879 \n",
      "[ 77/300] train_loss: 0.08832 valid_loss: 0.09573 test_loss: 0.10699 \n",
      "[ 78/300] train_loss: 0.08716 valid_loss: 0.09335 test_loss: 0.10675 \n",
      "Validation loss decreased (0.095261 --> 0.093346).  Saving model ...\n",
      "[ 79/300] train_loss: 0.08978 valid_loss: 0.09557 test_loss: 0.10699 \n",
      "[ 80/300] train_loss: 0.08774 valid_loss: 0.09406 test_loss: 0.10608 \n",
      "[ 81/300] train_loss: 0.08591 valid_loss: 0.09606 test_loss: 0.10756 \n",
      "[ 82/300] train_loss: 0.08745 valid_loss: 0.09503 test_loss: 0.10537 \n",
      "[ 83/300] train_loss: 0.08652 valid_loss: 0.09269 test_loss: 0.10630 \n",
      "Validation loss decreased (0.093346 --> 0.092689).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08612 valid_loss: 0.09478 test_loss: 0.10601 \n",
      "[ 85/300] train_loss: 0.08537 valid_loss: 0.09586 test_loss: 0.10613 \n",
      "[ 86/300] train_loss: 0.08563 valid_loss: 0.09296 test_loss: 0.10525 \n",
      "[ 87/300] train_loss: 0.08434 valid_loss: 0.09458 test_loss: 0.10604 \n",
      "[ 88/300] train_loss: 0.08602 valid_loss: 0.09364 test_loss: 0.10572 \n",
      "[ 89/300] train_loss: 0.08787 valid_loss: 0.09551 test_loss: 0.10584 \n",
      "[ 90/300] train_loss: 0.08598 valid_loss: 0.09257 test_loss: 0.10392 \n",
      "Validation loss decreased (0.092689 --> 0.092565).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08439 valid_loss: 0.09291 test_loss: 0.10548 \n",
      "[ 92/300] train_loss: 0.08323 valid_loss: 0.09304 test_loss: 0.10425 \n",
      "[ 93/300] train_loss: 0.08311 valid_loss: 0.09079 test_loss: 0.10353 \n",
      "Validation loss decreased (0.092565 --> 0.090789).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08300 valid_loss: 0.09226 test_loss: 0.10268 \n",
      "[ 95/300] train_loss: 0.08157 valid_loss: 0.09247 test_loss: 0.10445 \n",
      "[ 96/300] train_loss: 0.08349 valid_loss: 0.09169 test_loss: 0.10232 \n",
      "[ 97/300] train_loss: 0.08416 valid_loss: 0.09103 test_loss: 0.10201 \n",
      "[ 98/300] train_loss: 0.08163 valid_loss: 0.09216 test_loss: 0.10290 \n",
      "[ 99/300] train_loss: 0.08245 valid_loss: 0.09252 test_loss: 0.10314 \n",
      "[100/300] train_loss: 0.08250 valid_loss: 0.09073 test_loss: 0.10228 \n",
      "Validation loss decreased (0.090789 --> 0.090727).  Saving model ...\n",
      "[101/300] train_loss: 0.08247 valid_loss: 0.09103 test_loss: 0.10363 \n",
      "[102/300] train_loss: 0.08223 valid_loss: 0.09042 test_loss: 0.10249 \n",
      "Validation loss decreased (0.090727 --> 0.090424).  Saving model ...\n",
      "[103/300] train_loss: 0.08330 valid_loss: 0.09112 test_loss: 0.10338 \n",
      "[104/300] train_loss: 0.08025 valid_loss: 0.09043 test_loss: 0.10188 \n",
      "[105/300] train_loss: 0.08245 valid_loss: 0.08948 test_loss: 0.10169 \n",
      "Validation loss decreased (0.090424 --> 0.089477).  Saving model ...\n",
      "[106/300] train_loss: 0.08152 valid_loss: 0.08893 test_loss: 0.10253 \n",
      "Validation loss decreased (0.089477 --> 0.088929).  Saving model ...\n",
      "[107/300] train_loss: 0.08247 valid_loss: 0.08909 test_loss: 0.10090 \n",
      "[108/300] train_loss: 0.08152 valid_loss: 0.09398 test_loss: 0.10446 \n",
      "[109/300] train_loss: 0.08137 valid_loss: 0.08934 test_loss: 0.10001 \n",
      "[110/300] train_loss: 0.08257 valid_loss: 0.09176 test_loss: 0.10334 \n",
      "[111/300] train_loss: 0.07943 valid_loss: 0.09165 test_loss: 0.10261 \n",
      "[112/300] train_loss: 0.07936 valid_loss: 0.09077 test_loss: 0.10173 \n",
      "[113/300] train_loss: 0.07845 valid_loss: 0.08962 test_loss: 0.10093 \n",
      "[114/300] train_loss: 0.07952 valid_loss: 0.08839 test_loss: 0.10126 \n",
      "Validation loss decreased (0.088929 --> 0.088391).  Saving model ...\n",
      "[115/300] train_loss: 0.07884 valid_loss: 0.09011 test_loss: 0.10009 \n",
      "[116/300] train_loss: 0.08009 valid_loss: 0.09092 test_loss: 0.10222 \n",
      "[117/300] train_loss: 0.07870 valid_loss: 0.09022 test_loss: 0.10165 \n",
      "[118/300] train_loss: 0.07757 valid_loss: 0.08872 test_loss: 0.10045 \n",
      "[119/300] train_loss: 0.07768 valid_loss: 0.08769 test_loss: 0.10003 \n",
      "Validation loss decreased (0.088391 --> 0.087689).  Saving model ...\n",
      "[120/300] train_loss: 0.08039 valid_loss: 0.08914 test_loss: 0.10117 \n",
      "[121/300] train_loss: 0.07757 valid_loss: 0.08773 test_loss: 0.09966 \n",
      "[122/300] train_loss: 0.07905 valid_loss: 0.08712 test_loss: 0.09993 \n",
      "Validation loss decreased (0.087689 --> 0.087124).  Saving model ...\n",
      "[123/300] train_loss: 0.07896 valid_loss: 0.08941 test_loss: 0.10072 \n",
      "[124/300] train_loss: 0.07625 valid_loss: 0.08953 test_loss: 0.10090 \n",
      "[125/300] train_loss: 0.07755 valid_loss: 0.08747 test_loss: 0.10026 \n",
      "[126/300] train_loss: 0.07611 valid_loss: 0.08812 test_loss: 0.09950 \n",
      "[127/300] train_loss: 0.07788 valid_loss: 0.08700 test_loss: 0.09967 \n",
      "Validation loss decreased (0.087124 --> 0.087003).  Saving model ...\n",
      "[128/300] train_loss: 0.07566 valid_loss: 0.08712 test_loss: 0.09996 \n",
      "[129/300] train_loss: 0.07625 valid_loss: 0.08731 test_loss: 0.09954 \n",
      "[130/300] train_loss: 0.07784 valid_loss: 0.08847 test_loss: 0.09978 \n",
      "[131/300] train_loss: 0.07733 valid_loss: 0.08593 test_loss: 0.09794 \n",
      "Validation loss decreased (0.087003 --> 0.085930).  Saving model ...\n",
      "[132/300] train_loss: 0.07610 valid_loss: 0.08617 test_loss: 0.09894 \n",
      "[133/300] train_loss: 0.07660 valid_loss: 0.08786 test_loss: 0.09916 \n",
      "[134/300] train_loss: 0.07637 valid_loss: 0.08479 test_loss: 0.09730 \n",
      "Validation loss decreased (0.085930 --> 0.084787).  Saving model ...\n",
      "[135/300] train_loss: 0.07593 valid_loss: 0.08702 test_loss: 0.09940 \n",
      "[136/300] train_loss: 0.07593 valid_loss: 0.08787 test_loss: 0.09972 \n",
      "[137/300] train_loss: 0.07421 valid_loss: 0.08784 test_loss: 0.09844 \n",
      "[138/300] train_loss: 0.07443 valid_loss: 0.08504 test_loss: 0.09755 \n",
      "[139/300] train_loss: 0.07608 valid_loss: 0.08608 test_loss: 0.09757 \n",
      "[140/300] train_loss: 0.07589 valid_loss: 0.08563 test_loss: 0.09864 \n",
      "[141/300] train_loss: 0.07579 valid_loss: 0.08497 test_loss: 0.09687 \n",
      "[142/300] train_loss: 0.07717 valid_loss: 0.08508 test_loss: 0.09721 \n",
      "[143/300] train_loss: 0.07295 valid_loss: 0.08503 test_loss: 0.09882 \n",
      "[144/300] train_loss: 0.07453 valid_loss: 0.08675 test_loss: 0.09751 \n",
      "[145/300] train_loss: 0.07654 valid_loss: 0.08543 test_loss: 0.09798 \n",
      "[146/300] train_loss: 0.07360 valid_loss: 0.08479 test_loss: 0.09681 \n",
      "[147/300] train_loss: 0.07630 valid_loss: 0.08329 test_loss: 0.09583 \n",
      "Validation loss decreased (0.084787 --> 0.083287).  Saving model ...\n",
      "[148/300] train_loss: 0.07517 valid_loss: 0.08485 test_loss: 0.09699 \n",
      "[149/300] train_loss: 0.07229 valid_loss: 0.08409 test_loss: 0.09674 \n",
      "[150/300] train_loss: 0.07500 valid_loss: 0.08781 test_loss: 0.09762 \n",
      "[151/300] train_loss: 0.07369 valid_loss: 0.08429 test_loss: 0.09669 \n",
      "[152/300] train_loss: 0.07525 valid_loss: 0.08476 test_loss: 0.09527 \n",
      "[153/300] train_loss: 0.07312 valid_loss: 0.08615 test_loss: 0.09805 \n",
      "[154/300] train_loss: 0.07386 valid_loss: 0.08422 test_loss: 0.09652 \n",
      "[155/300] train_loss: 0.07356 valid_loss: 0.08333 test_loss: 0.09594 \n",
      "[156/300] train_loss: 0.07420 valid_loss: 0.08421 test_loss: 0.09708 \n",
      "[157/300] train_loss: 0.07431 valid_loss: 0.08366 test_loss: 0.09587 \n",
      "[158/300] train_loss: 0.07308 valid_loss: 0.08522 test_loss: 0.09802 \n",
      "[159/300] train_loss: 0.07416 valid_loss: 0.08370 test_loss: 0.09687 \n",
      "[160/300] train_loss: 0.07115 valid_loss: 0.08331 test_loss: 0.09584 \n",
      "[161/300] train_loss: 0.07131 valid_loss: 0.08394 test_loss: 0.09697 \n",
      "[162/300] train_loss: 0.07095 valid_loss: 0.08339 test_loss: 0.09592 \n",
      "[163/300] train_loss: 0.07365 valid_loss: 0.08419 test_loss: 0.09611 \n",
      "[164/300] train_loss: 0.07346 valid_loss: 0.08521 test_loss: 0.09597 \n",
      "[165/300] train_loss: 0.07390 valid_loss: 0.08300 test_loss: 0.09478 \n",
      "Validation loss decreased (0.083287 --> 0.083000).  Saving model ...\n",
      "[166/300] train_loss: 0.07347 valid_loss: 0.08405 test_loss: 0.09698 \n",
      "[167/300] train_loss: 0.07299 valid_loss: 0.08384 test_loss: 0.09730 \n",
      "[168/300] train_loss: 0.07021 valid_loss: 0.08302 test_loss: 0.09494 \n",
      "[169/300] train_loss: 0.07162 valid_loss: 0.08455 test_loss: 0.09709 \n",
      "[170/300] train_loss: 0.07218 valid_loss: 0.08262 test_loss: 0.09544 \n",
      "Validation loss decreased (0.083000 --> 0.082624).  Saving model ...\n",
      "[171/300] train_loss: 0.07068 valid_loss: 0.08286 test_loss: 0.09510 \n",
      "[172/300] train_loss: 0.07180 valid_loss: 0.08219 test_loss: 0.09546 \n",
      "Validation loss decreased (0.082624 --> 0.082192).  Saving model ...\n",
      "[173/300] train_loss: 0.07095 valid_loss: 0.08228 test_loss: 0.09572 \n",
      "[174/300] train_loss: 0.07258 valid_loss: 0.08674 test_loss: 0.09722 \n",
      "[175/300] train_loss: 0.06900 valid_loss: 0.08292 test_loss: 0.09473 \n",
      "[176/300] train_loss: 0.07065 valid_loss: 0.08237 test_loss: 0.09471 \n",
      "[177/300] train_loss: 0.07020 valid_loss: 0.08220 test_loss: 0.09388 \n",
      "[178/300] train_loss: 0.06769 valid_loss: 0.08232 test_loss: 0.09530 \n",
      "[179/300] train_loss: 0.07154 valid_loss: 0.08197 test_loss: 0.09536 \n",
      "Validation loss decreased (0.082192 --> 0.081974).  Saving model ...\n",
      "[180/300] train_loss: 0.07019 valid_loss: 0.08152 test_loss: 0.09504 \n",
      "Validation loss decreased (0.081974 --> 0.081521).  Saving model ...\n",
      "[181/300] train_loss: 0.07022 valid_loss: 0.08080 test_loss: 0.09362 \n",
      "Validation loss decreased (0.081521 --> 0.080799).  Saving model ...\n",
      "[182/300] train_loss: 0.06979 valid_loss: 0.08152 test_loss: 0.09524 \n",
      "[183/300] train_loss: 0.07026 valid_loss: 0.08426 test_loss: 0.09607 \n",
      "[184/300] train_loss: 0.06979 valid_loss: 0.08085 test_loss: 0.09434 \n",
      "[185/300] train_loss: 0.06952 valid_loss: 0.08263 test_loss: 0.09430 \n",
      "[186/300] train_loss: 0.06909 valid_loss: 0.08156 test_loss: 0.09375 \n",
      "[187/300] train_loss: 0.07092 valid_loss: 0.08113 test_loss: 0.09357 \n",
      "[188/300] train_loss: 0.06797 valid_loss: 0.08238 test_loss: 0.09534 \n",
      "[189/300] train_loss: 0.06811 valid_loss: 0.08062 test_loss: 0.09317 \n",
      "Validation loss decreased (0.080799 --> 0.080622).  Saving model ...\n",
      "[190/300] train_loss: 0.06996 valid_loss: 0.08253 test_loss: 0.09469 \n",
      "[191/300] train_loss: 0.06976 valid_loss: 0.08091 test_loss: 0.09481 \n",
      "[192/300] train_loss: 0.06951 valid_loss: 0.08163 test_loss: 0.09368 \n",
      "[193/300] train_loss: 0.07012 valid_loss: 0.08101 test_loss: 0.09337 \n",
      "[194/300] train_loss: 0.07001 valid_loss: 0.08160 test_loss: 0.09481 \n",
      "[195/300] train_loss: 0.06775 valid_loss: 0.08214 test_loss: 0.09350 \n",
      "[196/300] train_loss: 0.06635 valid_loss: 0.08140 test_loss: 0.09500 \n",
      "[197/300] train_loss: 0.06643 valid_loss: 0.08252 test_loss: 0.09558 \n",
      "[198/300] train_loss: 0.06805 valid_loss: 0.08070 test_loss: 0.09402 \n",
      "[199/300] train_loss: 0.06549 valid_loss: 0.08097 test_loss: 0.09354 \n",
      "[200/300] train_loss: 0.07077 valid_loss: 0.07991 test_loss: 0.09276 \n",
      "Validation loss decreased (0.080622 --> 0.079907).  Saving model ...\n",
      "[201/300] train_loss: 0.07042 valid_loss: 0.08155 test_loss: 0.09419 \n",
      "[202/300] train_loss: 0.06650 valid_loss: 0.08080 test_loss: 0.09258 \n",
      "[203/300] train_loss: 0.06705 valid_loss: 0.08221 test_loss: 0.09321 \n",
      "[204/300] train_loss: 0.06918 valid_loss: 0.08087 test_loss: 0.09222 \n",
      "[205/300] train_loss: 0.06992 valid_loss: 0.08204 test_loss: 0.09409 \n",
      "[206/300] train_loss: 0.06822 valid_loss: 0.07946 test_loss: 0.09243 \n",
      "Validation loss decreased (0.079907 --> 0.079458).  Saving model ...\n",
      "[207/300] train_loss: 0.06804 valid_loss: 0.08266 test_loss: 0.09315 \n",
      "[208/300] train_loss: 0.06781 valid_loss: 0.07904 test_loss: 0.09263 \n",
      "Validation loss decreased (0.079458 --> 0.079045).  Saving model ...\n",
      "[209/300] train_loss: 0.06922 valid_loss: 0.08089 test_loss: 0.09213 \n",
      "[210/300] train_loss: 0.06629 valid_loss: 0.07832 test_loss: 0.09117 \n",
      "Validation loss decreased (0.079045 --> 0.078322).  Saving model ...\n",
      "[211/300] train_loss: 0.06745 valid_loss: 0.08036 test_loss: 0.09270 \n",
      "[212/300] train_loss: 0.06616 valid_loss: 0.08030 test_loss: 0.09213 \n",
      "[213/300] train_loss: 0.06594 valid_loss: 0.08058 test_loss: 0.09178 \n",
      "[214/300] train_loss: 0.06562 valid_loss: 0.07900 test_loss: 0.09272 \n",
      "[215/300] train_loss: 0.06945 valid_loss: 0.08079 test_loss: 0.09354 \n",
      "[216/300] train_loss: 0.06600 valid_loss: 0.07849 test_loss: 0.09231 \n",
      "[217/300] train_loss: 0.06899 valid_loss: 0.07992 test_loss: 0.09205 \n",
      "[218/300] train_loss: 0.06571 valid_loss: 0.07899 test_loss: 0.09193 \n",
      "[219/300] train_loss: 0.06714 valid_loss: 0.07902 test_loss: 0.09176 \n",
      "[220/300] train_loss: 0.06685 valid_loss: 0.07922 test_loss: 0.09263 \n",
      "[221/300] train_loss: 0.06657 valid_loss: 0.07975 test_loss: 0.09322 \n",
      "[222/300] train_loss: 0.06493 valid_loss: 0.08110 test_loss: 0.09425 \n",
      "[223/300] train_loss: 0.06823 valid_loss: 0.07967 test_loss: 0.09204 \n",
      "[224/300] train_loss: 0.06923 valid_loss: 0.07960 test_loss: 0.09068 \n",
      "[225/300] train_loss: 0.06653 valid_loss: 0.08087 test_loss: 0.09328 \n",
      "[226/300] train_loss: 0.06560 valid_loss: 0.07845 test_loss: 0.09170 \n",
      "[227/300] train_loss: 0.06810 valid_loss: 0.07988 test_loss: 0.09102 \n",
      "[228/300] train_loss: 0.06825 valid_loss: 0.07868 test_loss: 0.09166 \n",
      "[229/300] train_loss: 0.06685 valid_loss: 0.07879 test_loss: 0.09174 \n",
      "[230/300] train_loss: 0.06644 valid_loss: 0.07939 test_loss: 0.09160 \n",
      "[231/300] train_loss: 0.06365 valid_loss: 0.07832 test_loss: 0.09109 \n",
      "Validation loss decreased (0.078322 --> 0.078317).  Saving model ...\n",
      "[232/300] train_loss: 0.06485 valid_loss: 0.07978 test_loss: 0.09267 \n",
      "[233/300] train_loss: 0.06607 valid_loss: 0.07986 test_loss: 0.09211 \n",
      "[234/300] train_loss: 0.06765 valid_loss: 0.07961 test_loss: 0.09231 \n",
      "[235/300] train_loss: 0.06669 valid_loss: 0.07821 test_loss: 0.09129 \n",
      "Validation loss decreased (0.078317 --> 0.078215).  Saving model ...\n",
      "[236/300] train_loss: 0.06491 valid_loss: 0.08027 test_loss: 0.09151 \n",
      "[237/300] train_loss: 0.06646 valid_loss: 0.07822 test_loss: 0.09085 \n",
      "[238/300] train_loss: 0.06459 valid_loss: 0.07896 test_loss: 0.09192 \n",
      "[239/300] train_loss: 0.06386 valid_loss: 0.07968 test_loss: 0.09093 \n",
      "[240/300] train_loss: 0.06346 valid_loss: 0.07887 test_loss: 0.09163 \n",
      "[241/300] train_loss: 0.06447 valid_loss: 0.07926 test_loss: 0.09118 \n",
      "[242/300] train_loss: 0.06467 valid_loss: 0.07923 test_loss: 0.09137 \n",
      "[243/300] train_loss: 0.06557 valid_loss: 0.07798 test_loss: 0.09087 \n",
      "Validation loss decreased (0.078215 --> 0.077981).  Saving model ...\n",
      "[244/300] train_loss: 0.06710 valid_loss: 0.07862 test_loss: 0.09099 \n",
      "[245/300] train_loss: 0.06671 valid_loss: 0.07879 test_loss: 0.09136 \n",
      "[246/300] train_loss: 0.06423 valid_loss: 0.07923 test_loss: 0.09195 \n",
      "[247/300] train_loss: 0.06511 valid_loss: 0.07863 test_loss: 0.08995 \n",
      "[248/300] train_loss: 0.06493 valid_loss: 0.07864 test_loss: 0.09056 \n",
      "[249/300] train_loss: 0.06476 valid_loss: 0.07797 test_loss: 0.09178 \n",
      "Validation loss decreased (0.077981 --> 0.077971).  Saving model ...\n",
      "[250/300] train_loss: 0.06327 valid_loss: 0.07672 test_loss: 0.08965 \n",
      "Validation loss decreased (0.077971 --> 0.076715).  Saving model ...\n",
      "[251/300] train_loss: 0.06335 valid_loss: 0.07853 test_loss: 0.09066 \n",
      "[252/300] train_loss: 0.06532 valid_loss: 0.07829 test_loss: 0.09058 \n",
      "[253/300] train_loss: 0.06544 valid_loss: 0.07716 test_loss: 0.08982 \n",
      "[254/300] train_loss: 0.06356 valid_loss: 0.07792 test_loss: 0.09144 \n",
      "[255/300] train_loss: 0.06331 valid_loss: 0.07676 test_loss: 0.08908 \n",
      "[256/300] train_loss: 0.06350 valid_loss: 0.07687 test_loss: 0.09134 \n",
      "[257/300] train_loss: 0.06257 valid_loss: 0.07856 test_loss: 0.09157 \n",
      "[258/300] train_loss: 0.06173 valid_loss: 0.07899 test_loss: 0.09125 \n",
      "[259/300] train_loss: 0.06330 valid_loss: 0.07755 test_loss: 0.08929 \n",
      "[260/300] train_loss: 0.06157 valid_loss: 0.07774 test_loss: 0.08956 \n",
      "[261/300] train_loss: 0.06461 valid_loss: 0.07835 test_loss: 0.09094 \n",
      "[262/300] train_loss: 0.06411 valid_loss: 0.07849 test_loss: 0.09134 \n",
      "[263/300] train_loss: 0.06414 valid_loss: 0.07798 test_loss: 0.09039 \n",
      "[264/300] train_loss: 0.06548 valid_loss: 0.07614 test_loss: 0.08834 \n",
      "Validation loss decreased (0.076715 --> 0.076141).  Saving model ...\n",
      "[265/300] train_loss: 0.06318 valid_loss: 0.07670 test_loss: 0.08960 \n",
      "[266/300] train_loss: 0.06296 valid_loss: 0.07879 test_loss: 0.09035 \n",
      "[267/300] train_loss: 0.06331 valid_loss: 0.07712 test_loss: 0.09024 \n",
      "[268/300] train_loss: 0.06385 valid_loss: 0.07638 test_loss: 0.08849 \n",
      "[269/300] train_loss: 0.06482 valid_loss: 0.07793 test_loss: 0.08744 \n",
      "[270/300] train_loss: 0.06544 valid_loss: 0.07776 test_loss: 0.08899 \n",
      "[271/300] train_loss: 0.06350 valid_loss: 0.07709 test_loss: 0.08959 \n",
      "[272/300] train_loss: 0.06099 valid_loss: 0.07606 test_loss: 0.08933 \n",
      "Validation loss decreased (0.076141 --> 0.076056).  Saving model ...\n",
      "[273/300] train_loss: 0.06091 valid_loss: 0.07710 test_loss: 0.09109 \n",
      "[274/300] train_loss: 0.06053 valid_loss: 0.07620 test_loss: 0.08919 \n",
      "[275/300] train_loss: 0.06291 valid_loss: 0.07617 test_loss: 0.09016 \n",
      "[276/300] train_loss: 0.06129 valid_loss: 0.07584 test_loss: 0.08840 \n",
      "Validation loss decreased (0.076056 --> 0.075843).  Saving model ...\n",
      "[277/300] train_loss: 0.06377 valid_loss: 0.07566 test_loss: 0.08907 \n",
      "Validation loss decreased (0.075843 --> 0.075664).  Saving model ...\n",
      "[278/300] train_loss: 0.06180 valid_loss: 0.07579 test_loss: 0.08877 \n",
      "[279/300] train_loss: 0.06071 valid_loss: 0.07632 test_loss: 0.08990 \n",
      "[280/300] train_loss: 0.06144 valid_loss: 0.07629 test_loss: 0.08981 \n",
      "[281/300] train_loss: 0.06082 valid_loss: 0.07749 test_loss: 0.09089 \n",
      "[282/300] train_loss: 0.06239 valid_loss: 0.07584 test_loss: 0.08993 \n",
      "[283/300] train_loss: 0.06100 valid_loss: 0.07712 test_loss: 0.09118 \n",
      "[284/300] train_loss: 0.06265 valid_loss: 0.07526 test_loss: 0.08863 \n",
      "Validation loss decreased (0.075664 --> 0.075259).  Saving model ...\n",
      "[285/300] train_loss: 0.06247 valid_loss: 0.07492 test_loss: 0.08860 \n",
      "Validation loss decreased (0.075259 --> 0.074924).  Saving model ...\n",
      "[286/300] train_loss: 0.06343 valid_loss: 0.07498 test_loss: 0.08790 \n",
      "[287/300] train_loss: 0.06213 valid_loss: 0.07569 test_loss: 0.08972 \n",
      "[288/300] train_loss: 0.06030 valid_loss: 0.07526 test_loss: 0.08885 \n",
      "[289/300] train_loss: 0.06158 valid_loss: 0.07650 test_loss: 0.08914 \n",
      "[290/300] train_loss: 0.06136 valid_loss: 0.07717 test_loss: 0.09032 \n",
      "[291/300] train_loss: 0.06096 valid_loss: 0.07535 test_loss: 0.08852 \n",
      "[292/300] train_loss: 0.05924 valid_loss: 0.07642 test_loss: 0.08857 \n",
      "[293/300] train_loss: 0.06046 valid_loss: 0.07521 test_loss: 0.08834 \n",
      "[294/300] train_loss: 0.06057 valid_loss: 0.07783 test_loss: 0.09001 \n",
      "[295/300] train_loss: 0.06177 valid_loss: 0.07651 test_loss: 0.08896 \n",
      "[296/300] train_loss: 0.06208 valid_loss: 0.07591 test_loss: 0.08830 \n",
      "[297/300] train_loss: 0.06077 valid_loss: 0.07593 test_loss: 0.08906 \n",
      "[298/300] train_loss: 0.06222 valid_loss: 0.07648 test_loss: 0.08856 \n",
      "[299/300] train_loss: 0.06176 valid_loss: 0.07535 test_loss: 0.08927 \n",
      "[300/300] train_loss: 0.06126 valid_loss: 0.07698 test_loss: 0.08826 \n",
      "TRAINING MODEL 13\n",
      "[  1/300] train_loss: 0.50287 valid_loss: 0.40530 test_loss: 0.41025 \n",
      "Validation loss decreased (inf --> 0.405295).  Saving model ...\n",
      "[  2/300] train_loss: 0.32794 valid_loss: 0.31679 test_loss: 0.32329 \n",
      "Validation loss decreased (0.405295 --> 0.316792).  Saving model ...\n",
      "[  3/300] train_loss: 0.26584 valid_loss: 0.26049 test_loss: 0.27105 \n",
      "Validation loss decreased (0.316792 --> 0.260493).  Saving model ...\n",
      "[  4/300] train_loss: 0.22264 valid_loss: 0.22819 test_loss: 0.24395 \n",
      "Validation loss decreased (0.260493 --> 0.228192).  Saving model ...\n",
      "[  5/300] train_loss: 0.20558 valid_loss: 0.20432 test_loss: 0.21850 \n",
      "Validation loss decreased (0.228192 --> 0.204323).  Saving model ...\n",
      "[  6/300] train_loss: 0.18854 valid_loss: 0.19353 test_loss: 0.20731 \n",
      "Validation loss decreased (0.204323 --> 0.193526).  Saving model ...\n",
      "[  7/300] train_loss: 0.18050 valid_loss: 0.18297 test_loss: 0.19750 \n",
      "Validation loss decreased (0.193526 --> 0.182970).  Saving model ...\n",
      "[  8/300] train_loss: 0.17058 valid_loss: 0.17618 test_loss: 0.19017 \n",
      "Validation loss decreased (0.182970 --> 0.176180).  Saving model ...\n",
      "[  9/300] train_loss: 0.16714 valid_loss: 0.17278 test_loss: 0.18696 \n",
      "Validation loss decreased (0.176180 --> 0.172779).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16137 valid_loss: 0.16838 test_loss: 0.18038 \n",
      "Validation loss decreased (0.172779 --> 0.168382).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15490 valid_loss: 0.16343 test_loss: 0.17371 \n",
      "Validation loss decreased (0.168382 --> 0.163430).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15189 valid_loss: 0.16304 test_loss: 0.17425 \n",
      "Validation loss decreased (0.163430 --> 0.163044).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14765 valid_loss: 0.16020 test_loss: 0.17039 \n",
      "Validation loss decreased (0.163044 --> 0.160195).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14633 valid_loss: 0.15482 test_loss: 0.16464 \n",
      "Validation loss decreased (0.160195 --> 0.154820).  Saving model ...\n",
      "[ 15/300] train_loss: 0.13834 valid_loss: 0.14953 test_loss: 0.15999 \n",
      "Validation loss decreased (0.154820 --> 0.149533).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14058 valid_loss: 0.15120 test_loss: 0.16221 \n",
      "[ 17/300] train_loss: 0.13355 valid_loss: 0.15029 test_loss: 0.16052 \n",
      "[ 18/300] train_loss: 0.13244 valid_loss: 0.14367 test_loss: 0.15322 \n",
      "Validation loss decreased (0.149533 --> 0.143674).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13178 valid_loss: 0.14351 test_loss: 0.15408 \n",
      "Validation loss decreased (0.143674 --> 0.143507).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13241 valid_loss: 0.14879 test_loss: 0.15579 \n",
      "[ 21/300] train_loss: 0.13184 valid_loss: 0.14150 test_loss: 0.15150 \n",
      "Validation loss decreased (0.143507 --> 0.141496).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12582 valid_loss: 0.13893 test_loss: 0.14896 \n",
      "Validation loss decreased (0.141496 --> 0.138931).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12305 valid_loss: 0.13838 test_loss: 0.15005 \n",
      "Validation loss decreased (0.138931 --> 0.138383).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12294 valid_loss: 0.13477 test_loss: 0.14544 \n",
      "Validation loss decreased (0.138383 --> 0.134767).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12230 valid_loss: 0.13793 test_loss: 0.14615 \n",
      "[ 26/300] train_loss: 0.12146 valid_loss: 0.13307 test_loss: 0.14269 \n",
      "Validation loss decreased (0.134767 --> 0.133069).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11837 valid_loss: 0.12713 test_loss: 0.13892 \n",
      "Validation loss decreased (0.133069 --> 0.127130).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11741 valid_loss: 0.12887 test_loss: 0.13802 \n",
      "[ 29/300] train_loss: 0.11794 valid_loss: 0.13192 test_loss: 0.14134 \n",
      "[ 30/300] train_loss: 0.11828 valid_loss: 0.12745 test_loss: 0.13792 \n",
      "[ 31/300] train_loss: 0.11575 valid_loss: 0.12641 test_loss: 0.13644 \n",
      "Validation loss decreased (0.127130 --> 0.126407).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11510 valid_loss: 0.12394 test_loss: 0.13610 \n",
      "Validation loss decreased (0.126407 --> 0.123936).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11392 valid_loss: 0.12413 test_loss: 0.13497 \n",
      "[ 34/300] train_loss: 0.11198 valid_loss: 0.12547 test_loss: 0.13590 \n",
      "[ 35/300] train_loss: 0.10798 valid_loss: 0.12285 test_loss: 0.13340 \n",
      "Validation loss decreased (0.123936 --> 0.122849).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11035 valid_loss: 0.11951 test_loss: 0.13050 \n",
      "Validation loss decreased (0.122849 --> 0.119506).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11090 valid_loss: 0.12086 test_loss: 0.13401 \n",
      "[ 38/300] train_loss: 0.11213 valid_loss: 0.11800 test_loss: 0.12869 \n",
      "Validation loss decreased (0.119506 --> 0.118000).  Saving model ...\n",
      "[ 39/300] train_loss: 0.10789 valid_loss: 0.11825 test_loss: 0.12927 \n",
      "[ 40/300] train_loss: 0.10546 valid_loss: 0.11821 test_loss: 0.12861 \n",
      "[ 41/300] train_loss: 0.10812 valid_loss: 0.11572 test_loss: 0.12648 \n",
      "Validation loss decreased (0.118000 --> 0.115725).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10645 valid_loss: 0.11693 test_loss: 0.12687 \n",
      "[ 43/300] train_loss: 0.10581 valid_loss: 0.11302 test_loss: 0.12450 \n",
      "Validation loss decreased (0.115725 --> 0.113021).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10154 valid_loss: 0.11201 test_loss: 0.12508 \n",
      "Validation loss decreased (0.113021 --> 0.112013).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10354 valid_loss: 0.11380 test_loss: 0.12386 \n",
      "[ 46/300] train_loss: 0.10524 valid_loss: 0.11497 test_loss: 0.12872 \n",
      "[ 47/300] train_loss: 0.10367 valid_loss: 0.11251 test_loss: 0.12308 \n",
      "[ 48/300] train_loss: 0.10402 valid_loss: 0.11130 test_loss: 0.12216 \n",
      "Validation loss decreased (0.112013 --> 0.111301).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10088 valid_loss: 0.10919 test_loss: 0.12157 \n",
      "Validation loss decreased (0.111301 --> 0.109195).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10396 valid_loss: 0.11066 test_loss: 0.12074 \n",
      "[ 51/300] train_loss: 0.10043 valid_loss: 0.11190 test_loss: 0.12251 \n",
      "[ 52/300] train_loss: 0.10158 valid_loss: 0.11053 test_loss: 0.12206 \n",
      "[ 53/300] train_loss: 0.09930 valid_loss: 0.10788 test_loss: 0.11939 \n",
      "Validation loss decreased (0.109195 --> 0.107880).  Saving model ...\n",
      "[ 54/300] train_loss: 0.09985 valid_loss: 0.10894 test_loss: 0.11960 \n",
      "[ 55/300] train_loss: 0.09920 valid_loss: 0.10840 test_loss: 0.11968 \n",
      "[ 56/300] train_loss: 0.09614 valid_loss: 0.10884 test_loss: 0.11943 \n",
      "[ 57/300] train_loss: 0.10083 valid_loss: 0.10836 test_loss: 0.11918 \n",
      "[ 58/300] train_loss: 0.09950 valid_loss: 0.10636 test_loss: 0.11946 \n",
      "Validation loss decreased (0.107880 --> 0.106356).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09990 valid_loss: 0.10774 test_loss: 0.12009 \n",
      "[ 60/300] train_loss: 0.09747 valid_loss: 0.10830 test_loss: 0.11926 \n",
      "[ 61/300] train_loss: 0.09894 valid_loss: 0.10684 test_loss: 0.11895 \n",
      "[ 62/300] train_loss: 0.09807 valid_loss: 0.10594 test_loss: 0.11771 \n",
      "Validation loss decreased (0.106356 --> 0.105937).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09481 valid_loss: 0.10456 test_loss: 0.11657 \n",
      "Validation loss decreased (0.105937 --> 0.104562).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09756 valid_loss: 0.10310 test_loss: 0.11382 \n",
      "Validation loss decreased (0.104562 --> 0.103096).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09539 valid_loss: 0.10451 test_loss: 0.11444 \n",
      "[ 66/300] train_loss: 0.09432 valid_loss: 0.10773 test_loss: 0.11764 \n",
      "[ 67/300] train_loss: 0.09377 valid_loss: 0.10388 test_loss: 0.11382 \n",
      "[ 68/300] train_loss: 0.09370 valid_loss: 0.10336 test_loss: 0.11434 \n",
      "[ 69/300] train_loss: 0.09459 valid_loss: 0.10315 test_loss: 0.11246 \n",
      "[ 70/300] train_loss: 0.09322 valid_loss: 0.10166 test_loss: 0.11291 \n",
      "Validation loss decreased (0.103096 --> 0.101660).  Saving model ...\n",
      "[ 71/300] train_loss: 0.09324 valid_loss: 0.10282 test_loss: 0.11408 \n",
      "[ 72/300] train_loss: 0.09198 valid_loss: 0.09963 test_loss: 0.11203 \n",
      "Validation loss decreased (0.101660 --> 0.099633).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09338 valid_loss: 0.10178 test_loss: 0.11265 \n",
      "[ 74/300] train_loss: 0.09348 valid_loss: 0.10203 test_loss: 0.11276 \n",
      "[ 75/300] train_loss: 0.09080 valid_loss: 0.10014 test_loss: 0.11165 \n",
      "[ 76/300] train_loss: 0.09398 valid_loss: 0.10000 test_loss: 0.11102 \n",
      "[ 77/300] train_loss: 0.09122 valid_loss: 0.09930 test_loss: 0.11186 \n",
      "Validation loss decreased (0.099633 --> 0.099301).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09038 valid_loss: 0.09895 test_loss: 0.11049 \n",
      "Validation loss decreased (0.099301 --> 0.098954).  Saving model ...\n",
      "[ 79/300] train_loss: 0.08932 valid_loss: 0.10026 test_loss: 0.11055 \n",
      "[ 80/300] train_loss: 0.09051 valid_loss: 0.10291 test_loss: 0.11437 \n",
      "[ 81/300] train_loss: 0.09021 valid_loss: 0.09882 test_loss: 0.10969 \n",
      "Validation loss decreased (0.098954 --> 0.098820).  Saving model ...\n",
      "[ 82/300] train_loss: 0.08991 valid_loss: 0.09893 test_loss: 0.10942 \n",
      "[ 83/300] train_loss: 0.09028 valid_loss: 0.09922 test_loss: 0.11145 \n",
      "[ 84/300] train_loss: 0.08944 valid_loss: 0.09739 test_loss: 0.10729 \n",
      "Validation loss decreased (0.098820 --> 0.097386).  Saving model ...\n",
      "[ 85/300] train_loss: 0.08665 valid_loss: 0.09721 test_loss: 0.10924 \n",
      "Validation loss decreased (0.097386 --> 0.097214).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08709 valid_loss: 0.09828 test_loss: 0.10770 \n",
      "[ 87/300] train_loss: 0.08874 valid_loss: 0.09798 test_loss: 0.10868 \n",
      "[ 88/300] train_loss: 0.08766 valid_loss: 0.09767 test_loss: 0.10763 \n",
      "[ 89/300] train_loss: 0.08858 valid_loss: 0.09806 test_loss: 0.10824 \n",
      "[ 90/300] train_loss: 0.08730 valid_loss: 0.09766 test_loss: 0.10684 \n",
      "[ 91/300] train_loss: 0.08617 valid_loss: 0.09776 test_loss: 0.10754 \n",
      "[ 92/300] train_loss: 0.08749 valid_loss: 0.09639 test_loss: 0.10648 \n",
      "Validation loss decreased (0.097214 --> 0.096395).  Saving model ...\n",
      "[ 93/300] train_loss: 0.08890 valid_loss: 0.09506 test_loss: 0.10494 \n",
      "Validation loss decreased (0.096395 --> 0.095055).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08641 valid_loss: 0.09475 test_loss: 0.10560 \n",
      "Validation loss decreased (0.095055 --> 0.094751).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08850 valid_loss: 0.09730 test_loss: 0.10719 \n",
      "[ 96/300] train_loss: 0.08700 valid_loss: 0.09506 test_loss: 0.10549 \n",
      "[ 97/300] train_loss: 0.08712 valid_loss: 0.09645 test_loss: 0.10617 \n",
      "[ 98/300] train_loss: 0.08405 valid_loss: 0.09680 test_loss: 0.10757 \n",
      "[ 99/300] train_loss: 0.08746 valid_loss: 0.09304 test_loss: 0.10420 \n",
      "Validation loss decreased (0.094751 --> 0.093040).  Saving model ...\n",
      "[100/300] train_loss: 0.08498 valid_loss: 0.09327 test_loss: 0.10348 \n",
      "[101/300] train_loss: 0.08420 valid_loss: 0.09352 test_loss: 0.10512 \n",
      "[102/300] train_loss: 0.08459 valid_loss: 0.09228 test_loss: 0.10314 \n",
      "Validation loss decreased (0.093040 --> 0.092276).  Saving model ...\n",
      "[103/300] train_loss: 0.08387 valid_loss: 0.09360 test_loss: 0.10384 \n",
      "[104/300] train_loss: 0.08423 valid_loss: 0.09311 test_loss: 0.10297 \n",
      "[105/300] train_loss: 0.08598 valid_loss: 0.09311 test_loss: 0.10304 \n",
      "[106/300] train_loss: 0.08415 valid_loss: 0.09500 test_loss: 0.10521 \n",
      "[107/300] train_loss: 0.08439 valid_loss: 0.09291 test_loss: 0.10327 \n",
      "[108/300] train_loss: 0.08317 valid_loss: 0.09190 test_loss: 0.10340 \n",
      "Validation loss decreased (0.092276 --> 0.091897).  Saving model ...\n",
      "[109/300] train_loss: 0.08189 valid_loss: 0.09186 test_loss: 0.10348 \n",
      "Validation loss decreased (0.091897 --> 0.091861).  Saving model ...\n",
      "[110/300] train_loss: 0.08173 valid_loss: 0.09210 test_loss: 0.10218 \n",
      "[111/300] train_loss: 0.08265 valid_loss: 0.09208 test_loss: 0.10320 \n",
      "[112/300] train_loss: 0.08152 valid_loss: 0.09250 test_loss: 0.10327 \n",
      "[113/300] train_loss: 0.08102 valid_loss: 0.09099 test_loss: 0.10223 \n",
      "Validation loss decreased (0.091861 --> 0.090987).  Saving model ...\n",
      "[114/300] train_loss: 0.08294 valid_loss: 0.09043 test_loss: 0.10219 \n",
      "Validation loss decreased (0.090987 --> 0.090433).  Saving model ...\n",
      "[115/300] train_loss: 0.08178 valid_loss: 0.09168 test_loss: 0.10298 \n",
      "[116/300] train_loss: 0.08152 valid_loss: 0.09369 test_loss: 0.10319 \n",
      "[117/300] train_loss: 0.08337 valid_loss: 0.09048 test_loss: 0.10146 \n",
      "[118/300] train_loss: 0.08402 valid_loss: 0.09071 test_loss: 0.10048 \n",
      "[119/300] train_loss: 0.08294 valid_loss: 0.09008 test_loss: 0.10111 \n",
      "Validation loss decreased (0.090433 --> 0.090078).  Saving model ...\n",
      "[120/300] train_loss: 0.08082 valid_loss: 0.09056 test_loss: 0.10175 \n",
      "[121/300] train_loss: 0.08143 valid_loss: 0.09103 test_loss: 0.10082 \n",
      "[122/300] train_loss: 0.08201 valid_loss: 0.09105 test_loss: 0.10099 \n",
      "[123/300] train_loss: 0.08041 valid_loss: 0.08897 test_loss: 0.09964 \n",
      "Validation loss decreased (0.090078 --> 0.088973).  Saving model ...\n",
      "[124/300] train_loss: 0.08238 valid_loss: 0.09028 test_loss: 0.10076 \n",
      "[125/300] train_loss: 0.07937 valid_loss: 0.08833 test_loss: 0.09935 \n",
      "Validation loss decreased (0.088973 --> 0.088333).  Saving model ...\n",
      "[126/300] train_loss: 0.08016 valid_loss: 0.08866 test_loss: 0.09906 \n",
      "[127/300] train_loss: 0.08706 valid_loss: 0.08887 test_loss: 0.09900 \n",
      "[128/300] train_loss: 0.08106 valid_loss: 0.08804 test_loss: 0.10098 \n",
      "Validation loss decreased (0.088333 --> 0.088035).  Saving model ...\n",
      "[129/300] train_loss: 0.08020 valid_loss: 0.08817 test_loss: 0.10003 \n",
      "[130/300] train_loss: 0.07919 valid_loss: 0.09081 test_loss: 0.10132 \n",
      "[131/300] train_loss: 0.07676 valid_loss: 0.08897 test_loss: 0.09959 \n",
      "[132/300] train_loss: 0.08017 valid_loss: 0.08798 test_loss: 0.09949 \n",
      "Validation loss decreased (0.088035 --> 0.087982).  Saving model ...\n",
      "[133/300] train_loss: 0.08121 valid_loss: 0.08627 test_loss: 0.09886 \n",
      "Validation loss decreased (0.087982 --> 0.086270).  Saving model ...\n",
      "[134/300] train_loss: 0.07823 valid_loss: 0.08754 test_loss: 0.09884 \n",
      "[135/300] train_loss: 0.07971 valid_loss: 0.08758 test_loss: 0.09843 \n",
      "[136/300] train_loss: 0.07764 valid_loss: 0.08629 test_loss: 0.09859 \n",
      "[137/300] train_loss: 0.08020 valid_loss: 0.08733 test_loss: 0.09855 \n",
      "[138/300] train_loss: 0.07816 valid_loss: 0.08671 test_loss: 0.09819 \n",
      "[139/300] train_loss: 0.07938 valid_loss: 0.08729 test_loss: 0.09842 \n",
      "[140/300] train_loss: 0.07716 valid_loss: 0.08753 test_loss: 0.09802 \n",
      "[141/300] train_loss: 0.08070 valid_loss: 0.08760 test_loss: 0.09863 \n",
      "[142/300] train_loss: 0.07805 valid_loss: 0.08770 test_loss: 0.09931 \n",
      "[143/300] train_loss: 0.07651 valid_loss: 0.08687 test_loss: 0.09782 \n",
      "[144/300] train_loss: 0.07796 valid_loss: 0.08534 test_loss: 0.09688 \n",
      "Validation loss decreased (0.086270 --> 0.085341).  Saving model ...\n",
      "[145/300] train_loss: 0.07687 valid_loss: 0.08732 test_loss: 0.09839 \n",
      "[146/300] train_loss: 0.07909 valid_loss: 0.08886 test_loss: 0.09761 \n",
      "[147/300] train_loss: 0.07984 valid_loss: 0.08648 test_loss: 0.09659 \n",
      "[148/300] train_loss: 0.07663 valid_loss: 0.08546 test_loss: 0.09691 \n",
      "[149/300] train_loss: 0.07478 valid_loss: 0.08545 test_loss: 0.09749 \n",
      "[150/300] train_loss: 0.07697 valid_loss: 0.08591 test_loss: 0.09688 \n",
      "[151/300] train_loss: 0.07340 valid_loss: 0.08729 test_loss: 0.09765 \n",
      "[152/300] train_loss: 0.07652 valid_loss: 0.08737 test_loss: 0.09833 \n",
      "[153/300] train_loss: 0.07688 valid_loss: 0.08618 test_loss: 0.09786 \n",
      "[154/300] train_loss: 0.07485 valid_loss: 0.08487 test_loss: 0.09765 \n",
      "Validation loss decreased (0.085341 --> 0.084871).  Saving model ...\n",
      "[155/300] train_loss: 0.07596 valid_loss: 0.08573 test_loss: 0.09632 \n",
      "[156/300] train_loss: 0.07681 valid_loss: 0.08502 test_loss: 0.09611 \n",
      "[157/300] train_loss: 0.07520 valid_loss: 0.08514 test_loss: 0.09550 \n",
      "[158/300] train_loss: 0.07514 valid_loss: 0.08446 test_loss: 0.09704 \n",
      "Validation loss decreased (0.084871 --> 0.084456).  Saving model ...\n",
      "[159/300] train_loss: 0.07479 valid_loss: 0.08570 test_loss: 0.09796 \n",
      "[160/300] train_loss: 0.07536 valid_loss: 0.08456 test_loss: 0.09766 \n",
      "[161/300] train_loss: 0.07510 valid_loss: 0.08512 test_loss: 0.09588 \n",
      "[162/300] train_loss: 0.07408 valid_loss: 0.08459 test_loss: 0.09506 \n",
      "[163/300] train_loss: 0.07410 valid_loss: 0.08535 test_loss: 0.09562 \n",
      "[164/300] train_loss: 0.07760 valid_loss: 0.08487 test_loss: 0.09689 \n",
      "[165/300] train_loss: 0.07178 valid_loss: 0.08390 test_loss: 0.09565 \n",
      "Validation loss decreased (0.084456 --> 0.083904).  Saving model ...\n",
      "[166/300] train_loss: 0.07583 valid_loss: 0.08440 test_loss: 0.09642 \n",
      "[167/300] train_loss: 0.07321 valid_loss: 0.08388 test_loss: 0.09560 \n",
      "Validation loss decreased (0.083904 --> 0.083879).  Saving model ...\n",
      "[168/300] train_loss: 0.07447 valid_loss: 0.08336 test_loss: 0.09499 \n",
      "Validation loss decreased (0.083879 --> 0.083364).  Saving model ...\n",
      "[169/300] train_loss: 0.07411 valid_loss: 0.08366 test_loss: 0.09475 \n",
      "[170/300] train_loss: 0.07408 valid_loss: 0.08523 test_loss: 0.09664 \n",
      "[171/300] train_loss: 0.07342 valid_loss: 0.08479 test_loss: 0.09545 \n",
      "[172/300] train_loss: 0.07274 valid_loss: 0.08651 test_loss: 0.09749 \n",
      "[173/300] train_loss: 0.07270 valid_loss: 0.08316 test_loss: 0.09432 \n",
      "Validation loss decreased (0.083364 --> 0.083160).  Saving model ...\n",
      "[174/300] train_loss: 0.07305 valid_loss: 0.08336 test_loss: 0.09457 \n",
      "[175/300] train_loss: 0.07084 valid_loss: 0.08288 test_loss: 0.09410 \n",
      "Validation loss decreased (0.083160 --> 0.082882).  Saving model ...\n",
      "[176/300] train_loss: 0.07516 valid_loss: 0.08363 test_loss: 0.09557 \n",
      "[177/300] train_loss: 0.07117 valid_loss: 0.08293 test_loss: 0.09494 \n",
      "[178/300] train_loss: 0.07100 valid_loss: 0.08368 test_loss: 0.09447 \n",
      "[179/300] train_loss: 0.07399 valid_loss: 0.08343 test_loss: 0.09414 \n",
      "[180/300] train_loss: 0.07297 valid_loss: 0.08440 test_loss: 0.09534 \n",
      "[181/300] train_loss: 0.07038 valid_loss: 0.08440 test_loss: 0.09475 \n",
      "[182/300] train_loss: 0.07104 valid_loss: 0.08285 test_loss: 0.09308 \n",
      "Validation loss decreased (0.082882 --> 0.082853).  Saving model ...\n",
      "[183/300] train_loss: 0.07434 valid_loss: 0.08376 test_loss: 0.09448 \n",
      "[184/300] train_loss: 0.07242 valid_loss: 0.08222 test_loss: 0.09303 \n",
      "Validation loss decreased (0.082853 --> 0.082218).  Saving model ...\n",
      "[185/300] train_loss: 0.07377 valid_loss: 0.08359 test_loss: 0.09518 \n",
      "[186/300] train_loss: 0.07187 valid_loss: 0.08159 test_loss: 0.09314 \n",
      "Validation loss decreased (0.082218 --> 0.081588).  Saving model ...\n",
      "[187/300] train_loss: 0.07057 valid_loss: 0.08138 test_loss: 0.09347 \n",
      "Validation loss decreased (0.081588 --> 0.081383).  Saving model ...\n",
      "[188/300] train_loss: 0.07202 valid_loss: 0.08124 test_loss: 0.09274 \n",
      "Validation loss decreased (0.081383 --> 0.081240).  Saving model ...\n",
      "[189/300] train_loss: 0.07150 valid_loss: 0.08205 test_loss: 0.09278 \n",
      "[190/300] train_loss: 0.07097 valid_loss: 0.08200 test_loss: 0.09260 \n",
      "[191/300] train_loss: 0.06870 valid_loss: 0.08142 test_loss: 0.09259 \n",
      "[192/300] train_loss: 0.07141 valid_loss: 0.08217 test_loss: 0.09321 \n",
      "[193/300] train_loss: 0.07159 valid_loss: 0.08182 test_loss: 0.09413 \n",
      "[194/300] train_loss: 0.07225 valid_loss: 0.08227 test_loss: 0.09297 \n",
      "[195/300] train_loss: 0.07135 valid_loss: 0.08172 test_loss: 0.09232 \n",
      "[196/300] train_loss: 0.07193 valid_loss: 0.08105 test_loss: 0.09229 \n",
      "Validation loss decreased (0.081240 --> 0.081054).  Saving model ...\n",
      "[197/300] train_loss: 0.07036 valid_loss: 0.08067 test_loss: 0.09228 \n",
      "Validation loss decreased (0.081054 --> 0.080666).  Saving model ...\n",
      "[198/300] train_loss: 0.07104 valid_loss: 0.08147 test_loss: 0.09242 \n",
      "[199/300] train_loss: 0.07006 valid_loss: 0.08104 test_loss: 0.09283 \n",
      "[200/300] train_loss: 0.06899 valid_loss: 0.08044 test_loss: 0.09200 \n",
      "Validation loss decreased (0.080666 --> 0.080443).  Saving model ...\n",
      "[201/300] train_loss: 0.07279 valid_loss: 0.08083 test_loss: 0.09169 \n",
      "[202/300] train_loss: 0.07288 valid_loss: 0.08218 test_loss: 0.09165 \n",
      "[203/300] train_loss: 0.07055 valid_loss: 0.08156 test_loss: 0.09223 \n",
      "[204/300] train_loss: 0.06920 valid_loss: 0.08068 test_loss: 0.09119 \n",
      "[205/300] train_loss: 0.06967 valid_loss: 0.08029 test_loss: 0.09194 \n",
      "Validation loss decreased (0.080443 --> 0.080289).  Saving model ...\n",
      "[206/300] train_loss: 0.07058 valid_loss: 0.08141 test_loss: 0.09292 \n",
      "[207/300] train_loss: 0.06842 valid_loss: 0.07975 test_loss: 0.09131 \n",
      "Validation loss decreased (0.080289 --> 0.079753).  Saving model ...\n",
      "[208/300] train_loss: 0.07200 valid_loss: 0.08048 test_loss: 0.09126 \n",
      "[209/300] train_loss: 0.07163 valid_loss: 0.08132 test_loss: 0.09196 \n",
      "[210/300] train_loss: 0.06988 valid_loss: 0.08010 test_loss: 0.09092 \n",
      "[211/300] train_loss: 0.07062 valid_loss: 0.08045 test_loss: 0.09179 \n",
      "[212/300] train_loss: 0.07079 valid_loss: 0.08167 test_loss: 0.09322 \n",
      "[213/300] train_loss: 0.06950 valid_loss: 0.08096 test_loss: 0.09137 \n",
      "[214/300] train_loss: 0.07051 valid_loss: 0.08075 test_loss: 0.09171 \n",
      "[215/300] train_loss: 0.06875 valid_loss: 0.08137 test_loss: 0.09175 \n",
      "[216/300] train_loss: 0.06904 valid_loss: 0.07976 test_loss: 0.09141 \n",
      "[217/300] train_loss: 0.06880 valid_loss: 0.08051 test_loss: 0.09084 \n",
      "[218/300] train_loss: 0.06892 valid_loss: 0.08038 test_loss: 0.09107 \n",
      "[219/300] train_loss: 0.06937 valid_loss: 0.07944 test_loss: 0.08982 \n",
      "Validation loss decreased (0.079753 --> 0.079442).  Saving model ...\n",
      "[220/300] train_loss: 0.06962 valid_loss: 0.08181 test_loss: 0.09166 \n",
      "[221/300] train_loss: 0.06728 valid_loss: 0.07939 test_loss: 0.09088 \n",
      "Validation loss decreased (0.079442 --> 0.079385).  Saving model ...\n",
      "[222/300] train_loss: 0.07084 valid_loss: 0.07865 test_loss: 0.09093 \n",
      "Validation loss decreased (0.079385 --> 0.078653).  Saving model ...\n",
      "[223/300] train_loss: 0.06883 valid_loss: 0.07935 test_loss: 0.09001 \n",
      "[224/300] train_loss: 0.06941 valid_loss: 0.07917 test_loss: 0.09087 \n",
      "[225/300] train_loss: 0.06604 valid_loss: 0.07827 test_loss: 0.08988 \n",
      "Validation loss decreased (0.078653 --> 0.078272).  Saving model ...\n",
      "[226/300] train_loss: 0.06876 valid_loss: 0.07961 test_loss: 0.09194 \n",
      "[227/300] train_loss: 0.06822 valid_loss: 0.08015 test_loss: 0.09138 \n",
      "[228/300] train_loss: 0.06569 valid_loss: 0.07842 test_loss: 0.09035 \n",
      "[229/300] train_loss: 0.06893 valid_loss: 0.07951 test_loss: 0.09045 \n",
      "[230/300] train_loss: 0.07126 valid_loss: 0.07883 test_loss: 0.08993 \n",
      "[231/300] train_loss: 0.06555 valid_loss: 0.08005 test_loss: 0.09026 \n",
      "[232/300] train_loss: 0.06781 valid_loss: 0.07964 test_loss: 0.09015 \n",
      "[233/300] train_loss: 0.06718 valid_loss: 0.08045 test_loss: 0.09154 \n",
      "[234/300] train_loss: 0.06692 valid_loss: 0.07837 test_loss: 0.08842 \n",
      "[235/300] train_loss: 0.06735 valid_loss: 0.07794 test_loss: 0.08923 \n",
      "Validation loss decreased (0.078272 --> 0.077937).  Saving model ...\n",
      "[236/300] train_loss: 0.06695 valid_loss: 0.07932 test_loss: 0.08980 \n",
      "[237/300] train_loss: 0.06782 valid_loss: 0.07966 test_loss: 0.08993 \n",
      "[238/300] train_loss: 0.06803 valid_loss: 0.07920 test_loss: 0.09092 \n",
      "[239/300] train_loss: 0.06814 valid_loss: 0.07842 test_loss: 0.08996 \n",
      "[240/300] train_loss: 0.06622 valid_loss: 0.07972 test_loss: 0.09108 \n",
      "[241/300] train_loss: 0.06627 valid_loss: 0.07827 test_loss: 0.08944 \n",
      "[242/300] train_loss: 0.06851 valid_loss: 0.07800 test_loss: 0.08844 \n",
      "[243/300] train_loss: 0.06711 valid_loss: 0.08124 test_loss: 0.09091 \n",
      "[244/300] train_loss: 0.06762 valid_loss: 0.07810 test_loss: 0.08892 \n",
      "[245/300] train_loss: 0.06648 valid_loss: 0.07898 test_loss: 0.08957 \n",
      "[246/300] train_loss: 0.06738 valid_loss: 0.07947 test_loss: 0.08888 \n",
      "[247/300] train_loss: 0.06737 valid_loss: 0.07855 test_loss: 0.08915 \n",
      "[248/300] train_loss: 0.06680 valid_loss: 0.07745 test_loss: 0.08795 \n",
      "Validation loss decreased (0.077937 --> 0.077454).  Saving model ...\n",
      "[249/300] train_loss: 0.06490 valid_loss: 0.07854 test_loss: 0.08824 \n",
      "[250/300] train_loss: 0.06501 valid_loss: 0.07901 test_loss: 0.08898 \n",
      "[251/300] train_loss: 0.06381 valid_loss: 0.07774 test_loss: 0.08838 \n",
      "[252/300] train_loss: 0.06508 valid_loss: 0.07811 test_loss: 0.08936 \n",
      "[253/300] train_loss: 0.06650 valid_loss: 0.07809 test_loss: 0.08953 \n",
      "[254/300] train_loss: 0.06665 valid_loss: 0.07825 test_loss: 0.08946 \n",
      "[255/300] train_loss: 0.06915 valid_loss: 0.07738 test_loss: 0.08907 \n",
      "Validation loss decreased (0.077454 --> 0.077376).  Saving model ...\n",
      "[256/300] train_loss: 0.06660 valid_loss: 0.07740 test_loss: 0.08852 \n",
      "[257/300] train_loss: 0.06577 valid_loss: 0.07698 test_loss: 0.08846 \n",
      "Validation loss decreased (0.077376 --> 0.076982).  Saving model ...\n",
      "[258/300] train_loss: 0.06613 valid_loss: 0.07778 test_loss: 0.08764 \n",
      "[259/300] train_loss: 0.06620 valid_loss: 0.07786 test_loss: 0.08860 \n",
      "[260/300] train_loss: 0.06487 valid_loss: 0.07885 test_loss: 0.08894 \n",
      "[261/300] train_loss: 0.06521 valid_loss: 0.07692 test_loss: 0.08815 \n",
      "Validation loss decreased (0.076982 --> 0.076917).  Saving model ...\n",
      "[262/300] train_loss: 0.06602 valid_loss: 0.07810 test_loss: 0.08864 \n",
      "[263/300] train_loss: 0.06399 valid_loss: 0.07780 test_loss: 0.08953 \n",
      "[264/300] train_loss: 0.06562 valid_loss: 0.07703 test_loss: 0.08805 \n",
      "[265/300] train_loss: 0.06524 valid_loss: 0.07650 test_loss: 0.08720 \n",
      "Validation loss decreased (0.076917 --> 0.076497).  Saving model ...\n",
      "[266/300] train_loss: 0.06458 valid_loss: 0.07619 test_loss: 0.08774 \n",
      "Validation loss decreased (0.076497 --> 0.076186).  Saving model ...\n",
      "[267/300] train_loss: 0.06732 valid_loss: 0.07607 test_loss: 0.08812 \n",
      "Validation loss decreased (0.076186 --> 0.076067).  Saving model ...\n",
      "[268/300] train_loss: 0.06600 valid_loss: 0.07690 test_loss: 0.08846 \n",
      "[269/300] train_loss: 0.06516 valid_loss: 0.07779 test_loss: 0.08795 \n",
      "[270/300] train_loss: 0.06445 valid_loss: 0.07661 test_loss: 0.08757 \n",
      "[271/300] train_loss: 0.06542 valid_loss: 0.07767 test_loss: 0.08786 \n",
      "[272/300] train_loss: 0.06823 valid_loss: 0.07729 test_loss: 0.08907 \n",
      "[273/300] train_loss: 0.06344 valid_loss: 0.07667 test_loss: 0.08798 \n",
      "[274/300] train_loss: 0.06591 valid_loss: 0.07648 test_loss: 0.08779 \n",
      "[275/300] train_loss: 0.06505 valid_loss: 0.07656 test_loss: 0.08727 \n",
      "[276/300] train_loss: 0.06545 valid_loss: 0.07737 test_loss: 0.08866 \n",
      "[277/300] train_loss: 0.06454 valid_loss: 0.07720 test_loss: 0.08729 \n",
      "[278/300] train_loss: 0.06274 valid_loss: 0.07770 test_loss: 0.08726 \n",
      "[279/300] train_loss: 0.06406 valid_loss: 0.07540 test_loss: 0.08593 \n",
      "Validation loss decreased (0.076067 --> 0.075398).  Saving model ...\n",
      "[280/300] train_loss: 0.06131 valid_loss: 0.07669 test_loss: 0.08724 \n",
      "[281/300] train_loss: 0.06212 valid_loss: 0.07712 test_loss: 0.08801 \n",
      "[282/300] train_loss: 0.06622 valid_loss: 0.07835 test_loss: 0.08945 \n",
      "[283/300] train_loss: 0.06419 valid_loss: 0.07635 test_loss: 0.08751 \n",
      "[284/300] train_loss: 0.06285 valid_loss: 0.07679 test_loss: 0.08633 \n",
      "[285/300] train_loss: 0.06508 valid_loss: 0.07747 test_loss: 0.08688 \n",
      "[286/300] train_loss: 0.06326 valid_loss: 0.07627 test_loss: 0.08591 \n",
      "[287/300] train_loss: 0.06317 valid_loss: 0.07657 test_loss: 0.08637 \n",
      "[288/300] train_loss: 0.06295 valid_loss: 0.07911 test_loss: 0.08862 \n",
      "[289/300] train_loss: 0.06239 valid_loss: 0.07643 test_loss: 0.08604 \n",
      "[290/300] train_loss: 0.06260 valid_loss: 0.07644 test_loss: 0.08726 \n",
      "[291/300] train_loss: 0.06388 valid_loss: 0.07671 test_loss: 0.08742 \n",
      "[292/300] train_loss: 0.06435 valid_loss: 0.07594 test_loss: 0.08715 \n",
      "[293/300] train_loss: 0.06437 valid_loss: 0.07630 test_loss: 0.08730 \n",
      "[294/300] train_loss: 0.06139 valid_loss: 0.07581 test_loss: 0.08706 \n",
      "[295/300] train_loss: 0.06179 valid_loss: 0.07576 test_loss: 0.08684 \n",
      "[296/300] train_loss: 0.06009 valid_loss: 0.07608 test_loss: 0.08763 \n",
      "[297/300] train_loss: 0.06292 valid_loss: 0.07607 test_loss: 0.08802 \n",
      "[298/300] train_loss: 0.06245 valid_loss: 0.07528 test_loss: 0.08674 \n",
      "Validation loss decreased (0.075398 --> 0.075277).  Saving model ...\n",
      "[299/300] train_loss: 0.06480 valid_loss: 0.07560 test_loss: 0.08634 \n",
      "[300/300] train_loss: 0.06257 valid_loss: 0.07696 test_loss: 0.08719 \n",
      "TRAINING MODEL 14\n",
      "[  1/300] train_loss: 0.53746 valid_loss: 0.42840 test_loss: 0.43793 \n",
      "Validation loss decreased (inf --> 0.428396).  Saving model ...\n",
      "[  2/300] train_loss: 0.35019 valid_loss: 0.32544 test_loss: 0.33354 \n",
      "Validation loss decreased (0.428396 --> 0.325437).  Saving model ...\n",
      "[  3/300] train_loss: 0.27017 valid_loss: 0.26616 test_loss: 0.27876 \n",
      "Validation loss decreased (0.325437 --> 0.266157).  Saving model ...\n",
      "[  4/300] train_loss: 0.23209 valid_loss: 0.23242 test_loss: 0.24639 \n",
      "Validation loss decreased (0.266157 --> 0.232423).  Saving model ...\n",
      "[  5/300] train_loss: 0.21016 valid_loss: 0.21221 test_loss: 0.22732 \n",
      "Validation loss decreased (0.232423 --> 0.212208).  Saving model ...\n",
      "[  6/300] train_loss: 0.19355 valid_loss: 0.19347 test_loss: 0.20717 \n",
      "Validation loss decreased (0.212208 --> 0.193474).  Saving model ...\n",
      "[  7/300] train_loss: 0.18334 valid_loss: 0.18195 test_loss: 0.19571 \n",
      "Validation loss decreased (0.193474 --> 0.181953).  Saving model ...\n",
      "[  8/300] train_loss: 0.17216 valid_loss: 0.17057 test_loss: 0.18345 \n",
      "Validation loss decreased (0.181953 --> 0.170566).  Saving model ...\n",
      "[  9/300] train_loss: 0.16739 valid_loss: 0.16494 test_loss: 0.17601 \n",
      "Validation loss decreased (0.170566 --> 0.164938).  Saving model ...\n",
      "[ 10/300] train_loss: 0.15084 valid_loss: 0.16100 test_loss: 0.17162 \n",
      "Validation loss decreased (0.164938 --> 0.160996).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15213 valid_loss: 0.15714 test_loss: 0.16651 \n",
      "Validation loss decreased (0.160996 --> 0.157137).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15041 valid_loss: 0.15333 test_loss: 0.16239 \n",
      "Validation loss decreased (0.157137 --> 0.153331).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14188 valid_loss: 0.14768 test_loss: 0.15944 \n",
      "Validation loss decreased (0.153331 --> 0.147685).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14317 valid_loss: 0.14703 test_loss: 0.15631 \n",
      "Validation loss decreased (0.147685 --> 0.147030).  Saving model ...\n",
      "[ 15/300] train_loss: 0.13745 valid_loss: 0.14568 test_loss: 0.15602 \n",
      "Validation loss decreased (0.147030 --> 0.145680).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13541 valid_loss: 0.14386 test_loss: 0.15510 \n",
      "Validation loss decreased (0.145680 --> 0.143856).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13462 valid_loss: 0.14253 test_loss: 0.15286 \n",
      "Validation loss decreased (0.143856 --> 0.142530).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13168 valid_loss: 0.14180 test_loss: 0.15146 \n",
      "Validation loss decreased (0.142530 --> 0.141801).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13134 valid_loss: 0.14121 test_loss: 0.15013 \n",
      "Validation loss decreased (0.141801 --> 0.141210).  Saving model ...\n",
      "[ 20/300] train_loss: 0.12874 valid_loss: 0.13616 test_loss: 0.14692 \n",
      "Validation loss decreased (0.141210 --> 0.136165).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13167 valid_loss: 0.13836 test_loss: 0.14825 \n",
      "[ 22/300] train_loss: 0.12578 valid_loss: 0.13821 test_loss: 0.14902 \n",
      "[ 23/300] train_loss: 0.12452 valid_loss: 0.13620 test_loss: 0.14486 \n",
      "[ 24/300] train_loss: 0.12653 valid_loss: 0.13917 test_loss: 0.14789 \n",
      "[ 25/300] train_loss: 0.12116 valid_loss: 0.13532 test_loss: 0.14402 \n",
      "Validation loss decreased (0.136165 --> 0.135321).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12324 valid_loss: 0.12924 test_loss: 0.14068 \n",
      "Validation loss decreased (0.135321 --> 0.129244).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12292 valid_loss: 0.12873 test_loss: 0.14057 \n",
      "Validation loss decreased (0.129244 --> 0.128728).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12138 valid_loss: 0.12820 test_loss: 0.13962 \n",
      "Validation loss decreased (0.128728 --> 0.128201).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11733 valid_loss: 0.12778 test_loss: 0.13817 \n",
      "Validation loss decreased (0.128201 --> 0.127782).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11858 valid_loss: 0.12732 test_loss: 0.13733 \n",
      "Validation loss decreased (0.127782 --> 0.127320).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11631 valid_loss: 0.12514 test_loss: 0.13585 \n",
      "Validation loss decreased (0.127320 --> 0.125136).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11465 valid_loss: 0.12132 test_loss: 0.13253 \n",
      "Validation loss decreased (0.125136 --> 0.121320).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11674 valid_loss: 0.12828 test_loss: 0.13750 \n",
      "[ 34/300] train_loss: 0.11642 valid_loss: 0.12574 test_loss: 0.13532 \n",
      "[ 35/300] train_loss: 0.11173 valid_loss: 0.12125 test_loss: 0.13151 \n",
      "Validation loss decreased (0.121320 --> 0.121252).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11298 valid_loss: 0.11934 test_loss: 0.13005 \n",
      "Validation loss decreased (0.121252 --> 0.119336).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11211 valid_loss: 0.12146 test_loss: 0.13075 \n",
      "[ 38/300] train_loss: 0.10697 valid_loss: 0.12099 test_loss: 0.13070 \n",
      "[ 39/300] train_loss: 0.10905 valid_loss: 0.12246 test_loss: 0.13270 \n",
      "[ 40/300] train_loss: 0.10981 valid_loss: 0.12265 test_loss: 0.13135 \n",
      "[ 41/300] train_loss: 0.11199 valid_loss: 0.12000 test_loss: 0.12966 \n",
      "[ 42/300] train_loss: 0.10830 valid_loss: 0.11796 test_loss: 0.13134 \n",
      "Validation loss decreased (0.119336 --> 0.117956).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10625 valid_loss: 0.11697 test_loss: 0.12838 \n",
      "Validation loss decreased (0.117956 --> 0.116971).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10740 valid_loss: 0.11740 test_loss: 0.12844 \n",
      "[ 45/300] train_loss: 0.10365 valid_loss: 0.11618 test_loss: 0.12616 \n",
      "Validation loss decreased (0.116971 --> 0.116181).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10310 valid_loss: 0.11722 test_loss: 0.12680 \n",
      "[ 47/300] train_loss: 0.10353 valid_loss: 0.11592 test_loss: 0.12618 \n",
      "Validation loss decreased (0.116181 --> 0.115922).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10458 valid_loss: 0.11361 test_loss: 0.12449 \n",
      "Validation loss decreased (0.115922 --> 0.113606).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10421 valid_loss: 0.11376 test_loss: 0.12380 \n",
      "[ 50/300] train_loss: 0.10465 valid_loss: 0.11918 test_loss: 0.12763 \n",
      "[ 51/300] train_loss: 0.10150 valid_loss: 0.11439 test_loss: 0.12350 \n",
      "[ 52/300] train_loss: 0.10167 valid_loss: 0.11257 test_loss: 0.12154 \n",
      "Validation loss decreased (0.113606 --> 0.112567).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10037 valid_loss: 0.11460 test_loss: 0.12416 \n",
      "[ 54/300] train_loss: 0.10167 valid_loss: 0.11297 test_loss: 0.12254 \n",
      "[ 55/300] train_loss: 0.10088 valid_loss: 0.11048 test_loss: 0.11920 \n",
      "Validation loss decreased (0.112567 --> 0.110482).  Saving model ...\n",
      "[ 56/300] train_loss: 0.09845 valid_loss: 0.10862 test_loss: 0.11847 \n",
      "Validation loss decreased (0.110482 --> 0.108623).  Saving model ...\n",
      "[ 57/300] train_loss: 0.10159 valid_loss: 0.11003 test_loss: 0.11918 \n",
      "[ 58/300] train_loss: 0.10015 valid_loss: 0.11073 test_loss: 0.11980 \n",
      "[ 59/300] train_loss: 0.09883 valid_loss: 0.11070 test_loss: 0.12004 \n",
      "[ 60/300] train_loss: 0.09833 valid_loss: 0.11074 test_loss: 0.11996 \n",
      "[ 61/300] train_loss: 0.09907 valid_loss: 0.10736 test_loss: 0.11668 \n",
      "Validation loss decreased (0.108623 --> 0.107356).  Saving model ...\n",
      "[ 62/300] train_loss: 0.10098 valid_loss: 0.10792 test_loss: 0.11788 \n",
      "[ 63/300] train_loss: 0.09647 valid_loss: 0.10787 test_loss: 0.11698 \n",
      "[ 64/300] train_loss: 0.09676 valid_loss: 0.10768 test_loss: 0.11676 \n",
      "[ 65/300] train_loss: 0.10011 valid_loss: 0.10461 test_loss: 0.11422 \n",
      "Validation loss decreased (0.107356 --> 0.104605).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09441 valid_loss: 0.10606 test_loss: 0.11629 \n",
      "[ 67/300] train_loss: 0.09560 valid_loss: 0.10321 test_loss: 0.11329 \n",
      "Validation loss decreased (0.104605 --> 0.103213).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09563 valid_loss: 0.10356 test_loss: 0.11421 \n",
      "[ 69/300] train_loss: 0.09447 valid_loss: 0.10678 test_loss: 0.11673 \n",
      "[ 70/300] train_loss: 0.09440 valid_loss: 0.10472 test_loss: 0.11552 \n",
      "[ 71/300] train_loss: 0.09326 valid_loss: 0.10219 test_loss: 0.11413 \n",
      "Validation loss decreased (0.103213 --> 0.102194).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09825 valid_loss: 0.10367 test_loss: 0.11353 \n",
      "[ 73/300] train_loss: 0.09304 valid_loss: 0.10396 test_loss: 0.11481 \n",
      "[ 74/300] train_loss: 0.09396 valid_loss: 0.10376 test_loss: 0.11477 \n",
      "[ 75/300] train_loss: 0.09321 valid_loss: 0.10207 test_loss: 0.11147 \n",
      "Validation loss decreased (0.102194 --> 0.102074).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09269 valid_loss: 0.10525 test_loss: 0.11515 \n",
      "[ 77/300] train_loss: 0.09342 valid_loss: 0.09950 test_loss: 0.11087 \n",
      "Validation loss decreased (0.102074 --> 0.099500).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09349 valid_loss: 0.09996 test_loss: 0.11209 \n",
      "[ 79/300] train_loss: 0.09301 valid_loss: 0.10388 test_loss: 0.11343 \n",
      "[ 80/300] train_loss: 0.08960 valid_loss: 0.10210 test_loss: 0.11231 \n",
      "[ 81/300] train_loss: 0.09138 valid_loss: 0.10056 test_loss: 0.11024 \n",
      "[ 82/300] train_loss: 0.08916 valid_loss: 0.10032 test_loss: 0.11106 \n",
      "[ 83/300] train_loss: 0.09031 valid_loss: 0.09836 test_loss: 0.10980 \n",
      "Validation loss decreased (0.099500 --> 0.098364).  Saving model ...\n",
      "[ 84/300] train_loss: 0.09024 valid_loss: 0.09862 test_loss: 0.10943 \n",
      "[ 85/300] train_loss: 0.08975 valid_loss: 0.09755 test_loss: 0.10814 \n",
      "Validation loss decreased (0.098364 --> 0.097549).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08812 valid_loss: 0.09648 test_loss: 0.10744 \n",
      "Validation loss decreased (0.097549 --> 0.096478).  Saving model ...\n",
      "[ 87/300] train_loss: 0.09016 valid_loss: 0.10182 test_loss: 0.11314 \n",
      "[ 88/300] train_loss: 0.08770 valid_loss: 0.09922 test_loss: 0.10949 \n",
      "[ 89/300] train_loss: 0.09201 valid_loss: 0.09818 test_loss: 0.10797 \n",
      "[ 90/300] train_loss: 0.08675 valid_loss: 0.09675 test_loss: 0.10758 \n",
      "[ 91/300] train_loss: 0.09078 valid_loss: 0.09523 test_loss: 0.10699 \n",
      "Validation loss decreased (0.096478 --> 0.095226).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08938 valid_loss: 0.09628 test_loss: 0.10815 \n",
      "[ 93/300] train_loss: 0.08890 valid_loss: 0.09882 test_loss: 0.10898 \n",
      "[ 94/300] train_loss: 0.08899 valid_loss: 0.09505 test_loss: 0.10716 \n",
      "Validation loss decreased (0.095226 --> 0.095048).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08664 valid_loss: 0.09639 test_loss: 0.10694 \n",
      "[ 96/300] train_loss: 0.08608 valid_loss: 0.09393 test_loss: 0.10593 \n",
      "Validation loss decreased (0.095048 --> 0.093926).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08684 valid_loss: 0.09677 test_loss: 0.10780 \n",
      "[ 98/300] train_loss: 0.08633 valid_loss: 0.09377 test_loss: 0.10539 \n",
      "Validation loss decreased (0.093926 --> 0.093773).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08587 valid_loss: 0.09518 test_loss: 0.10696 \n",
      "[100/300] train_loss: 0.08627 valid_loss: 0.09476 test_loss: 0.10609 \n",
      "[101/300] train_loss: 0.08826 valid_loss: 0.09400 test_loss: 0.10654 \n",
      "[102/300] train_loss: 0.08675 valid_loss: 0.09433 test_loss: 0.10677 \n",
      "[103/300] train_loss: 0.08458 valid_loss: 0.09353 test_loss: 0.10465 \n",
      "Validation loss decreased (0.093773 --> 0.093525).  Saving model ...\n",
      "[104/300] train_loss: 0.08899 valid_loss: 0.09395 test_loss: 0.10476 \n",
      "[105/300] train_loss: 0.08537 valid_loss: 0.09364 test_loss: 0.10512 \n",
      "[106/300] train_loss: 0.08371 valid_loss: 0.09583 test_loss: 0.10779 \n",
      "[107/300] train_loss: 0.08241 valid_loss: 0.09603 test_loss: 0.10823 \n",
      "[108/300] train_loss: 0.08468 valid_loss: 0.09224 test_loss: 0.10627 \n",
      "Validation loss decreased (0.093525 --> 0.092239).  Saving model ...\n",
      "[109/300] train_loss: 0.08321 valid_loss: 0.09322 test_loss: 0.10457 \n",
      "[110/300] train_loss: 0.08168 valid_loss: 0.09366 test_loss: 0.10489 \n",
      "[111/300] train_loss: 0.08430 valid_loss: 0.09179 test_loss: 0.10313 \n",
      "Validation loss decreased (0.092239 --> 0.091792).  Saving model ...\n",
      "[112/300] train_loss: 0.08153 valid_loss: 0.09473 test_loss: 0.10531 \n",
      "[113/300] train_loss: 0.08399 valid_loss: 0.09107 test_loss: 0.10302 \n",
      "Validation loss decreased (0.091792 --> 0.091067).  Saving model ...\n",
      "[114/300] train_loss: 0.08131 valid_loss: 0.09067 test_loss: 0.10433 \n",
      "Validation loss decreased (0.091067 --> 0.090671).  Saving model ...\n",
      "[115/300] train_loss: 0.08321 valid_loss: 0.09393 test_loss: 0.10588 \n",
      "[116/300] train_loss: 0.08225 valid_loss: 0.09358 test_loss: 0.10581 \n",
      "[117/300] train_loss: 0.08383 valid_loss: 0.09134 test_loss: 0.10300 \n",
      "[118/300] train_loss: 0.08226 valid_loss: 0.08993 test_loss: 0.10169 \n",
      "Validation loss decreased (0.090671 --> 0.089933).  Saving model ...\n",
      "[119/300] train_loss: 0.07887 valid_loss: 0.09131 test_loss: 0.10445 \n",
      "[120/300] train_loss: 0.07955 valid_loss: 0.09021 test_loss: 0.10112 \n",
      "[121/300] train_loss: 0.07991 valid_loss: 0.08876 test_loss: 0.10276 \n",
      "Validation loss decreased (0.089933 --> 0.088761).  Saving model ...\n",
      "[122/300] train_loss: 0.08369 valid_loss: 0.08961 test_loss: 0.10217 \n",
      "[123/300] train_loss: 0.08068 valid_loss: 0.08924 test_loss: 0.10153 \n",
      "[124/300] train_loss: 0.08218 valid_loss: 0.09065 test_loss: 0.10303 \n",
      "[125/300] train_loss: 0.08286 valid_loss: 0.08893 test_loss: 0.09944 \n",
      "[126/300] train_loss: 0.07932 valid_loss: 0.08976 test_loss: 0.10197 \n",
      "[127/300] train_loss: 0.08004 valid_loss: 0.09135 test_loss: 0.10353 \n",
      "[128/300] train_loss: 0.07676 valid_loss: 0.09135 test_loss: 0.10217 \n",
      "[129/300] train_loss: 0.08012 valid_loss: 0.08946 test_loss: 0.10145 \n",
      "[130/300] train_loss: 0.08066 valid_loss: 0.08890 test_loss: 0.10022 \n",
      "[131/300] train_loss: 0.07625 valid_loss: 0.08759 test_loss: 0.10043 \n",
      "Validation loss decreased (0.088761 --> 0.087589).  Saving model ...\n",
      "[132/300] train_loss: 0.07858 valid_loss: 0.09041 test_loss: 0.10171 \n",
      "[133/300] train_loss: 0.08170 valid_loss: 0.08950 test_loss: 0.10110 \n",
      "[134/300] train_loss: 0.07959 valid_loss: 0.09057 test_loss: 0.10306 \n",
      "[135/300] train_loss: 0.08048 valid_loss: 0.08967 test_loss: 0.09997 \n",
      "[136/300] train_loss: 0.07735 valid_loss: 0.08863 test_loss: 0.09973 \n",
      "[137/300] train_loss: 0.07955 valid_loss: 0.08877 test_loss: 0.10053 \n",
      "[138/300] train_loss: 0.07641 valid_loss: 0.08827 test_loss: 0.09976 \n",
      "[139/300] train_loss: 0.07822 valid_loss: 0.08898 test_loss: 0.09944 \n",
      "[140/300] train_loss: 0.07652 valid_loss: 0.08796 test_loss: 0.09937 \n",
      "[141/300] train_loss: 0.07624 valid_loss: 0.08618 test_loss: 0.09788 \n",
      "Validation loss decreased (0.087589 --> 0.086180).  Saving model ...\n",
      "[142/300] train_loss: 0.07830 valid_loss: 0.08861 test_loss: 0.09940 \n",
      "[143/300] train_loss: 0.07624 valid_loss: 0.08650 test_loss: 0.09879 \n",
      "[144/300] train_loss: 0.07780 valid_loss: 0.08740 test_loss: 0.09951 \n",
      "[145/300] train_loss: 0.07598 valid_loss: 0.08628 test_loss: 0.09762 \n",
      "[146/300] train_loss: 0.07843 valid_loss: 0.08887 test_loss: 0.09931 \n",
      "[147/300] train_loss: 0.07652 valid_loss: 0.08855 test_loss: 0.09925 \n",
      "[148/300] train_loss: 0.07484 valid_loss: 0.08811 test_loss: 0.09999 \n",
      "[149/300] train_loss: 0.07867 valid_loss: 0.08707 test_loss: 0.09716 \n",
      "[150/300] train_loss: 0.07773 valid_loss: 0.08554 test_loss: 0.09735 \n",
      "Validation loss decreased (0.086180 --> 0.085542).  Saving model ...\n",
      "[151/300] train_loss: 0.07637 valid_loss: 0.08566 test_loss: 0.09686 \n",
      "[152/300] train_loss: 0.07646 valid_loss: 0.08674 test_loss: 0.09813 \n",
      "[153/300] train_loss: 0.07705 valid_loss: 0.08674 test_loss: 0.09848 \n",
      "[154/300] train_loss: 0.07410 valid_loss: 0.08516 test_loss: 0.09673 \n",
      "Validation loss decreased (0.085542 --> 0.085162).  Saving model ...\n",
      "[155/300] train_loss: 0.07451 valid_loss: 0.08549 test_loss: 0.09763 \n",
      "[156/300] train_loss: 0.07505 valid_loss: 0.08507 test_loss: 0.09753 \n",
      "Validation loss decreased (0.085162 --> 0.085074).  Saving model ...\n",
      "[157/300] train_loss: 0.07562 valid_loss: 0.08843 test_loss: 0.09923 \n",
      "[158/300] train_loss: 0.07416 valid_loss: 0.08487 test_loss: 0.09686 \n",
      "Validation loss decreased (0.085074 --> 0.084871).  Saving model ...\n",
      "[159/300] train_loss: 0.07582 valid_loss: 0.08556 test_loss: 0.09771 \n",
      "[160/300] train_loss: 0.07530 valid_loss: 0.08609 test_loss: 0.09698 \n",
      "[161/300] train_loss: 0.07607 valid_loss: 0.08614 test_loss: 0.09722 \n",
      "[162/300] train_loss: 0.07380 valid_loss: 0.08487 test_loss: 0.09760 \n",
      "Validation loss decreased (0.084871 --> 0.084865).  Saving model ...\n",
      "[163/300] train_loss: 0.07382 valid_loss: 0.08583 test_loss: 0.09754 \n",
      "[164/300] train_loss: 0.07583 valid_loss: 0.08481 test_loss: 0.09541 \n",
      "Validation loss decreased (0.084865 --> 0.084815).  Saving model ...\n",
      "[165/300] train_loss: 0.07506 valid_loss: 0.08556 test_loss: 0.09736 \n",
      "[166/300] train_loss: 0.07188 valid_loss: 0.08491 test_loss: 0.09621 \n",
      "[167/300] train_loss: 0.07335 valid_loss: 0.08543 test_loss: 0.09642 \n",
      "[168/300] train_loss: 0.07581 valid_loss: 0.08520 test_loss: 0.09584 \n",
      "[169/300] train_loss: 0.07614 valid_loss: 0.08498 test_loss: 0.09674 \n",
      "[170/300] train_loss: 0.07136 valid_loss: 0.08581 test_loss: 0.09575 \n",
      "[171/300] train_loss: 0.07651 valid_loss: 0.08446 test_loss: 0.09521 \n",
      "Validation loss decreased (0.084815 --> 0.084456).  Saving model ...\n",
      "[172/300] train_loss: 0.07391 valid_loss: 0.08308 test_loss: 0.09375 \n",
      "Validation loss decreased (0.084456 --> 0.083076).  Saving model ...\n",
      "[173/300] train_loss: 0.07716 valid_loss: 0.08365 test_loss: 0.09494 \n",
      "[174/300] train_loss: 0.07381 valid_loss: 0.08567 test_loss: 0.09744 \n",
      "[175/300] train_loss: 0.07339 valid_loss: 0.08409 test_loss: 0.09547 \n",
      "[176/300] train_loss: 0.07393 valid_loss: 0.08353 test_loss: 0.09477 \n",
      "[177/300] train_loss: 0.07217 valid_loss: 0.08563 test_loss: 0.09697 \n",
      "[178/300] train_loss: 0.07378 valid_loss: 0.08403 test_loss: 0.09533 \n",
      "[179/300] train_loss: 0.07434 valid_loss: 0.08357 test_loss: 0.09519 \n",
      "[180/300] train_loss: 0.07210 valid_loss: 0.08336 test_loss: 0.09410 \n",
      "[181/300] train_loss: 0.07343 valid_loss: 0.08497 test_loss: 0.09526 \n",
      "[182/300] train_loss: 0.07265 valid_loss: 0.08376 test_loss: 0.09507 \n",
      "[183/300] train_loss: 0.07405 valid_loss: 0.08333 test_loss: 0.09488 \n",
      "[184/300] train_loss: 0.07436 valid_loss: 0.08316 test_loss: 0.09437 \n",
      "[185/300] train_loss: 0.07329 valid_loss: 0.08408 test_loss: 0.09432 \n",
      "[186/300] train_loss: 0.07402 valid_loss: 0.08335 test_loss: 0.09319 \n",
      "[187/300] train_loss: 0.07387 valid_loss: 0.08310 test_loss: 0.09458 \n",
      "[188/300] train_loss: 0.07039 valid_loss: 0.08361 test_loss: 0.09395 \n",
      "[189/300] train_loss: 0.07165 valid_loss: 0.08313 test_loss: 0.09363 \n",
      "[190/300] train_loss: 0.07013 valid_loss: 0.08467 test_loss: 0.09509 \n",
      "[191/300] train_loss: 0.07235 valid_loss: 0.08452 test_loss: 0.09499 \n",
      "[192/300] train_loss: 0.07130 valid_loss: 0.08417 test_loss: 0.09455 \n",
      "[193/300] train_loss: 0.07151 valid_loss: 0.08224 test_loss: 0.09329 \n",
      "Validation loss decreased (0.083076 --> 0.082240).  Saving model ...\n",
      "[194/300] train_loss: 0.06875 valid_loss: 0.08306 test_loss: 0.09321 \n",
      "[195/300] train_loss: 0.06971 valid_loss: 0.08258 test_loss: 0.09352 \n",
      "[196/300] train_loss: 0.07194 valid_loss: 0.08265 test_loss: 0.09261 \n",
      "[197/300] train_loss: 0.07000 valid_loss: 0.08214 test_loss: 0.09347 \n",
      "Validation loss decreased (0.082240 --> 0.082141).  Saving model ...\n",
      "[198/300] train_loss: 0.06946 valid_loss: 0.08094 test_loss: 0.09225 \n",
      "Validation loss decreased (0.082141 --> 0.080940).  Saving model ...\n",
      "[199/300] train_loss: 0.07056 valid_loss: 0.08300 test_loss: 0.09361 \n",
      "[200/300] train_loss: 0.07022 valid_loss: 0.08314 test_loss: 0.09475 \n",
      "[201/300] train_loss: 0.06973 valid_loss: 0.08329 test_loss: 0.09340 \n",
      "[202/300] train_loss: 0.07167 valid_loss: 0.08139 test_loss: 0.09308 \n",
      "[203/300] train_loss: 0.07034 valid_loss: 0.08432 test_loss: 0.09399 \n",
      "[204/300] train_loss: 0.07123 valid_loss: 0.08381 test_loss: 0.09311 \n",
      "[205/300] train_loss: 0.06954 valid_loss: 0.08463 test_loss: 0.09330 \n",
      "[206/300] train_loss: 0.07263 valid_loss: 0.08213 test_loss: 0.09226 \n",
      "[207/300] train_loss: 0.07001 valid_loss: 0.08265 test_loss: 0.09347 \n",
      "[208/300] train_loss: 0.06970 valid_loss: 0.08113 test_loss: 0.09213 \n",
      "[209/300] train_loss: 0.06873 valid_loss: 0.08271 test_loss: 0.09284 \n",
      "[210/300] train_loss: 0.06892 valid_loss: 0.08199 test_loss: 0.09283 \n",
      "[211/300] train_loss: 0.07163 valid_loss: 0.08080 test_loss: 0.09086 \n",
      "Validation loss decreased (0.080940 --> 0.080799).  Saving model ...\n",
      "[212/300] train_loss: 0.07030 valid_loss: 0.08125 test_loss: 0.09181 \n",
      "[213/300] train_loss: 0.06987 valid_loss: 0.08183 test_loss: 0.09306 \n",
      "[214/300] train_loss: 0.07098 valid_loss: 0.08164 test_loss: 0.09283 \n",
      "[215/300] train_loss: 0.06984 valid_loss: 0.08009 test_loss: 0.09136 \n",
      "Validation loss decreased (0.080799 --> 0.080091).  Saving model ...\n",
      "[216/300] train_loss: 0.06873 valid_loss: 0.08076 test_loss: 0.09227 \n",
      "[217/300] train_loss: 0.07013 valid_loss: 0.08192 test_loss: 0.09271 \n",
      "[218/300] train_loss: 0.07332 valid_loss: 0.08058 test_loss: 0.09218 \n",
      "[219/300] train_loss: 0.06837 valid_loss: 0.08085 test_loss: 0.09174 \n",
      "[220/300] train_loss: 0.06696 valid_loss: 0.08191 test_loss: 0.09221 \n",
      "[221/300] train_loss: 0.06785 valid_loss: 0.08037 test_loss: 0.09138 \n",
      "[222/300] train_loss: 0.07165 valid_loss: 0.08115 test_loss: 0.09125 \n",
      "[223/300] train_loss: 0.06943 valid_loss: 0.08212 test_loss: 0.09192 \n",
      "[224/300] train_loss: 0.06740 valid_loss: 0.07998 test_loss: 0.09122 \n",
      "Validation loss decreased (0.080091 --> 0.079977).  Saving model ...\n",
      "[225/300] train_loss: 0.07093 valid_loss: 0.08090 test_loss: 0.09194 \n",
      "[226/300] train_loss: 0.06815 valid_loss: 0.08192 test_loss: 0.09303 \n",
      "[227/300] train_loss: 0.06877 valid_loss: 0.08045 test_loss: 0.09092 \n",
      "[228/300] train_loss: 0.06726 valid_loss: 0.08019 test_loss: 0.09126 \n",
      "[229/300] train_loss: 0.06663 valid_loss: 0.08037 test_loss: 0.09131 \n",
      "[230/300] train_loss: 0.06817 valid_loss: 0.08097 test_loss: 0.09106 \n",
      "[231/300] train_loss: 0.06672 valid_loss: 0.08060 test_loss: 0.09077 \n",
      "[232/300] train_loss: 0.06842 valid_loss: 0.07978 test_loss: 0.09044 \n",
      "Validation loss decreased (0.079977 --> 0.079781).  Saving model ...\n",
      "[233/300] train_loss: 0.06822 valid_loss: 0.07893 test_loss: 0.08989 \n",
      "Validation loss decreased (0.079781 --> 0.078930).  Saving model ...\n",
      "[234/300] train_loss: 0.06687 valid_loss: 0.07925 test_loss: 0.09042 \n",
      "[235/300] train_loss: 0.06659 valid_loss: 0.08048 test_loss: 0.09146 \n",
      "[236/300] train_loss: 0.06785 valid_loss: 0.08018 test_loss: 0.09172 \n",
      "[237/300] train_loss: 0.06498 valid_loss: 0.08021 test_loss: 0.09171 \n",
      "[238/300] train_loss: 0.06704 valid_loss: 0.07890 test_loss: 0.09008 \n",
      "Validation loss decreased (0.078930 --> 0.078902).  Saving model ...\n",
      "[239/300] train_loss: 0.06913 valid_loss: 0.08015 test_loss: 0.09034 \n",
      "[240/300] train_loss: 0.06763 valid_loss: 0.08021 test_loss: 0.09040 \n",
      "[241/300] train_loss: 0.06667 valid_loss: 0.07984 test_loss: 0.09029 \n",
      "[242/300] train_loss: 0.06710 valid_loss: 0.08016 test_loss: 0.09031 \n",
      "[243/300] train_loss: 0.06805 valid_loss: 0.08131 test_loss: 0.09107 \n",
      "[244/300] train_loss: 0.06610 valid_loss: 0.07916 test_loss: 0.09056 \n",
      "[245/300] train_loss: 0.06768 valid_loss: 0.07904 test_loss: 0.09074 \n",
      "[246/300] train_loss: 0.06793 valid_loss: 0.07868 test_loss: 0.09029 \n",
      "Validation loss decreased (0.078902 --> 0.078676).  Saving model ...\n",
      "[247/300] train_loss: 0.06849 valid_loss: 0.07809 test_loss: 0.08969 \n",
      "Validation loss decreased (0.078676 --> 0.078095).  Saving model ...\n",
      "[248/300] train_loss: 0.06717 valid_loss: 0.07881 test_loss: 0.09033 \n",
      "[249/300] train_loss: 0.06507 valid_loss: 0.08034 test_loss: 0.08987 \n",
      "[250/300] train_loss: 0.06770 valid_loss: 0.07939 test_loss: 0.09093 \n",
      "[251/300] train_loss: 0.06383 valid_loss: 0.08013 test_loss: 0.09101 \n",
      "[252/300] train_loss: 0.06674 valid_loss: 0.07868 test_loss: 0.09031 \n",
      "[253/300] train_loss: 0.06642 valid_loss: 0.07829 test_loss: 0.08977 \n",
      "[254/300] train_loss: 0.06278 valid_loss: 0.08095 test_loss: 0.09150 \n",
      "[255/300] train_loss: 0.06553 valid_loss: 0.07896 test_loss: 0.09008 \n",
      "[256/300] train_loss: 0.06655 valid_loss: 0.07921 test_loss: 0.09096 \n",
      "[257/300] train_loss: 0.06553 valid_loss: 0.07811 test_loss: 0.08994 \n",
      "[258/300] train_loss: 0.06602 valid_loss: 0.07835 test_loss: 0.09045 \n",
      "[259/300] train_loss: 0.06556 valid_loss: 0.07820 test_loss: 0.09011 \n",
      "[260/300] train_loss: 0.06643 valid_loss: 0.07970 test_loss: 0.09093 \n",
      "[261/300] train_loss: 0.06520 valid_loss: 0.07912 test_loss: 0.09016 \n",
      "[262/300] train_loss: 0.06669 valid_loss: 0.07986 test_loss: 0.08885 \n",
      "[263/300] train_loss: 0.06608 valid_loss: 0.07920 test_loss: 0.08991 \n",
      "[264/300] train_loss: 0.06671 valid_loss: 0.07769 test_loss: 0.08979 \n",
      "Validation loss decreased (0.078095 --> 0.077693).  Saving model ...\n",
      "[265/300] train_loss: 0.06473 valid_loss: 0.07812 test_loss: 0.08902 \n",
      "[266/300] train_loss: 0.06816 valid_loss: 0.07805 test_loss: 0.09026 \n",
      "[267/300] train_loss: 0.06685 valid_loss: 0.07722 test_loss: 0.08848 \n",
      "Validation loss decreased (0.077693 --> 0.077218).  Saving model ...\n",
      "[268/300] train_loss: 0.06591 valid_loss: 0.07803 test_loss: 0.08919 \n",
      "[269/300] train_loss: 0.06737 valid_loss: 0.07801 test_loss: 0.08959 \n",
      "[270/300] train_loss: 0.06456 valid_loss: 0.07851 test_loss: 0.09026 \n",
      "[271/300] train_loss: 0.06639 valid_loss: 0.07872 test_loss: 0.09007 \n",
      "[272/300] train_loss: 0.06639 valid_loss: 0.07974 test_loss: 0.09038 \n",
      "[273/300] train_loss: 0.06407 valid_loss: 0.07768 test_loss: 0.08880 \n",
      "[274/300] train_loss: 0.06690 valid_loss: 0.07759 test_loss: 0.08918 \n",
      "[275/300] train_loss: 0.06568 valid_loss: 0.07798 test_loss: 0.08957 \n",
      "[276/300] train_loss: 0.06653 valid_loss: 0.07776 test_loss: 0.08933 \n",
      "[277/300] train_loss: 0.06562 valid_loss: 0.07694 test_loss: 0.08810 \n",
      "Validation loss decreased (0.077218 --> 0.076939).  Saving model ...\n",
      "[278/300] train_loss: 0.06474 valid_loss: 0.07785 test_loss: 0.08994 \n",
      "[279/300] train_loss: 0.06463 valid_loss: 0.07714 test_loss: 0.08773 \n",
      "[280/300] train_loss: 0.06564 valid_loss: 0.07766 test_loss: 0.08870 \n",
      "[281/300] train_loss: 0.06515 valid_loss: 0.07810 test_loss: 0.08814 \n",
      "[282/300] train_loss: 0.06304 valid_loss: 0.07741 test_loss: 0.08883 \n",
      "[283/300] train_loss: 0.06433 valid_loss: 0.07682 test_loss: 0.08857 \n",
      "Validation loss decreased (0.076939 --> 0.076820).  Saving model ...\n",
      "[284/300] train_loss: 0.06307 valid_loss: 0.07635 test_loss: 0.08842 \n",
      "Validation loss decreased (0.076820 --> 0.076348).  Saving model ...\n",
      "[285/300] train_loss: 0.06384 valid_loss: 0.07703 test_loss: 0.08873 \n",
      "[286/300] train_loss: 0.06437 valid_loss: 0.07731 test_loss: 0.08927 \n",
      "[287/300] train_loss: 0.06307 valid_loss: 0.07701 test_loss: 0.08963 \n",
      "[288/300] train_loss: 0.06269 valid_loss: 0.07652 test_loss: 0.08786 \n",
      "[289/300] train_loss: 0.06558 valid_loss: 0.07680 test_loss: 0.08811 \n",
      "[290/300] train_loss: 0.06399 valid_loss: 0.07680 test_loss: 0.08819 \n",
      "[291/300] train_loss: 0.06310 valid_loss: 0.07762 test_loss: 0.08885 \n",
      "[292/300] train_loss: 0.06374 valid_loss: 0.07754 test_loss: 0.08817 \n",
      "[293/300] train_loss: 0.06456 valid_loss: 0.07874 test_loss: 0.08846 \n",
      "[294/300] train_loss: 0.06219 valid_loss: 0.07738 test_loss: 0.08676 \n",
      "[295/300] train_loss: 0.06362 valid_loss: 0.07705 test_loss: 0.08770 \n",
      "[296/300] train_loss: 0.06531 valid_loss: 0.07751 test_loss: 0.08840 \n",
      "[297/300] train_loss: 0.06434 valid_loss: 0.07828 test_loss: 0.08898 \n",
      "[298/300] train_loss: 0.06162 valid_loss: 0.07696 test_loss: 0.08797 \n",
      "[299/300] train_loss: 0.06286 valid_loss: 0.07680 test_loss: 0.08714 \n",
      "[300/300] train_loss: 0.06308 valid_loss: 0.07768 test_loss: 0.08892 \n",
      "TRAINING MODEL 15\n",
      "[  1/300] train_loss: 0.59128 valid_loss: 0.47065 test_loss: 0.47860 \n",
      "Validation loss decreased (inf --> 0.470651).  Saving model ...\n",
      "[  2/300] train_loss: 0.38516 valid_loss: 0.34112 test_loss: 0.35200 \n",
      "Validation loss decreased (0.470651 --> 0.341118).  Saving model ...\n",
      "[  3/300] train_loss: 0.28209 valid_loss: 0.27371 test_loss: 0.28779 \n",
      "Validation loss decreased (0.341118 --> 0.273712).  Saving model ...\n",
      "[  4/300] train_loss: 0.23564 valid_loss: 0.23719 test_loss: 0.25218 \n",
      "Validation loss decreased (0.273712 --> 0.237186).  Saving model ...\n",
      "[  5/300] train_loss: 0.21395 valid_loss: 0.21536 test_loss: 0.23206 \n",
      "Validation loss decreased (0.237186 --> 0.215362).  Saving model ...\n",
      "[  6/300] train_loss: 0.19714 valid_loss: 0.19820 test_loss: 0.21519 \n",
      "Validation loss decreased (0.215362 --> 0.198197).  Saving model ...\n",
      "[  7/300] train_loss: 0.18493 valid_loss: 0.18991 test_loss: 0.20614 \n",
      "Validation loss decreased (0.198197 --> 0.189915).  Saving model ...\n",
      "[  8/300] train_loss: 0.17700 valid_loss: 0.18100 test_loss: 0.19626 \n",
      "Validation loss decreased (0.189915 --> 0.180998).  Saving model ...\n",
      "[  9/300] train_loss: 0.17290 valid_loss: 0.17334 test_loss: 0.18657 \n",
      "Validation loss decreased (0.180998 --> 0.173343).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16646 valid_loss: 0.17239 test_loss: 0.18725 \n",
      "Validation loss decreased (0.173343 --> 0.172386).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16196 valid_loss: 0.16774 test_loss: 0.18138 \n",
      "Validation loss decreased (0.172386 --> 0.167739).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15907 valid_loss: 0.16497 test_loss: 0.17760 \n",
      "Validation loss decreased (0.167739 --> 0.164973).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15616 valid_loss: 0.16199 test_loss: 0.17556 \n",
      "Validation loss decreased (0.164973 --> 0.161993).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15186 valid_loss: 0.16014 test_loss: 0.17119 \n",
      "Validation loss decreased (0.161993 --> 0.160135).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14646 valid_loss: 0.15850 test_loss: 0.16749 \n",
      "Validation loss decreased (0.160135 --> 0.158502).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14503 valid_loss: 0.15600 test_loss: 0.17026 \n",
      "Validation loss decreased (0.158502 --> 0.156003).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14304 valid_loss: 0.15406 test_loss: 0.16222 \n",
      "Validation loss decreased (0.156003 --> 0.154056).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13889 valid_loss: 0.15247 test_loss: 0.16426 \n",
      "Validation loss decreased (0.154056 --> 0.152468).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13938 valid_loss: 0.15576 test_loss: 0.16608 \n",
      "[ 20/300] train_loss: 0.13177 valid_loss: 0.14955 test_loss: 0.15838 \n",
      "Validation loss decreased (0.152468 --> 0.149546).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13589 valid_loss: 0.14945 test_loss: 0.16308 \n",
      "Validation loss decreased (0.149546 --> 0.149448).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13180 valid_loss: 0.14501 test_loss: 0.15563 \n",
      "Validation loss decreased (0.149448 --> 0.145014).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12808 valid_loss: 0.14567 test_loss: 0.15503 \n",
      "[ 24/300] train_loss: 0.12543 valid_loss: 0.13929 test_loss: 0.15075 \n",
      "Validation loss decreased (0.145014 --> 0.139287).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12865 valid_loss: 0.13920 test_loss: 0.15286 \n",
      "Validation loss decreased (0.139287 --> 0.139200).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12632 valid_loss: 0.13779 test_loss: 0.14692 \n",
      "Validation loss decreased (0.139200 --> 0.137793).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12510 valid_loss: 0.13665 test_loss: 0.14753 \n",
      "Validation loss decreased (0.137793 --> 0.136645).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12115 valid_loss: 0.13508 test_loss: 0.14650 \n",
      "Validation loss decreased (0.136645 --> 0.135083).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11782 valid_loss: 0.13341 test_loss: 0.14469 \n",
      "Validation loss decreased (0.135083 --> 0.133413).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11765 valid_loss: 0.13521 test_loss: 0.14365 \n",
      "[ 31/300] train_loss: 0.11743 valid_loss: 0.13517 test_loss: 0.14510 \n",
      "[ 32/300] train_loss: 0.11474 valid_loss: 0.13033 test_loss: 0.14011 \n",
      "Validation loss decreased (0.133413 --> 0.130333).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11558 valid_loss: 0.13344 test_loss: 0.14287 \n",
      "[ 34/300] train_loss: 0.11499 valid_loss: 0.12833 test_loss: 0.13837 \n",
      "Validation loss decreased (0.130333 --> 0.128325).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11188 valid_loss: 0.12440 test_loss: 0.13625 \n",
      "Validation loss decreased (0.128325 --> 0.124396).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11180 valid_loss: 0.12512 test_loss: 0.13823 \n",
      "[ 37/300] train_loss: 0.11257 valid_loss: 0.12619 test_loss: 0.13690 \n",
      "[ 38/300] train_loss: 0.11313 valid_loss: 0.12303 test_loss: 0.13339 \n",
      "Validation loss decreased (0.124396 --> 0.123035).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11041 valid_loss: 0.13064 test_loss: 0.13967 \n",
      "[ 40/300] train_loss: 0.10736 valid_loss: 0.12296 test_loss: 0.13267 \n",
      "Validation loss decreased (0.123035 --> 0.122957).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10869 valid_loss: 0.12053 test_loss: 0.13166 \n",
      "Validation loss decreased (0.122957 --> 0.120534).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10752 valid_loss: 0.12127 test_loss: 0.13274 \n",
      "[ 43/300] train_loss: 0.10666 valid_loss: 0.11649 test_loss: 0.12958 \n",
      "Validation loss decreased (0.120534 --> 0.116493).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10427 valid_loss: 0.11572 test_loss: 0.12850 \n",
      "Validation loss decreased (0.116493 --> 0.115723).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10450 valid_loss: 0.11580 test_loss: 0.12806 \n",
      "[ 46/300] train_loss: 0.10531 valid_loss: 0.11551 test_loss: 0.12638 \n",
      "Validation loss decreased (0.115723 --> 0.115508).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10613 valid_loss: 0.11334 test_loss: 0.12576 \n",
      "Validation loss decreased (0.115508 --> 0.113342).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10248 valid_loss: 0.11770 test_loss: 0.12936 \n",
      "[ 49/300] train_loss: 0.10359 valid_loss: 0.11313 test_loss: 0.12498 \n",
      "Validation loss decreased (0.113342 --> 0.113134).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10113 valid_loss: 0.11422 test_loss: 0.12548 \n",
      "[ 51/300] train_loss: 0.10302 valid_loss: 0.11116 test_loss: 0.12222 \n",
      "Validation loss decreased (0.113134 --> 0.111162).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10231 valid_loss: 0.11325 test_loss: 0.12557 \n",
      "[ 53/300] train_loss: 0.10081 valid_loss: 0.10904 test_loss: 0.12268 \n",
      "Validation loss decreased (0.111162 --> 0.109038).  Saving model ...\n",
      "[ 54/300] train_loss: 0.09956 valid_loss: 0.10960 test_loss: 0.12116 \n",
      "[ 55/300] train_loss: 0.10254 valid_loss: 0.10985 test_loss: 0.12067 \n",
      "[ 56/300] train_loss: 0.10205 valid_loss: 0.10761 test_loss: 0.11998 \n",
      "Validation loss decreased (0.109038 --> 0.107606).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09805 valid_loss: 0.10833 test_loss: 0.12086 \n",
      "[ 58/300] train_loss: 0.10049 valid_loss: 0.10827 test_loss: 0.11894 \n",
      "[ 59/300] train_loss: 0.10166 valid_loss: 0.10880 test_loss: 0.11943 \n",
      "[ 60/300] train_loss: 0.09783 valid_loss: 0.10788 test_loss: 0.11916 \n",
      "[ 61/300] train_loss: 0.09909 valid_loss: 0.10886 test_loss: 0.11898 \n",
      "[ 62/300] train_loss: 0.09732 valid_loss: 0.10734 test_loss: 0.11976 \n",
      "Validation loss decreased (0.107606 --> 0.107339).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09329 valid_loss: 0.10521 test_loss: 0.11917 \n",
      "Validation loss decreased (0.107339 --> 0.105205).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09726 valid_loss: 0.10551 test_loss: 0.11750 \n",
      "[ 65/300] train_loss: 0.09731 valid_loss: 0.10514 test_loss: 0.11691 \n",
      "Validation loss decreased (0.105205 --> 0.105140).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09727 valid_loss: 0.10338 test_loss: 0.11520 \n",
      "Validation loss decreased (0.105140 --> 0.103378).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09259 valid_loss: 0.10316 test_loss: 0.11511 \n",
      "Validation loss decreased (0.103378 --> 0.103162).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09714 valid_loss: 0.10289 test_loss: 0.11562 \n",
      "Validation loss decreased (0.103162 --> 0.102886).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09277 valid_loss: 0.10459 test_loss: 0.11674 \n",
      "[ 70/300] train_loss: 0.09407 valid_loss: 0.10295 test_loss: 0.11562 \n",
      "[ 71/300] train_loss: 0.09302 valid_loss: 0.10248 test_loss: 0.11381 \n",
      "Validation loss decreased (0.102886 --> 0.102477).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09197 valid_loss: 0.10170 test_loss: 0.11239 \n",
      "Validation loss decreased (0.102477 --> 0.101696).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09217 valid_loss: 0.10148 test_loss: 0.11254 \n",
      "Validation loss decreased (0.101696 --> 0.101483).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09248 valid_loss: 0.09958 test_loss: 0.11241 \n",
      "Validation loss decreased (0.101483 --> 0.099581).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09240 valid_loss: 0.10115 test_loss: 0.11161 \n",
      "[ 76/300] train_loss: 0.09078 valid_loss: 0.09925 test_loss: 0.11119 \n",
      "Validation loss decreased (0.099581 --> 0.099255).  Saving model ...\n",
      "[ 77/300] train_loss: 0.08924 valid_loss: 0.10000 test_loss: 0.11190 \n",
      "[ 78/300] train_loss: 0.09076 valid_loss: 0.10026 test_loss: 0.11061 \n",
      "[ 79/300] train_loss: 0.09221 valid_loss: 0.09929 test_loss: 0.11073 \n",
      "[ 80/300] train_loss: 0.09181 valid_loss: 0.09945 test_loss: 0.10968 \n",
      "[ 81/300] train_loss: 0.08954 valid_loss: 0.09931 test_loss: 0.11072 \n",
      "[ 82/300] train_loss: 0.08888 valid_loss: 0.09877 test_loss: 0.11045 \n",
      "Validation loss decreased (0.099255 --> 0.098766).  Saving model ...\n",
      "[ 83/300] train_loss: 0.09053 valid_loss: 0.09692 test_loss: 0.10836 \n",
      "Validation loss decreased (0.098766 --> 0.096925).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08806 valid_loss: 0.09981 test_loss: 0.10934 \n",
      "[ 85/300] train_loss: 0.08739 valid_loss: 0.09719 test_loss: 0.10798 \n",
      "[ 86/300] train_loss: 0.08830 valid_loss: 0.09637 test_loss: 0.10896 \n",
      "Validation loss decreased (0.096925 --> 0.096366).  Saving model ...\n",
      "[ 87/300] train_loss: 0.08537 valid_loss: 0.09535 test_loss: 0.10693 \n",
      "Validation loss decreased (0.096366 --> 0.095353).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08684 valid_loss: 0.09832 test_loss: 0.11021 \n",
      "[ 89/300] train_loss: 0.08615 valid_loss: 0.09766 test_loss: 0.10789 \n",
      "[ 90/300] train_loss: 0.08578 valid_loss: 0.09637 test_loss: 0.10785 \n",
      "[ 91/300] train_loss: 0.08904 valid_loss: 0.09575 test_loss: 0.10643 \n",
      "[ 92/300] train_loss: 0.08550 valid_loss: 0.09492 test_loss: 0.10708 \n",
      "Validation loss decreased (0.095353 --> 0.094918).  Saving model ...\n",
      "[ 93/300] train_loss: 0.08657 valid_loss: 0.09707 test_loss: 0.10758 \n",
      "[ 94/300] train_loss: 0.08617 valid_loss: 0.09519 test_loss: 0.10538 \n",
      "[ 95/300] train_loss: 0.08470 valid_loss: 0.09433 test_loss: 0.10497 \n",
      "Validation loss decreased (0.094918 --> 0.094331).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08682 valid_loss: 0.09515 test_loss: 0.10547 \n",
      "[ 97/300] train_loss: 0.08520 valid_loss: 0.09757 test_loss: 0.10759 \n",
      "[ 98/300] train_loss: 0.08716 valid_loss: 0.09585 test_loss: 0.10686 \n",
      "[ 99/300] train_loss: 0.08581 valid_loss: 0.09354 test_loss: 0.10574 \n",
      "Validation loss decreased (0.094331 --> 0.093538).  Saving model ...\n",
      "[100/300] train_loss: 0.08221 valid_loss: 0.09523 test_loss: 0.10549 \n",
      "[101/300] train_loss: 0.08476 valid_loss: 0.09506 test_loss: 0.10637 \n",
      "[102/300] train_loss: 0.08363 valid_loss: 0.09293 test_loss: 0.10394 \n",
      "Validation loss decreased (0.093538 --> 0.092929).  Saving model ...\n",
      "[103/300] train_loss: 0.08516 valid_loss: 0.09369 test_loss: 0.10528 \n",
      "[104/300] train_loss: 0.08146 valid_loss: 0.09286 test_loss: 0.10347 \n",
      "Validation loss decreased (0.092929 --> 0.092863).  Saving model ...\n",
      "[105/300] train_loss: 0.08392 valid_loss: 0.09259 test_loss: 0.10392 \n",
      "Validation loss decreased (0.092863 --> 0.092591).  Saving model ...\n",
      "[106/300] train_loss: 0.08300 valid_loss: 0.09155 test_loss: 0.10379 \n",
      "Validation loss decreased (0.092591 --> 0.091546).  Saving model ...\n",
      "[107/300] train_loss: 0.08166 valid_loss: 0.09328 test_loss: 0.10364 \n",
      "[108/300] train_loss: 0.08322 valid_loss: 0.09160 test_loss: 0.10360 \n",
      "[109/300] train_loss: 0.08136 valid_loss: 0.09351 test_loss: 0.10414 \n",
      "[110/300] train_loss: 0.08303 valid_loss: 0.09519 test_loss: 0.10660 \n",
      "[111/300] train_loss: 0.08582 valid_loss: 0.08999 test_loss: 0.10217 \n",
      "Validation loss decreased (0.091546 --> 0.089992).  Saving model ...\n",
      "[112/300] train_loss: 0.08334 valid_loss: 0.09128 test_loss: 0.10299 \n",
      "[113/300] train_loss: 0.08395 valid_loss: 0.09009 test_loss: 0.10283 \n",
      "[114/300] train_loss: 0.08231 valid_loss: 0.09171 test_loss: 0.10502 \n",
      "[115/300] train_loss: 0.08216 valid_loss: 0.09147 test_loss: 0.10295 \n",
      "[116/300] train_loss: 0.08247 valid_loss: 0.09341 test_loss: 0.10288 \n",
      "[117/300] train_loss: 0.08173 valid_loss: 0.08964 test_loss: 0.10094 \n",
      "Validation loss decreased (0.089992 --> 0.089637).  Saving model ...\n",
      "[118/300] train_loss: 0.08230 valid_loss: 0.08950 test_loss: 0.10285 \n",
      "Validation loss decreased (0.089637 --> 0.089500).  Saving model ...\n",
      "[119/300] train_loss: 0.08283 valid_loss: 0.09533 test_loss: 0.10388 \n",
      "[120/300] train_loss: 0.07800 valid_loss: 0.08937 test_loss: 0.10140 \n",
      "Validation loss decreased (0.089500 --> 0.089370).  Saving model ...\n",
      "[121/300] train_loss: 0.07975 valid_loss: 0.09045 test_loss: 0.10213 \n",
      "[122/300] train_loss: 0.08142 valid_loss: 0.08964 test_loss: 0.10036 \n",
      "[123/300] train_loss: 0.08018 valid_loss: 0.09133 test_loss: 0.10115 \n",
      "[124/300] train_loss: 0.07918 valid_loss: 0.08887 test_loss: 0.09996 \n",
      "Validation loss decreased (0.089370 --> 0.088871).  Saving model ...\n",
      "[125/300] train_loss: 0.07998 valid_loss: 0.08861 test_loss: 0.09880 \n",
      "Validation loss decreased (0.088871 --> 0.088612).  Saving model ...\n",
      "[126/300] train_loss: 0.07950 valid_loss: 0.08872 test_loss: 0.09995 \n",
      "[127/300] train_loss: 0.07906 valid_loss: 0.08880 test_loss: 0.10090 \n",
      "[128/300] train_loss: 0.08134 valid_loss: 0.08871 test_loss: 0.09967 \n",
      "[129/300] train_loss: 0.07781 valid_loss: 0.08762 test_loss: 0.09887 \n",
      "Validation loss decreased (0.088612 --> 0.087619).  Saving model ...\n",
      "[130/300] train_loss: 0.07706 valid_loss: 0.08962 test_loss: 0.09924 \n",
      "[131/300] train_loss: 0.08036 valid_loss: 0.09018 test_loss: 0.10099 \n",
      "[132/300] train_loss: 0.07815 valid_loss: 0.08915 test_loss: 0.10031 \n",
      "[133/300] train_loss: 0.07884 valid_loss: 0.09017 test_loss: 0.09919 \n",
      "[134/300] train_loss: 0.07635 valid_loss: 0.08826 test_loss: 0.09930 \n",
      "[135/300] train_loss: 0.07579 valid_loss: 0.09023 test_loss: 0.10149 \n",
      "[136/300] train_loss: 0.07903 valid_loss: 0.08769 test_loss: 0.09773 \n",
      "[137/300] train_loss: 0.07658 valid_loss: 0.08697 test_loss: 0.09796 \n",
      "Validation loss decreased (0.087619 --> 0.086972).  Saving model ...\n",
      "[138/300] train_loss: 0.07477 valid_loss: 0.08963 test_loss: 0.09939 \n",
      "[139/300] train_loss: 0.07822 valid_loss: 0.08720 test_loss: 0.09788 \n",
      "[140/300] train_loss: 0.07767 valid_loss: 0.08604 test_loss: 0.09674 \n",
      "Validation loss decreased (0.086972 --> 0.086044).  Saving model ...\n",
      "[141/300] train_loss: 0.07717 valid_loss: 0.08685 test_loss: 0.09877 \n",
      "[142/300] train_loss: 0.07869 valid_loss: 0.08835 test_loss: 0.09851 \n",
      "[143/300] train_loss: 0.07730 valid_loss: 0.08492 test_loss: 0.09658 \n",
      "Validation loss decreased (0.086044 --> 0.084918).  Saving model ...\n",
      "[144/300] train_loss: 0.07415 valid_loss: 0.08768 test_loss: 0.09840 \n",
      "[145/300] train_loss: 0.07768 valid_loss: 0.08854 test_loss: 0.09875 \n",
      "[146/300] train_loss: 0.07755 valid_loss: 0.08652 test_loss: 0.09655 \n",
      "[147/300] train_loss: 0.07628 valid_loss: 0.08968 test_loss: 0.10029 \n",
      "[148/300] train_loss: 0.07405 valid_loss: 0.08708 test_loss: 0.09777 \n",
      "[149/300] train_loss: 0.07530 valid_loss: 0.08669 test_loss: 0.09661 \n",
      "[150/300] train_loss: 0.07561 valid_loss: 0.08639 test_loss: 0.09812 \n",
      "[151/300] train_loss: 0.07747 valid_loss: 0.08966 test_loss: 0.09797 \n",
      "[152/300] train_loss: 0.07557 valid_loss: 0.08650 test_loss: 0.09713 \n",
      "[153/300] train_loss: 0.07444 valid_loss: 0.08731 test_loss: 0.09708 \n",
      "[154/300] train_loss: 0.07600 valid_loss: 0.08765 test_loss: 0.09811 \n",
      "[155/300] train_loss: 0.07479 valid_loss: 0.08662 test_loss: 0.09659 \n",
      "[156/300] train_loss: 0.07547 valid_loss: 0.08552 test_loss: 0.09672 \n",
      "[157/300] train_loss: 0.07195 valid_loss: 0.08563 test_loss: 0.09471 \n",
      "[158/300] train_loss: 0.07614 valid_loss: 0.08608 test_loss: 0.09611 \n",
      "[159/300] train_loss: 0.07236 valid_loss: 0.08504 test_loss: 0.09615 \n",
      "[160/300] train_loss: 0.07507 valid_loss: 0.08593 test_loss: 0.09518 \n",
      "[161/300] train_loss: 0.07363 valid_loss: 0.08488 test_loss: 0.09639 \n",
      "Validation loss decreased (0.084918 --> 0.084882).  Saving model ...\n",
      "[162/300] train_loss: 0.07374 valid_loss: 0.08683 test_loss: 0.09581 \n",
      "[163/300] train_loss: 0.07450 valid_loss: 0.08360 test_loss: 0.09591 \n",
      "Validation loss decreased (0.084882 --> 0.083602).  Saving model ...\n",
      "[164/300] train_loss: 0.07508 valid_loss: 0.08540 test_loss: 0.09584 \n",
      "[165/300] train_loss: 0.07274 valid_loss: 0.08482 test_loss: 0.09481 \n",
      "[166/300] train_loss: 0.07340 valid_loss: 0.08396 test_loss: 0.09435 \n",
      "[167/300] train_loss: 0.07258 valid_loss: 0.08326 test_loss: 0.09448 \n",
      "Validation loss decreased (0.083602 --> 0.083261).  Saving model ...\n",
      "[168/300] train_loss: 0.07268 valid_loss: 0.08360 test_loss: 0.09381 \n",
      "[169/300] train_loss: 0.07018 valid_loss: 0.08390 test_loss: 0.09476 \n",
      "[170/300] train_loss: 0.07367 valid_loss: 0.08425 test_loss: 0.09452 \n",
      "[171/300] train_loss: 0.07297 valid_loss: 0.08482 test_loss: 0.09628 \n",
      "[172/300] train_loss: 0.07278 valid_loss: 0.08239 test_loss: 0.09349 \n",
      "Validation loss decreased (0.083261 --> 0.082387).  Saving model ...\n",
      "[173/300] train_loss: 0.07310 valid_loss: 0.08473 test_loss: 0.09461 \n",
      "[174/300] train_loss: 0.07113 valid_loss: 0.08511 test_loss: 0.09672 \n",
      "[175/300] train_loss: 0.07171 valid_loss: 0.08443 test_loss: 0.09531 \n",
      "[176/300] train_loss: 0.07421 valid_loss: 0.08304 test_loss: 0.09376 \n",
      "[177/300] train_loss: 0.07160 valid_loss: 0.08317 test_loss: 0.09335 \n",
      "[178/300] train_loss: 0.07207 valid_loss: 0.08516 test_loss: 0.09566 \n",
      "[179/300] train_loss: 0.07162 valid_loss: 0.08551 test_loss: 0.09442 \n",
      "[180/300] train_loss: 0.07336 valid_loss: 0.08409 test_loss: 0.09532 \n",
      "[181/300] train_loss: 0.07109 valid_loss: 0.08377 test_loss: 0.09340 \n",
      "[182/300] train_loss: 0.07047 valid_loss: 0.08535 test_loss: 0.09479 \n",
      "[183/300] train_loss: 0.07142 valid_loss: 0.08245 test_loss: 0.09247 \n",
      "[184/300] train_loss: 0.07041 valid_loss: 0.08343 test_loss: 0.09415 \n",
      "[185/300] train_loss: 0.06831 valid_loss: 0.08377 test_loss: 0.09481 \n",
      "[186/300] train_loss: 0.07054 valid_loss: 0.08178 test_loss: 0.09509 \n",
      "Validation loss decreased (0.082387 --> 0.081779).  Saving model ...\n",
      "[187/300] train_loss: 0.07249 valid_loss: 0.08355 test_loss: 0.09430 \n",
      "[188/300] train_loss: 0.07129 valid_loss: 0.08239 test_loss: 0.09264 \n",
      "[189/300] train_loss: 0.07021 valid_loss: 0.08186 test_loss: 0.09305 \n",
      "[190/300] train_loss: 0.07211 valid_loss: 0.08157 test_loss: 0.09195 \n",
      "Validation loss decreased (0.081779 --> 0.081574).  Saving model ...\n",
      "[191/300] train_loss: 0.07274 valid_loss: 0.08216 test_loss: 0.09236 \n",
      "[192/300] train_loss: 0.07073 valid_loss: 0.08206 test_loss: 0.09179 \n",
      "[193/300] train_loss: 0.06980 valid_loss: 0.08201 test_loss: 0.09309 \n",
      "[194/300] train_loss: 0.07097 valid_loss: 0.08267 test_loss: 0.09349 \n",
      "[195/300] train_loss: 0.07005 valid_loss: 0.08061 test_loss: 0.09166 \n",
      "Validation loss decreased (0.081574 --> 0.080611).  Saving model ...\n",
      "[196/300] train_loss: 0.06964 valid_loss: 0.08193 test_loss: 0.09207 \n",
      "[197/300] train_loss: 0.07030 valid_loss: 0.08286 test_loss: 0.09422 \n",
      "[198/300] train_loss: 0.07195 valid_loss: 0.08133 test_loss: 0.09220 \n",
      "[199/300] train_loss: 0.06892 valid_loss: 0.08183 test_loss: 0.09307 \n",
      "[200/300] train_loss: 0.06913 valid_loss: 0.08087 test_loss: 0.09249 \n",
      "[201/300] train_loss: 0.06912 valid_loss: 0.08236 test_loss: 0.09238 \n",
      "[202/300] train_loss: 0.07052 valid_loss: 0.08123 test_loss: 0.09173 \n",
      "[203/300] train_loss: 0.06951 valid_loss: 0.08081 test_loss: 0.09187 \n",
      "[204/300] train_loss: 0.07052 valid_loss: 0.08297 test_loss: 0.09270 \n",
      "[205/300] train_loss: 0.06908 valid_loss: 0.08214 test_loss: 0.09209 \n",
      "[206/300] train_loss: 0.06848 valid_loss: 0.08008 test_loss: 0.09096 \n",
      "Validation loss decreased (0.080611 --> 0.080078).  Saving model ...\n",
      "[207/300] train_loss: 0.06780 valid_loss: 0.08043 test_loss: 0.09156 \n",
      "[208/300] train_loss: 0.06997 valid_loss: 0.08226 test_loss: 0.09139 \n",
      "[209/300] train_loss: 0.06877 valid_loss: 0.08208 test_loss: 0.09315 \n",
      "[210/300] train_loss: 0.06747 valid_loss: 0.08143 test_loss: 0.09270 \n",
      "[211/300] train_loss: 0.06760 valid_loss: 0.08061 test_loss: 0.09093 \n",
      "[212/300] train_loss: 0.07004 valid_loss: 0.08119 test_loss: 0.09091 \n",
      "[213/300] train_loss: 0.06725 valid_loss: 0.08039 test_loss: 0.09091 \n",
      "[214/300] train_loss: 0.06782 valid_loss: 0.07981 test_loss: 0.09131 \n",
      "Validation loss decreased (0.080078 --> 0.079811).  Saving model ...\n",
      "[215/300] train_loss: 0.06846 valid_loss: 0.07988 test_loss: 0.08925 \n",
      "[216/300] train_loss: 0.06677 valid_loss: 0.08004 test_loss: 0.09137 \n",
      "[217/300] train_loss: 0.06615 valid_loss: 0.08030 test_loss: 0.09118 \n",
      "[218/300] train_loss: 0.06540 valid_loss: 0.08051 test_loss: 0.09070 \n",
      "[219/300] train_loss: 0.06673 valid_loss: 0.08153 test_loss: 0.09145 \n",
      "[220/300] train_loss: 0.06755 valid_loss: 0.07891 test_loss: 0.08994 \n",
      "Validation loss decreased (0.079811 --> 0.078910).  Saving model ...\n",
      "[221/300] train_loss: 0.06677 valid_loss: 0.08001 test_loss: 0.09094 \n",
      "[222/300] train_loss: 0.06679 valid_loss: 0.07985 test_loss: 0.09050 \n",
      "[223/300] train_loss: 0.06667 valid_loss: 0.08097 test_loss: 0.09116 \n",
      "[224/300] train_loss: 0.06684 valid_loss: 0.08043 test_loss: 0.09032 \n",
      "[225/300] train_loss: 0.06641 valid_loss: 0.07960 test_loss: 0.09081 \n",
      "[226/300] train_loss: 0.06636 valid_loss: 0.07848 test_loss: 0.08893 \n",
      "Validation loss decreased (0.078910 --> 0.078485).  Saving model ...\n",
      "[227/300] train_loss: 0.06896 valid_loss: 0.08057 test_loss: 0.09021 \n",
      "[228/300] train_loss: 0.06471 valid_loss: 0.07926 test_loss: 0.08941 \n",
      "[229/300] train_loss: 0.06514 valid_loss: 0.07976 test_loss: 0.08946 \n",
      "[230/300] train_loss: 0.06464 valid_loss: 0.08071 test_loss: 0.08962 \n",
      "[231/300] train_loss: 0.06603 valid_loss: 0.07909 test_loss: 0.09004 \n",
      "[232/300] train_loss: 0.06640 valid_loss: 0.07968 test_loss: 0.09010 \n",
      "[233/300] train_loss: 0.06618 valid_loss: 0.08075 test_loss: 0.08979 \n",
      "[234/300] train_loss: 0.06612 valid_loss: 0.07886 test_loss: 0.08983 \n",
      "[235/300] train_loss: 0.06629 valid_loss: 0.07883 test_loss: 0.08843 \n",
      "[236/300] train_loss: 0.06506 valid_loss: 0.07923 test_loss: 0.08884 \n",
      "[237/300] train_loss: 0.06598 valid_loss: 0.07890 test_loss: 0.08911 \n",
      "[238/300] train_loss: 0.06866 valid_loss: 0.07952 test_loss: 0.09064 \n",
      "[239/300] train_loss: 0.06546 valid_loss: 0.07794 test_loss: 0.08902 \n",
      "Validation loss decreased (0.078485 --> 0.077944).  Saving model ...\n",
      "[240/300] train_loss: 0.06489 valid_loss: 0.07866 test_loss: 0.08864 \n",
      "[241/300] train_loss: 0.06453 valid_loss: 0.07880 test_loss: 0.08832 \n",
      "[242/300] train_loss: 0.06623 valid_loss: 0.07916 test_loss: 0.09023 \n",
      "[243/300] train_loss: 0.06544 valid_loss: 0.07884 test_loss: 0.08894 \n",
      "[244/300] train_loss: 0.06568 valid_loss: 0.07971 test_loss: 0.08894 \n",
      "[245/300] train_loss: 0.06481 valid_loss: 0.07949 test_loss: 0.08956 \n",
      "[246/300] train_loss: 0.06627 valid_loss: 0.07986 test_loss: 0.08913 \n",
      "[247/300] train_loss: 0.06608 valid_loss: 0.07853 test_loss: 0.08950 \n",
      "[248/300] train_loss: 0.06526 valid_loss: 0.07831 test_loss: 0.08895 \n",
      "[249/300] train_loss: 0.06521 valid_loss: 0.07984 test_loss: 0.08967 \n",
      "[250/300] train_loss: 0.06450 valid_loss: 0.07861 test_loss: 0.08932 \n",
      "[251/300] train_loss: 0.06292 valid_loss: 0.07978 test_loss: 0.09075 \n",
      "[252/300] train_loss: 0.06581 valid_loss: 0.07865 test_loss: 0.08899 \n",
      "[253/300] train_loss: 0.06505 valid_loss: 0.07911 test_loss: 0.09003 \n",
      "[254/300] train_loss: 0.06681 valid_loss: 0.07871 test_loss: 0.08911 \n",
      "[255/300] train_loss: 0.06427 valid_loss: 0.07959 test_loss: 0.08851 \n",
      "[256/300] train_loss: 0.06458 valid_loss: 0.07900 test_loss: 0.08914 \n",
      "[257/300] train_loss: 0.06445 valid_loss: 0.07742 test_loss: 0.08906 \n",
      "Validation loss decreased (0.077944 --> 0.077419).  Saving model ...\n",
      "[258/300] train_loss: 0.06439 valid_loss: 0.07821 test_loss: 0.08855 \n",
      "[259/300] train_loss: 0.06580 valid_loss: 0.07931 test_loss: 0.08870 \n",
      "[260/300] train_loss: 0.06313 valid_loss: 0.07754 test_loss: 0.08885 \n",
      "[261/300] train_loss: 0.06519 valid_loss: 0.07829 test_loss: 0.08868 \n",
      "[262/300] train_loss: 0.06427 valid_loss: 0.07873 test_loss: 0.08955 \n",
      "[263/300] train_loss: 0.06324 valid_loss: 0.07734 test_loss: 0.08762 \n",
      "Validation loss decreased (0.077419 --> 0.077344).  Saving model ...\n",
      "[264/300] train_loss: 0.06464 valid_loss: 0.07850 test_loss: 0.08857 \n",
      "[265/300] train_loss: 0.06407 valid_loss: 0.07722 test_loss: 0.08897 \n",
      "Validation loss decreased (0.077344 --> 0.077215).  Saving model ...\n",
      "[266/300] train_loss: 0.06457 valid_loss: 0.07796 test_loss: 0.08803 \n",
      "[267/300] train_loss: 0.06302 valid_loss: 0.07806 test_loss: 0.08973 \n",
      "[268/300] train_loss: 0.06222 valid_loss: 0.07780 test_loss: 0.08824 \n",
      "[269/300] train_loss: 0.06157 valid_loss: 0.07785 test_loss: 0.08829 \n",
      "[270/300] train_loss: 0.06481 valid_loss: 0.07806 test_loss: 0.08771 \n",
      "[271/300] train_loss: 0.06618 valid_loss: 0.07744 test_loss: 0.08837 \n",
      "[272/300] train_loss: 0.06241 valid_loss: 0.07813 test_loss: 0.08865 \n",
      "[273/300] train_loss: 0.06236 valid_loss: 0.07712 test_loss: 0.08846 \n",
      "Validation loss decreased (0.077215 --> 0.077122).  Saving model ...\n",
      "[274/300] train_loss: 0.06522 valid_loss: 0.07849 test_loss: 0.08965 \n",
      "[275/300] train_loss: 0.06293 valid_loss: 0.07642 test_loss: 0.08771 \n",
      "Validation loss decreased (0.077122 --> 0.076422).  Saving model ...\n",
      "[276/300] train_loss: 0.06170 valid_loss: 0.07798 test_loss: 0.08846 \n",
      "[277/300] train_loss: 0.06668 valid_loss: 0.07745 test_loss: 0.08827 \n",
      "[278/300] train_loss: 0.06402 valid_loss: 0.07617 test_loss: 0.08821 \n",
      "Validation loss decreased (0.076422 --> 0.076175).  Saving model ...\n",
      "[279/300] train_loss: 0.06330 valid_loss: 0.07622 test_loss: 0.08715 \n",
      "[280/300] train_loss: 0.06343 valid_loss: 0.07679 test_loss: 0.08768 \n",
      "[281/300] train_loss: 0.06343 valid_loss: 0.07845 test_loss: 0.08850 \n",
      "[282/300] train_loss: 0.06067 valid_loss: 0.07699 test_loss: 0.08766 \n",
      "[283/300] train_loss: 0.06428 valid_loss: 0.07704 test_loss: 0.08730 \n",
      "[284/300] train_loss: 0.06244 valid_loss: 0.07638 test_loss: 0.08780 \n",
      "[285/300] train_loss: 0.06273 valid_loss: 0.07809 test_loss: 0.08846 \n",
      "[286/300] train_loss: 0.06084 valid_loss: 0.07599 test_loss: 0.08722 \n",
      "Validation loss decreased (0.076175 --> 0.075993).  Saving model ...\n",
      "[287/300] train_loss: 0.06188 valid_loss: 0.07667 test_loss: 0.08794 \n",
      "[288/300] train_loss: 0.06158 valid_loss: 0.07729 test_loss: 0.08762 \n",
      "[289/300] train_loss: 0.06250 valid_loss: 0.07884 test_loss: 0.08809 \n",
      "[290/300] train_loss: 0.06266 valid_loss: 0.07763 test_loss: 0.08763 \n",
      "[291/300] train_loss: 0.06353 valid_loss: 0.07644 test_loss: 0.08770 \n",
      "[292/300] train_loss: 0.06228 valid_loss: 0.07777 test_loss: 0.08678 \n",
      "[293/300] train_loss: 0.06168 valid_loss: 0.07710 test_loss: 0.08612 \n",
      "[294/300] train_loss: 0.05952 valid_loss: 0.07568 test_loss: 0.08628 \n",
      "Validation loss decreased (0.075993 --> 0.075677).  Saving model ...\n",
      "[295/300] train_loss: 0.06334 valid_loss: 0.07664 test_loss: 0.08620 \n",
      "[296/300] train_loss: 0.06240 valid_loss: 0.07605 test_loss: 0.08643 \n",
      "[297/300] train_loss: 0.06046 valid_loss: 0.07607 test_loss: 0.08635 \n",
      "[298/300] train_loss: 0.06058 valid_loss: 0.07622 test_loss: 0.08717 \n",
      "[299/300] train_loss: 0.06094 valid_loss: 0.07630 test_loss: 0.08666 \n",
      "[300/300] train_loss: 0.06144 valid_loss: 0.07747 test_loss: 0.08710 \n",
      "TRAINING MODEL 16\n",
      "[  1/300] train_loss: 0.55097 valid_loss: 0.45935 test_loss: 0.46474 \n",
      "Validation loss decreased (inf --> 0.459352).  Saving model ...\n",
      "[  2/300] train_loss: 0.36683 valid_loss: 0.33572 test_loss: 0.34275 \n",
      "Validation loss decreased (0.459352 --> 0.335715).  Saving model ...\n",
      "[  3/300] train_loss: 0.27596 valid_loss: 0.26608 test_loss: 0.27724 \n",
      "Validation loss decreased (0.335715 --> 0.266085).  Saving model ...\n",
      "[  4/300] train_loss: 0.23222 valid_loss: 0.23112 test_loss: 0.24557 \n",
      "Validation loss decreased (0.266085 --> 0.231117).  Saving model ...\n",
      "[  5/300] train_loss: 0.20807 valid_loss: 0.20780 test_loss: 0.22341 \n",
      "Validation loss decreased (0.231117 --> 0.207803).  Saving model ...\n",
      "[  6/300] train_loss: 0.19087 valid_loss: 0.20027 test_loss: 0.21971 \n",
      "Validation loss decreased (0.207803 --> 0.200275).  Saving model ...\n",
      "[  7/300] train_loss: 0.18104 valid_loss: 0.18460 test_loss: 0.19807 \n",
      "Validation loss decreased (0.200275 --> 0.184601).  Saving model ...\n",
      "[  8/300] train_loss: 0.17467 valid_loss: 0.17865 test_loss: 0.19030 \n",
      "Validation loss decreased (0.184601 --> 0.178654).  Saving model ...\n",
      "[  9/300] train_loss: 0.16657 valid_loss: 0.17142 test_loss: 0.18576 \n",
      "Validation loss decreased (0.178654 --> 0.171421).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16244 valid_loss: 0.16498 test_loss: 0.17670 \n",
      "Validation loss decreased (0.171421 --> 0.164980).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16086 valid_loss: 0.16005 test_loss: 0.17163 \n",
      "Validation loss decreased (0.164980 --> 0.160051).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15281 valid_loss: 0.15871 test_loss: 0.16826 \n",
      "Validation loss decreased (0.160051 --> 0.158711).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14972 valid_loss: 0.15442 test_loss: 0.16501 \n",
      "Validation loss decreased (0.158711 --> 0.154418).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14890 valid_loss: 0.14989 test_loss: 0.15955 \n",
      "Validation loss decreased (0.154418 --> 0.149894).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14476 valid_loss: 0.14720 test_loss: 0.15681 \n",
      "Validation loss decreased (0.149894 --> 0.147200).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13928 valid_loss: 0.14926 test_loss: 0.15799 \n",
      "[ 17/300] train_loss: 0.13904 valid_loss: 0.14511 test_loss: 0.15462 \n",
      "Validation loss decreased (0.147200 --> 0.145112).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13396 valid_loss: 0.14252 test_loss: 0.15364 \n",
      "Validation loss decreased (0.145112 --> 0.142519).  Saving model ...\n",
      "[ 19/300] train_loss: 0.12997 valid_loss: 0.13759 test_loss: 0.14986 \n",
      "Validation loss decreased (0.142519 --> 0.137588).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13135 valid_loss: 0.13422 test_loss: 0.14623 \n",
      "Validation loss decreased (0.137588 --> 0.134219).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12891 valid_loss: 0.13717 test_loss: 0.14992 \n",
      "[ 22/300] train_loss: 0.12420 valid_loss: 0.13105 test_loss: 0.14443 \n",
      "Validation loss decreased (0.134219 --> 0.131047).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12918 valid_loss: 0.13077 test_loss: 0.14201 \n",
      "Validation loss decreased (0.131047 --> 0.130769).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12430 valid_loss: 0.12883 test_loss: 0.14275 \n",
      "Validation loss decreased (0.130769 --> 0.128832).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12100 valid_loss: 0.12567 test_loss: 0.13900 \n",
      "Validation loss decreased (0.128832 --> 0.125669).  Saving model ...\n",
      "[ 26/300] train_loss: 0.11933 valid_loss: 0.12863 test_loss: 0.13982 \n",
      "[ 27/300] train_loss: 0.12170 valid_loss: 0.12317 test_loss: 0.13812 \n",
      "Validation loss decreased (0.125669 --> 0.123173).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11660 valid_loss: 0.12227 test_loss: 0.13652 \n",
      "Validation loss decreased (0.123173 --> 0.122268).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11685 valid_loss: 0.12072 test_loss: 0.13479 \n",
      "Validation loss decreased (0.122268 --> 0.120720).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11710 valid_loss: 0.12210 test_loss: 0.13483 \n",
      "[ 31/300] train_loss: 0.11631 valid_loss: 0.11934 test_loss: 0.13316 \n",
      "Validation loss decreased (0.120720 --> 0.119336).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11373 valid_loss: 0.12336 test_loss: 0.13604 \n",
      "[ 33/300] train_loss: 0.11049 valid_loss: 0.11992 test_loss: 0.13392 \n",
      "[ 34/300] train_loss: 0.11346 valid_loss: 0.11573 test_loss: 0.13036 \n",
      "Validation loss decreased (0.119336 --> 0.115732).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11424 valid_loss: 0.11727 test_loss: 0.13030 \n",
      "[ 36/300] train_loss: 0.11297 valid_loss: 0.11454 test_loss: 0.12892 \n",
      "Validation loss decreased (0.115732 --> 0.114538).  Saving model ...\n",
      "[ 37/300] train_loss: 0.10992 valid_loss: 0.11608 test_loss: 0.13004 \n",
      "[ 38/300] train_loss: 0.10664 valid_loss: 0.11308 test_loss: 0.12906 \n",
      "Validation loss decreased (0.114538 --> 0.113085).  Saving model ...\n",
      "[ 39/300] train_loss: 0.10871 valid_loss: 0.11288 test_loss: 0.12781 \n",
      "Validation loss decreased (0.113085 --> 0.112882).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10856 valid_loss: 0.11298 test_loss: 0.12657 \n",
      "[ 41/300] train_loss: 0.10662 valid_loss: 0.11495 test_loss: 0.12847 \n",
      "[ 42/300] train_loss: 0.10636 valid_loss: 0.11120 test_loss: 0.12459 \n",
      "Validation loss decreased (0.112882 --> 0.111200).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10584 valid_loss: 0.11058 test_loss: 0.12469 \n",
      "Validation loss decreased (0.111200 --> 0.110578).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10394 valid_loss: 0.11047 test_loss: 0.12480 \n",
      "Validation loss decreased (0.110578 --> 0.110472).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10421 valid_loss: 0.10952 test_loss: 0.12397 \n",
      "Validation loss decreased (0.110472 --> 0.109521).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10461 valid_loss: 0.10886 test_loss: 0.12279 \n",
      "Validation loss decreased (0.109521 --> 0.108862).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10275 valid_loss: 0.10885 test_loss: 0.12180 \n",
      "Validation loss decreased (0.108862 --> 0.108850).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10294 valid_loss: 0.10851 test_loss: 0.12256 \n",
      "Validation loss decreased (0.108850 --> 0.108511).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10026 valid_loss: 0.10882 test_loss: 0.12219 \n",
      "[ 50/300] train_loss: 0.10196 valid_loss: 0.10992 test_loss: 0.12324 \n",
      "[ 51/300] train_loss: 0.10048 valid_loss: 0.10660 test_loss: 0.11915 \n",
      "Validation loss decreased (0.108511 --> 0.106604).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10200 valid_loss: 0.10582 test_loss: 0.12076 \n",
      "Validation loss decreased (0.106604 --> 0.105820).  Saving model ...\n",
      "[ 53/300] train_loss: 0.09868 valid_loss: 0.10595 test_loss: 0.11949 \n",
      "[ 54/300] train_loss: 0.09843 valid_loss: 0.10644 test_loss: 0.11841 \n",
      "[ 55/300] train_loss: 0.09850 valid_loss: 0.10658 test_loss: 0.11933 \n",
      "[ 56/300] train_loss: 0.09663 valid_loss: 0.10494 test_loss: 0.11808 \n",
      "Validation loss decreased (0.105820 --> 0.104937).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09774 valid_loss: 0.10491 test_loss: 0.12010 \n",
      "Validation loss decreased (0.104937 --> 0.104910).  Saving model ...\n",
      "[ 58/300] train_loss: 0.09719 valid_loss: 0.10343 test_loss: 0.11646 \n",
      "Validation loss decreased (0.104910 --> 0.103429).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09505 valid_loss: 0.10692 test_loss: 0.12017 \n",
      "[ 60/300] train_loss: 0.09757 valid_loss: 0.10441 test_loss: 0.11661 \n",
      "[ 61/300] train_loss: 0.09716 valid_loss: 0.10464 test_loss: 0.11737 \n",
      "[ 62/300] train_loss: 0.09654 valid_loss: 0.10160 test_loss: 0.11567 \n",
      "Validation loss decreased (0.103429 --> 0.101599).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09526 valid_loss: 0.10070 test_loss: 0.11499 \n",
      "Validation loss decreased (0.101599 --> 0.100697).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09555 valid_loss: 0.10034 test_loss: 0.11494 \n",
      "Validation loss decreased (0.100697 --> 0.100335).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09318 valid_loss: 0.10105 test_loss: 0.11500 \n",
      "[ 66/300] train_loss: 0.09337 valid_loss: 0.10442 test_loss: 0.11656 \n",
      "[ 67/300] train_loss: 0.09164 valid_loss: 0.09887 test_loss: 0.11366 \n",
      "Validation loss decreased (0.100335 --> 0.098865).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09178 valid_loss: 0.09943 test_loss: 0.11301 \n",
      "[ 69/300] train_loss: 0.09171 valid_loss: 0.10312 test_loss: 0.11683 \n",
      "[ 70/300] train_loss: 0.09327 valid_loss: 0.10153 test_loss: 0.11391 \n",
      "[ 71/300] train_loss: 0.08975 valid_loss: 0.09910 test_loss: 0.11185 \n",
      "[ 72/300] train_loss: 0.09185 valid_loss: 0.09913 test_loss: 0.11348 \n",
      "[ 73/300] train_loss: 0.08916 valid_loss: 0.10062 test_loss: 0.11249 \n",
      "[ 74/300] train_loss: 0.09113 valid_loss: 0.09884 test_loss: 0.11207 \n",
      "Validation loss decreased (0.098865 --> 0.098839).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09062 valid_loss: 0.10047 test_loss: 0.11283 \n",
      "[ 76/300] train_loss: 0.08921 valid_loss: 0.09616 test_loss: 0.11043 \n",
      "Validation loss decreased (0.098839 --> 0.096162).  Saving model ...\n",
      "[ 77/300] train_loss: 0.08757 valid_loss: 0.09927 test_loss: 0.11302 \n",
      "[ 78/300] train_loss: 0.08775 valid_loss: 0.09768 test_loss: 0.11022 \n",
      "[ 79/300] train_loss: 0.08895 valid_loss: 0.09747 test_loss: 0.11034 \n",
      "[ 80/300] train_loss: 0.08660 valid_loss: 0.09638 test_loss: 0.11028 \n",
      "[ 81/300] train_loss: 0.08843 valid_loss: 0.09436 test_loss: 0.10846 \n",
      "Validation loss decreased (0.096162 --> 0.094364).  Saving model ...\n",
      "[ 82/300] train_loss: 0.09241 valid_loss: 0.09775 test_loss: 0.11080 \n",
      "[ 83/300] train_loss: 0.08913 valid_loss: 0.09370 test_loss: 0.10826 \n",
      "Validation loss decreased (0.094364 --> 0.093703).  Saving model ...\n",
      "[ 84/300] train_loss: 0.08646 valid_loss: 0.09645 test_loss: 0.11060 \n",
      "[ 85/300] train_loss: 0.08583 valid_loss: 0.09748 test_loss: 0.10887 \n",
      "[ 86/300] train_loss: 0.08639 valid_loss: 0.09431 test_loss: 0.10934 \n",
      "[ 87/300] train_loss: 0.08488 valid_loss: 0.09625 test_loss: 0.10757 \n",
      "[ 88/300] train_loss: 0.08907 valid_loss: 0.09768 test_loss: 0.10888 \n",
      "[ 89/300] train_loss: 0.08754 valid_loss: 0.09636 test_loss: 0.10886 \n",
      "[ 90/300] train_loss: 0.08725 valid_loss: 0.09542 test_loss: 0.10903 \n",
      "[ 91/300] train_loss: 0.08560 valid_loss: 0.09557 test_loss: 0.10790 \n",
      "[ 92/300] train_loss: 0.08311 valid_loss: 0.09491 test_loss: 0.10665 \n",
      "[ 93/300] train_loss: 0.08869 valid_loss: 0.09332 test_loss: 0.10791 \n",
      "Validation loss decreased (0.093703 --> 0.093318).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08478 valid_loss: 0.09342 test_loss: 0.10661 \n",
      "[ 95/300] train_loss: 0.08593 valid_loss: 0.09650 test_loss: 0.10896 \n",
      "[ 96/300] train_loss: 0.08465 valid_loss: 0.09378 test_loss: 0.10605 \n",
      "[ 97/300] train_loss: 0.08253 valid_loss: 0.09499 test_loss: 0.10668 \n",
      "[ 98/300] train_loss: 0.08480 valid_loss: 0.09387 test_loss: 0.10562 \n",
      "[ 99/300] train_loss: 0.08355 valid_loss: 0.09319 test_loss: 0.10577 \n",
      "Validation loss decreased (0.093318 --> 0.093191).  Saving model ...\n",
      "[100/300] train_loss: 0.08559 valid_loss: 0.09289 test_loss: 0.10532 \n",
      "Validation loss decreased (0.093191 --> 0.092887).  Saving model ...\n",
      "[101/300] train_loss: 0.08432 valid_loss: 0.09212 test_loss: 0.10598 \n",
      "Validation loss decreased (0.092887 --> 0.092123).  Saving model ...\n",
      "[102/300] train_loss: 0.08251 valid_loss: 0.09080 test_loss: 0.10430 \n",
      "Validation loss decreased (0.092123 --> 0.090804).  Saving model ...\n",
      "[103/300] train_loss: 0.08207 valid_loss: 0.09207 test_loss: 0.10522 \n",
      "[104/300] train_loss: 0.08434 valid_loss: 0.09377 test_loss: 0.10472 \n",
      "[105/300] train_loss: 0.08380 valid_loss: 0.09199 test_loss: 0.10430 \n",
      "[106/300] train_loss: 0.08211 valid_loss: 0.09146 test_loss: 0.10299 \n",
      "[107/300] train_loss: 0.08275 valid_loss: 0.09253 test_loss: 0.10491 \n",
      "[108/300] train_loss: 0.08107 valid_loss: 0.08971 test_loss: 0.10332 \n",
      "Validation loss decreased (0.090804 --> 0.089707).  Saving model ...\n",
      "[109/300] train_loss: 0.08286 valid_loss: 0.08947 test_loss: 0.10285 \n",
      "Validation loss decreased (0.089707 --> 0.089467).  Saving model ...\n",
      "[110/300] train_loss: 0.07904 valid_loss: 0.09214 test_loss: 0.10281 \n",
      "[111/300] train_loss: 0.08267 valid_loss: 0.09127 test_loss: 0.10199 \n",
      "[112/300] train_loss: 0.08132 valid_loss: 0.08929 test_loss: 0.10132 \n",
      "Validation loss decreased (0.089467 --> 0.089286).  Saving model ...\n",
      "[113/300] train_loss: 0.07921 valid_loss: 0.09174 test_loss: 0.10392 \n",
      "[114/300] train_loss: 0.08019 valid_loss: 0.09070 test_loss: 0.10377 \n",
      "[115/300] train_loss: 0.08057 valid_loss: 0.08830 test_loss: 0.10340 \n",
      "Validation loss decreased (0.089286 --> 0.088300).  Saving model ...\n",
      "[116/300] train_loss: 0.08264 valid_loss: 0.09098 test_loss: 0.10240 \n",
      "[117/300] train_loss: 0.08042 valid_loss: 0.08995 test_loss: 0.10268 \n",
      "[118/300] train_loss: 0.08060 valid_loss: 0.09046 test_loss: 0.10244 \n",
      "[119/300] train_loss: 0.08135 valid_loss: 0.09149 test_loss: 0.10323 \n",
      "[120/300] train_loss: 0.07988 valid_loss: 0.08963 test_loss: 0.10144 \n",
      "[121/300] train_loss: 0.07919 valid_loss: 0.08941 test_loss: 0.10114 \n",
      "[122/300] train_loss: 0.07922 valid_loss: 0.08939 test_loss: 0.10172 \n",
      "[123/300] train_loss: 0.07980 valid_loss: 0.08800 test_loss: 0.10219 \n",
      "Validation loss decreased (0.088300 --> 0.088004).  Saving model ...\n",
      "[124/300] train_loss: 0.07905 valid_loss: 0.08680 test_loss: 0.09993 \n",
      "Validation loss decreased (0.088004 --> 0.086801).  Saving model ...\n",
      "[125/300] train_loss: 0.07913 valid_loss: 0.08853 test_loss: 0.10095 \n",
      "[126/300] train_loss: 0.07847 valid_loss: 0.08708 test_loss: 0.09996 \n",
      "[127/300] train_loss: 0.07545 valid_loss: 0.08733 test_loss: 0.10063 \n",
      "[128/300] train_loss: 0.07972 valid_loss: 0.08784 test_loss: 0.10063 \n",
      "[129/300] train_loss: 0.07703 valid_loss: 0.08889 test_loss: 0.10127 \n",
      "[130/300] train_loss: 0.07770 valid_loss: 0.08861 test_loss: 0.09996 \n",
      "[131/300] train_loss: 0.07873 valid_loss: 0.08763 test_loss: 0.09941 \n",
      "[132/300] train_loss: 0.07648 valid_loss: 0.08747 test_loss: 0.09963 \n",
      "[133/300] train_loss: 0.07833 valid_loss: 0.08783 test_loss: 0.09949 \n",
      "[134/300] train_loss: 0.07702 valid_loss: 0.08676 test_loss: 0.09964 \n",
      "Validation loss decreased (0.086801 --> 0.086763).  Saving model ...\n",
      "[135/300] train_loss: 0.07739 valid_loss: 0.08628 test_loss: 0.09844 \n",
      "Validation loss decreased (0.086763 --> 0.086278).  Saving model ...\n",
      "[136/300] train_loss: 0.07841 valid_loss: 0.08784 test_loss: 0.09963 \n",
      "[137/300] train_loss: 0.07657 valid_loss: 0.08922 test_loss: 0.10140 \n",
      "[138/300] train_loss: 0.07627 valid_loss: 0.08770 test_loss: 0.09867 \n",
      "[139/300] train_loss: 0.07679 valid_loss: 0.08601 test_loss: 0.09889 \n",
      "Validation loss decreased (0.086278 --> 0.086010).  Saving model ...\n",
      "[140/300] train_loss: 0.07568 valid_loss: 0.08799 test_loss: 0.10049 \n",
      "[141/300] train_loss: 0.07555 valid_loss: 0.08595 test_loss: 0.09990 \n",
      "Validation loss decreased (0.086010 --> 0.085945).  Saving model ...\n",
      "[142/300] train_loss: 0.07391 valid_loss: 0.08711 test_loss: 0.09877 \n",
      "[143/300] train_loss: 0.07625 valid_loss: 0.08639 test_loss: 0.09834 \n",
      "[144/300] train_loss: 0.07461 valid_loss: 0.08523 test_loss: 0.09793 \n",
      "Validation loss decreased (0.085945 --> 0.085226).  Saving model ...\n",
      "[145/300] train_loss: 0.07587 valid_loss: 0.08556 test_loss: 0.09836 \n",
      "[146/300] train_loss: 0.07709 valid_loss: 0.08760 test_loss: 0.09933 \n",
      "[147/300] train_loss: 0.07740 valid_loss: 0.08611 test_loss: 0.09825 \n",
      "[148/300] train_loss: 0.07343 valid_loss: 0.08447 test_loss: 0.09751 \n",
      "Validation loss decreased (0.085226 --> 0.084465).  Saving model ...\n",
      "[149/300] train_loss: 0.07605 valid_loss: 0.08741 test_loss: 0.09926 \n",
      "[150/300] train_loss: 0.07464 valid_loss: 0.08533 test_loss: 0.09812 \n",
      "[151/300] train_loss: 0.07377 valid_loss: 0.08521 test_loss: 0.09791 \n",
      "[152/300] train_loss: 0.07594 valid_loss: 0.08503 test_loss: 0.09804 \n",
      "[153/300] train_loss: 0.07404 valid_loss: 0.08681 test_loss: 0.09867 \n",
      "[154/300] train_loss: 0.07363 valid_loss: 0.08400 test_loss: 0.09725 \n",
      "Validation loss decreased (0.084465 --> 0.084004).  Saving model ...\n",
      "[155/300] train_loss: 0.07563 valid_loss: 0.08463 test_loss: 0.09816 \n",
      "[156/300] train_loss: 0.07478 valid_loss: 0.08827 test_loss: 0.10060 \n",
      "[157/300] train_loss: 0.07423 valid_loss: 0.08687 test_loss: 0.09809 \n",
      "[158/300] train_loss: 0.07375 valid_loss: 0.08536 test_loss: 0.09659 \n",
      "[159/300] train_loss: 0.07230 valid_loss: 0.08587 test_loss: 0.09755 \n",
      "[160/300] train_loss: 0.07217 valid_loss: 0.08611 test_loss: 0.09712 \n",
      "[161/300] train_loss: 0.07360 valid_loss: 0.08571 test_loss: 0.09770 \n",
      "[162/300] train_loss: 0.07286 valid_loss: 0.08601 test_loss: 0.09711 \n",
      "[163/300] train_loss: 0.07327 valid_loss: 0.08376 test_loss: 0.09801 \n",
      "Validation loss decreased (0.084004 --> 0.083758).  Saving model ...\n",
      "[164/300] train_loss: 0.07167 valid_loss: 0.08403 test_loss: 0.09533 \n",
      "[165/300] train_loss: 0.07399 valid_loss: 0.08519 test_loss: 0.09671 \n",
      "[166/300] train_loss: 0.07443 valid_loss: 0.08535 test_loss: 0.09689 \n",
      "[167/300] train_loss: 0.07288 valid_loss: 0.08336 test_loss: 0.09502 \n",
      "Validation loss decreased (0.083758 --> 0.083362).  Saving model ...\n",
      "[168/300] train_loss: 0.07218 valid_loss: 0.08455 test_loss: 0.09572 \n",
      "[169/300] train_loss: 0.07251 valid_loss: 0.08606 test_loss: 0.09707 \n",
      "[170/300] train_loss: 0.07317 valid_loss: 0.08291 test_loss: 0.09520 \n",
      "Validation loss decreased (0.083362 --> 0.082908).  Saving model ...\n",
      "[171/300] train_loss: 0.07234 valid_loss: 0.08343 test_loss: 0.09596 \n",
      "[172/300] train_loss: 0.07142 valid_loss: 0.08187 test_loss: 0.09451 \n",
      "Validation loss decreased (0.082908 --> 0.081868).  Saving model ...\n",
      "[173/300] train_loss: 0.07213 valid_loss: 0.08231 test_loss: 0.09482 \n",
      "[174/300] train_loss: 0.07247 valid_loss: 0.08387 test_loss: 0.09588 \n",
      "[175/300] train_loss: 0.07145 valid_loss: 0.08232 test_loss: 0.09438 \n",
      "[176/300] train_loss: 0.07131 valid_loss: 0.08368 test_loss: 0.09613 \n",
      "[177/300] train_loss: 0.07204 valid_loss: 0.08554 test_loss: 0.09694 \n",
      "[178/300] train_loss: 0.06877 valid_loss: 0.08529 test_loss: 0.09637 \n",
      "[179/300] train_loss: 0.07326 valid_loss: 0.08400 test_loss: 0.09559 \n",
      "[180/300] train_loss: 0.07252 valid_loss: 0.08501 test_loss: 0.09594 \n",
      "[181/300] train_loss: 0.07061 valid_loss: 0.08198 test_loss: 0.09480 \n",
      "[182/300] train_loss: 0.06821 valid_loss: 0.08264 test_loss: 0.09537 \n",
      "[183/300] train_loss: 0.07105 valid_loss: 0.08231 test_loss: 0.09365 \n",
      "[184/300] train_loss: 0.07206 valid_loss: 0.08173 test_loss: 0.09391 \n",
      "Validation loss decreased (0.081868 --> 0.081734).  Saving model ...\n",
      "[185/300] train_loss: 0.06710 valid_loss: 0.08429 test_loss: 0.09714 \n",
      "[186/300] train_loss: 0.06912 valid_loss: 0.08265 test_loss: 0.09362 \n",
      "[187/300] train_loss: 0.06835 valid_loss: 0.08195 test_loss: 0.09345 \n",
      "[188/300] train_loss: 0.07054 valid_loss: 0.08523 test_loss: 0.09499 \n",
      "[189/300] train_loss: 0.06991 valid_loss: 0.08177 test_loss: 0.09353 \n",
      "[190/300] train_loss: 0.06963 valid_loss: 0.08263 test_loss: 0.09326 \n",
      "[191/300] train_loss: 0.07030 valid_loss: 0.08288 test_loss: 0.09238 \n",
      "[192/300] train_loss: 0.06879 valid_loss: 0.08326 test_loss: 0.09403 \n",
      "[193/300] train_loss: 0.07008 valid_loss: 0.08171 test_loss: 0.09263 \n",
      "Validation loss decreased (0.081734 --> 0.081713).  Saving model ...\n",
      "[194/300] train_loss: 0.06859 valid_loss: 0.08219 test_loss: 0.09355 \n",
      "[195/300] train_loss: 0.06857 valid_loss: 0.08253 test_loss: 0.09490 \n",
      "[196/300] train_loss: 0.06901 valid_loss: 0.08238 test_loss: 0.09281 \n",
      "[197/300] train_loss: 0.06838 valid_loss: 0.08259 test_loss: 0.09221 \n",
      "[198/300] train_loss: 0.06780 valid_loss: 0.08275 test_loss: 0.09304 \n",
      "[199/300] train_loss: 0.06956 valid_loss: 0.08113 test_loss: 0.09200 \n",
      "Validation loss decreased (0.081713 --> 0.081126).  Saving model ...\n",
      "[200/300] train_loss: 0.06906 valid_loss: 0.08350 test_loss: 0.09424 \n",
      "[201/300] train_loss: 0.06850 valid_loss: 0.08311 test_loss: 0.09472 \n",
      "[202/300] train_loss: 0.06945 valid_loss: 0.08148 test_loss: 0.09312 \n",
      "[203/300] train_loss: 0.06816 valid_loss: 0.08126 test_loss: 0.09345 \n",
      "[204/300] train_loss: 0.06653 valid_loss: 0.08320 test_loss: 0.09288 \n",
      "[205/300] train_loss: 0.06786 valid_loss: 0.08073 test_loss: 0.09217 \n",
      "Validation loss decreased (0.081126 --> 0.080733).  Saving model ...\n",
      "[206/300] train_loss: 0.06967 valid_loss: 0.08236 test_loss: 0.09232 \n",
      "[207/300] train_loss: 0.06867 valid_loss: 0.08137 test_loss: 0.09175 \n",
      "[208/300] train_loss: 0.06769 valid_loss: 0.08093 test_loss: 0.09132 \n",
      "[209/300] train_loss: 0.06988 valid_loss: 0.08149 test_loss: 0.09108 \n",
      "[210/300] train_loss: 0.06798 valid_loss: 0.08275 test_loss: 0.09197 \n",
      "[211/300] train_loss: 0.06573 valid_loss: 0.08025 test_loss: 0.09071 \n",
      "Validation loss decreased (0.080733 --> 0.080253).  Saving model ...\n",
      "[212/300] train_loss: 0.06609 valid_loss: 0.08071 test_loss: 0.09064 \n",
      "[213/300] train_loss: 0.06706 valid_loss: 0.08282 test_loss: 0.09233 \n",
      "[214/300] train_loss: 0.06809 valid_loss: 0.08127 test_loss: 0.09192 \n",
      "[215/300] train_loss: 0.06562 valid_loss: 0.08218 test_loss: 0.09198 \n",
      "[216/300] train_loss: 0.06707 valid_loss: 0.08050 test_loss: 0.09161 \n",
      "[217/300] train_loss: 0.06878 valid_loss: 0.08080 test_loss: 0.09194 \n",
      "[218/300] train_loss: 0.06623 valid_loss: 0.08039 test_loss: 0.09216 \n",
      "[219/300] train_loss: 0.06432 valid_loss: 0.08136 test_loss: 0.09086 \n",
      "[220/300] train_loss: 0.06594 valid_loss: 0.08136 test_loss: 0.09083 \n",
      "[221/300] train_loss: 0.06734 valid_loss: 0.07908 test_loss: 0.09014 \n",
      "Validation loss decreased (0.080253 --> 0.079080).  Saving model ...\n",
      "[222/300] train_loss: 0.06633 valid_loss: 0.07903 test_loss: 0.09111 \n",
      "Validation loss decreased (0.079080 --> 0.079026).  Saving model ...\n",
      "[223/300] train_loss: 0.06806 valid_loss: 0.08007 test_loss: 0.08899 \n",
      "[224/300] train_loss: 0.06589 valid_loss: 0.08058 test_loss: 0.08989 \n",
      "[225/300] train_loss: 0.06534 valid_loss: 0.07940 test_loss: 0.09054 \n",
      "[226/300] train_loss: 0.06564 valid_loss: 0.08055 test_loss: 0.08921 \n",
      "[227/300] train_loss: 0.06718 valid_loss: 0.08023 test_loss: 0.09096 \n",
      "[228/300] train_loss: 0.06661 valid_loss: 0.08011 test_loss: 0.09190 \n",
      "[229/300] train_loss: 0.06563 valid_loss: 0.07950 test_loss: 0.08987 \n",
      "[230/300] train_loss: 0.06723 valid_loss: 0.08003 test_loss: 0.08925 \n",
      "[231/300] train_loss: 0.06555 valid_loss: 0.07972 test_loss: 0.08917 \n",
      "[232/300] train_loss: 0.06671 valid_loss: 0.07896 test_loss: 0.09015 \n",
      "Validation loss decreased (0.079026 --> 0.078961).  Saving model ...\n",
      "[233/300] train_loss: 0.06655 valid_loss: 0.08159 test_loss: 0.09002 \n",
      "[234/300] train_loss: 0.06579 valid_loss: 0.08007 test_loss: 0.09034 \n",
      "[235/300] train_loss: 0.06538 valid_loss: 0.07982 test_loss: 0.08996 \n",
      "[236/300] train_loss: 0.06475 valid_loss: 0.08321 test_loss: 0.09142 \n",
      "[237/300] train_loss: 0.06748 valid_loss: 0.07898 test_loss: 0.09037 \n",
      "[238/300] train_loss: 0.06463 valid_loss: 0.08161 test_loss: 0.09033 \n",
      "[239/300] train_loss: 0.06667 valid_loss: 0.08000 test_loss: 0.08906 \n",
      "[240/300] train_loss: 0.06588 valid_loss: 0.08077 test_loss: 0.08920 \n",
      "[241/300] train_loss: 0.06461 valid_loss: 0.07995 test_loss: 0.08895 \n",
      "[242/300] train_loss: 0.06342 valid_loss: 0.07911 test_loss: 0.09036 \n",
      "[243/300] train_loss: 0.06616 valid_loss: 0.07967 test_loss: 0.08874 \n",
      "[244/300] train_loss: 0.06355 valid_loss: 0.07964 test_loss: 0.08839 \n",
      "[245/300] train_loss: 0.06653 valid_loss: 0.07885 test_loss: 0.08759 \n",
      "Validation loss decreased (0.078961 --> 0.078845).  Saving model ...\n",
      "[246/300] train_loss: 0.06412 valid_loss: 0.07987 test_loss: 0.08875 \n",
      "[247/300] train_loss: 0.06514 valid_loss: 0.08022 test_loss: 0.08817 \n",
      "[248/300] train_loss: 0.06515 valid_loss: 0.07987 test_loss: 0.08894 \n",
      "[249/300] train_loss: 0.06240 valid_loss: 0.07934 test_loss: 0.08906 \n",
      "[250/300] train_loss: 0.06413 valid_loss: 0.07752 test_loss: 0.08834 \n",
      "Validation loss decreased (0.078845 --> 0.077525).  Saving model ...\n",
      "[251/300] train_loss: 0.06386 valid_loss: 0.07938 test_loss: 0.08824 \n",
      "[252/300] train_loss: 0.06472 valid_loss: 0.07965 test_loss: 0.08818 \n",
      "[253/300] train_loss: 0.06394 valid_loss: 0.07967 test_loss: 0.08781 \n",
      "[254/300] train_loss: 0.06449 valid_loss: 0.07836 test_loss: 0.08876 \n",
      "[255/300] train_loss: 0.06336 valid_loss: 0.08071 test_loss: 0.08910 \n",
      "[256/300] train_loss: 0.06363 valid_loss: 0.07834 test_loss: 0.08879 \n",
      "[257/300] train_loss: 0.06604 valid_loss: 0.07911 test_loss: 0.08771 \n",
      "[258/300] train_loss: 0.06324 valid_loss: 0.07869 test_loss: 0.08748 \n",
      "[259/300] train_loss: 0.06507 valid_loss: 0.07920 test_loss: 0.08775 \n",
      "[260/300] train_loss: 0.06363 valid_loss: 0.07934 test_loss: 0.08875 \n",
      "[261/300] train_loss: 0.06565 valid_loss: 0.08046 test_loss: 0.08836 \n",
      "[262/300] train_loss: 0.06272 valid_loss: 0.07815 test_loss: 0.08887 \n",
      "[263/300] train_loss: 0.06196 valid_loss: 0.08093 test_loss: 0.08897 \n",
      "[264/300] train_loss: 0.06260 valid_loss: 0.07992 test_loss: 0.08832 \n",
      "[265/300] train_loss: 0.06313 valid_loss: 0.07980 test_loss: 0.08732 \n",
      "[266/300] train_loss: 0.06138 valid_loss: 0.08146 test_loss: 0.08814 \n",
      "[267/300] train_loss: 0.06358 valid_loss: 0.08007 test_loss: 0.08835 \n",
      "[268/300] train_loss: 0.06490 valid_loss: 0.08022 test_loss: 0.08773 \n",
      "[269/300] train_loss: 0.06368 valid_loss: 0.07851 test_loss: 0.08653 \n",
      "[270/300] train_loss: 0.06295 valid_loss: 0.07842 test_loss: 0.08794 \n",
      "[271/300] train_loss: 0.06401 valid_loss: 0.07783 test_loss: 0.08774 \n",
      "[272/300] train_loss: 0.06357 valid_loss: 0.07740 test_loss: 0.08709 \n",
      "Validation loss decreased (0.077525 --> 0.077404).  Saving model ...\n",
      "[273/300] train_loss: 0.06354 valid_loss: 0.07722 test_loss: 0.08868 \n",
      "Validation loss decreased (0.077404 --> 0.077224).  Saving model ...\n",
      "[274/300] train_loss: 0.06404 valid_loss: 0.07732 test_loss: 0.08742 \n",
      "[275/300] train_loss: 0.06355 valid_loss: 0.07929 test_loss: 0.08699 \n",
      "[276/300] train_loss: 0.06252 valid_loss: 0.08002 test_loss: 0.08944 \n",
      "[277/300] train_loss: 0.06157 valid_loss: 0.07853 test_loss: 0.08860 \n",
      "[278/300] train_loss: 0.06206 valid_loss: 0.07781 test_loss: 0.08917 \n",
      "[279/300] train_loss: 0.06124 valid_loss: 0.07785 test_loss: 0.08791 \n",
      "[280/300] train_loss: 0.06238 valid_loss: 0.07733 test_loss: 0.08729 \n",
      "[281/300] train_loss: 0.06194 valid_loss: 0.07840 test_loss: 0.08652 \n",
      "[282/300] train_loss: 0.06256 valid_loss: 0.07830 test_loss: 0.08762 \n",
      "[283/300] train_loss: 0.06209 valid_loss: 0.07850 test_loss: 0.08794 \n",
      "[284/300] train_loss: 0.06260 valid_loss: 0.07745 test_loss: 0.08645 \n",
      "[285/300] train_loss: 0.06271 valid_loss: 0.07653 test_loss: 0.08642 \n",
      "Validation loss decreased (0.077224 --> 0.076530).  Saving model ...\n",
      "[286/300] train_loss: 0.06348 valid_loss: 0.07899 test_loss: 0.08697 \n",
      "[287/300] train_loss: 0.06307 valid_loss: 0.07860 test_loss: 0.08712 \n",
      "[288/300] train_loss: 0.06087 valid_loss: 0.07767 test_loss: 0.08724 \n",
      "[289/300] train_loss: 0.06230 valid_loss: 0.07782 test_loss: 0.08671 \n",
      "[290/300] train_loss: 0.06285 valid_loss: 0.07806 test_loss: 0.08673 \n",
      "[291/300] train_loss: 0.06142 valid_loss: 0.07800 test_loss: 0.08797 \n",
      "[292/300] train_loss: 0.06048 valid_loss: 0.07700 test_loss: 0.08718 \n",
      "[293/300] train_loss: 0.06160 valid_loss: 0.07874 test_loss: 0.08872 \n",
      "[294/300] train_loss: 0.06152 valid_loss: 0.07785 test_loss: 0.08791 \n",
      "[295/300] train_loss: 0.06117 valid_loss: 0.07727 test_loss: 0.08787 \n",
      "[296/300] train_loss: 0.06133 valid_loss: 0.07819 test_loss: 0.08803 \n",
      "[297/300] train_loss: 0.06076 valid_loss: 0.07681 test_loss: 0.08677 \n",
      "[298/300] train_loss: 0.05968 valid_loss: 0.07800 test_loss: 0.08691 \n",
      "[299/300] train_loss: 0.06134 valid_loss: 0.07787 test_loss: 0.08695 \n",
      "[300/300] train_loss: 0.05993 valid_loss: 0.07789 test_loss: 0.08725 \n",
      "TRAINING MODEL 17\n",
      "[  1/300] train_loss: 0.63547 valid_loss: 0.52924 test_loss: 0.53745 \n",
      "Validation loss decreased (inf --> 0.529237).  Saving model ...\n",
      "[  2/300] train_loss: 0.42830 valid_loss: 0.37887 test_loss: 0.38685 \n",
      "Validation loss decreased (0.529237 --> 0.378866).  Saving model ...\n",
      "[  3/300] train_loss: 0.31721 valid_loss: 0.30698 test_loss: 0.31723 \n",
      "Validation loss decreased (0.378866 --> 0.306977).  Saving model ...\n",
      "[  4/300] train_loss: 0.26072 valid_loss: 0.26309 test_loss: 0.27537 \n",
      "Validation loss decreased (0.306977 --> 0.263090).  Saving model ...\n",
      "[  5/300] train_loss: 0.22902 valid_loss: 0.23475 test_loss: 0.24798 \n",
      "Validation loss decreased (0.263090 --> 0.234745).  Saving model ...\n",
      "[  6/300] train_loss: 0.20811 valid_loss: 0.21803 test_loss: 0.23275 \n",
      "Validation loss decreased (0.234745 --> 0.218032).  Saving model ...\n",
      "[  7/300] train_loss: 0.19185 valid_loss: 0.20058 test_loss: 0.21355 \n",
      "Validation loss decreased (0.218032 --> 0.200581).  Saving model ...\n",
      "[  8/300] train_loss: 0.18821 valid_loss: 0.18858 test_loss: 0.20323 \n",
      "Validation loss decreased (0.200581 --> 0.188583).  Saving model ...\n",
      "[  9/300] train_loss: 0.17723 valid_loss: 0.18115 test_loss: 0.19571 \n",
      "Validation loss decreased (0.188583 --> 0.181152).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16977 valid_loss: 0.17405 test_loss: 0.18765 \n",
      "Validation loss decreased (0.181152 --> 0.174048).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16581 valid_loss: 0.16992 test_loss: 0.18257 \n",
      "Validation loss decreased (0.174048 --> 0.169924).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15903 valid_loss: 0.16143 test_loss: 0.17312 \n",
      "Validation loss decreased (0.169924 --> 0.161426).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15870 valid_loss: 0.15964 test_loss: 0.17178 \n",
      "Validation loss decreased (0.161426 --> 0.159640).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15189 valid_loss: 0.15632 test_loss: 0.16861 \n",
      "Validation loss decreased (0.159640 --> 0.156321).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14700 valid_loss: 0.15709 test_loss: 0.16881 \n",
      "[ 16/300] train_loss: 0.14193 valid_loss: 0.15526 test_loss: 0.16382 \n",
      "Validation loss decreased (0.156321 --> 0.155262).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14145 valid_loss: 0.15495 test_loss: 0.16175 \n",
      "Validation loss decreased (0.155262 --> 0.154953).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14318 valid_loss: 0.15400 test_loss: 0.16303 \n",
      "Validation loss decreased (0.154953 --> 0.153999).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13693 valid_loss: 0.15222 test_loss: 0.15983 \n",
      "Validation loss decreased (0.153999 --> 0.152225).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13423 valid_loss: 0.14934 test_loss: 0.15926 \n",
      "Validation loss decreased (0.152225 --> 0.149343).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13261 valid_loss: 0.14775 test_loss: 0.15591 \n",
      "Validation loss decreased (0.149343 --> 0.147751).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13289 valid_loss: 0.16303 test_loss: 0.16750 \n",
      "[ 23/300] train_loss: 0.12737 valid_loss: 0.14759 test_loss: 0.15464 \n",
      "Validation loss decreased (0.147751 --> 0.147590).  Saving model ...\n",
      "[ 24/300] train_loss: 0.13162 valid_loss: 0.15087 test_loss: 0.15650 \n",
      "[ 25/300] train_loss: 0.12980 valid_loss: 0.14339 test_loss: 0.15344 \n",
      "Validation loss decreased (0.147590 --> 0.143390).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12630 valid_loss: 0.14452 test_loss: 0.15274 \n",
      "[ 27/300] train_loss: 0.12811 valid_loss: 0.14782 test_loss: 0.15705 \n",
      "[ 28/300] train_loss: 0.12410 valid_loss: 0.13749 test_loss: 0.14859 \n",
      "Validation loss decreased (0.143390 --> 0.137488).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12163 valid_loss: 0.13624 test_loss: 0.14711 \n",
      "Validation loss decreased (0.137488 --> 0.136237).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11956 valid_loss: 0.13528 test_loss: 0.14550 \n",
      "Validation loss decreased (0.136237 --> 0.135284).  Saving model ...\n",
      "[ 31/300] train_loss: 0.12095 valid_loss: 0.13354 test_loss: 0.14543 \n",
      "Validation loss decreased (0.135284 --> 0.133541).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11852 valid_loss: 0.13403 test_loss: 0.14853 \n",
      "[ 33/300] train_loss: 0.11883 valid_loss: 0.12880 test_loss: 0.14037 \n",
      "Validation loss decreased (0.133541 --> 0.128800).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11700 valid_loss: 0.13595 test_loss: 0.14824 \n",
      "[ 35/300] train_loss: 0.11621 valid_loss: 0.13350 test_loss: 0.14436 \n",
      "[ 36/300] train_loss: 0.11901 valid_loss: 0.12559 test_loss: 0.13641 \n",
      "Validation loss decreased (0.128800 --> 0.125586).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11358 valid_loss: 0.12366 test_loss: 0.13508 \n",
      "Validation loss decreased (0.125586 --> 0.123659).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11229 valid_loss: 0.12752 test_loss: 0.13808 \n",
      "[ 39/300] train_loss: 0.11285 valid_loss: 0.12397 test_loss: 0.13530 \n",
      "[ 40/300] train_loss: 0.11030 valid_loss: 0.12363 test_loss: 0.13690 \n",
      "Validation loss decreased (0.123659 --> 0.123629).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11237 valid_loss: 0.12883 test_loss: 0.13949 \n",
      "[ 42/300] train_loss: 0.11192 valid_loss: 0.12176 test_loss: 0.13351 \n",
      "Validation loss decreased (0.123629 --> 0.121757).  Saving model ...\n",
      "[ 43/300] train_loss: 0.11222 valid_loss: 0.12240 test_loss: 0.13339 \n",
      "[ 44/300] train_loss: 0.10978 valid_loss: 0.11806 test_loss: 0.13172 \n",
      "Validation loss decreased (0.121757 --> 0.118058).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10914 valid_loss: 0.12461 test_loss: 0.13381 \n",
      "[ 46/300] train_loss: 0.10709 valid_loss: 0.11965 test_loss: 0.13096 \n",
      "[ 47/300] train_loss: 0.10668 valid_loss: 0.11670 test_loss: 0.12865 \n",
      "Validation loss decreased (0.118058 --> 0.116700).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10405 valid_loss: 0.11892 test_loss: 0.12973 \n",
      "[ 49/300] train_loss: 0.10469 valid_loss: 0.11912 test_loss: 0.12991 \n",
      "[ 50/300] train_loss: 0.10374 valid_loss: 0.11478 test_loss: 0.12893 \n",
      "Validation loss decreased (0.116700 --> 0.114776).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10577 valid_loss: 0.12299 test_loss: 0.13237 \n",
      "[ 52/300] train_loss: 0.10432 valid_loss: 0.11483 test_loss: 0.12615 \n",
      "[ 53/300] train_loss: 0.10209 valid_loss: 0.11872 test_loss: 0.13095 \n",
      "[ 54/300] train_loss: 0.10222 valid_loss: 0.11452 test_loss: 0.12623 \n",
      "Validation loss decreased (0.114776 --> 0.114524).  Saving model ...\n",
      "[ 55/300] train_loss: 0.10083 valid_loss: 0.11230 test_loss: 0.12365 \n",
      "Validation loss decreased (0.114524 --> 0.112296).  Saving model ...\n",
      "[ 56/300] train_loss: 0.10021 valid_loss: 0.11347 test_loss: 0.12702 \n",
      "[ 57/300] train_loss: 0.10165 valid_loss: 0.11106 test_loss: 0.12329 \n",
      "Validation loss decreased (0.112296 --> 0.111056).  Saving model ...\n",
      "[ 58/300] train_loss: 0.10143 valid_loss: 0.11078 test_loss: 0.12340 \n",
      "Validation loss decreased (0.111056 --> 0.110781).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09978 valid_loss: 0.10906 test_loss: 0.12158 \n",
      "Validation loss decreased (0.110781 --> 0.109061).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09828 valid_loss: 0.10823 test_loss: 0.12161 \n",
      "Validation loss decreased (0.109061 --> 0.108233).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09794 valid_loss: 0.10990 test_loss: 0.12240 \n",
      "[ 62/300] train_loss: 0.10051 valid_loss: 0.10923 test_loss: 0.12129 \n",
      "[ 63/300] train_loss: 0.10023 valid_loss: 0.11196 test_loss: 0.12285 \n",
      "[ 64/300] train_loss: 0.09618 valid_loss: 0.10717 test_loss: 0.11990 \n",
      "Validation loss decreased (0.108233 --> 0.107167).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09540 valid_loss: 0.10568 test_loss: 0.11870 \n",
      "Validation loss decreased (0.107167 --> 0.105681).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09727 valid_loss: 0.11124 test_loss: 0.12365 \n",
      "[ 67/300] train_loss: 0.09520 valid_loss: 0.10315 test_loss: 0.11772 \n",
      "Validation loss decreased (0.105681 --> 0.103151).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09913 valid_loss: 0.10358 test_loss: 0.11656 \n",
      "[ 69/300] train_loss: 0.09360 valid_loss: 0.10479 test_loss: 0.11662 \n",
      "[ 70/300] train_loss: 0.09493 valid_loss: 0.10327 test_loss: 0.11747 \n",
      "[ 71/300] train_loss: 0.09381 valid_loss: 0.10336 test_loss: 0.11667 \n",
      "[ 72/300] train_loss: 0.09400 valid_loss: 0.10157 test_loss: 0.11553 \n",
      "Validation loss decreased (0.103151 --> 0.101574).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09515 valid_loss: 0.10187 test_loss: 0.11542 \n",
      "[ 74/300] train_loss: 0.09398 valid_loss: 0.10256 test_loss: 0.11501 \n",
      "[ 75/300] train_loss: 0.09503 valid_loss: 0.10011 test_loss: 0.11364 \n",
      "Validation loss decreased (0.101574 --> 0.100114).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09580 valid_loss: 0.10335 test_loss: 0.11575 \n",
      "[ 77/300] train_loss: 0.09496 valid_loss: 0.10160 test_loss: 0.11466 \n",
      "[ 78/300] train_loss: 0.09359 valid_loss: 0.10284 test_loss: 0.11638 \n",
      "[ 79/300] train_loss: 0.09059 valid_loss: 0.10086 test_loss: 0.11537 \n",
      "[ 80/300] train_loss: 0.09183 valid_loss: 0.10050 test_loss: 0.11287 \n",
      "[ 81/300] train_loss: 0.09281 valid_loss: 0.09953 test_loss: 0.11253 \n",
      "Validation loss decreased (0.100114 --> 0.099531).  Saving model ...\n",
      "[ 82/300] train_loss: 0.08898 valid_loss: 0.10038 test_loss: 0.11302 \n",
      "[ 83/300] train_loss: 0.09136 valid_loss: 0.10137 test_loss: 0.11476 \n",
      "[ 84/300] train_loss: 0.08937 valid_loss: 0.10159 test_loss: 0.11316 \n",
      "[ 85/300] train_loss: 0.08957 valid_loss: 0.10003 test_loss: 0.11323 \n",
      "[ 86/300] train_loss: 0.09126 valid_loss: 0.10160 test_loss: 0.11423 \n",
      "[ 87/300] train_loss: 0.09077 valid_loss: 0.09808 test_loss: 0.11049 \n",
      "Validation loss decreased (0.099531 --> 0.098077).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08752 valid_loss: 0.09661 test_loss: 0.11236 \n",
      "Validation loss decreased (0.098077 --> 0.096614).  Saving model ...\n",
      "[ 89/300] train_loss: 0.08776 valid_loss: 0.10051 test_loss: 0.11369 \n",
      "[ 90/300] train_loss: 0.08727 valid_loss: 0.09884 test_loss: 0.11247 \n",
      "[ 91/300] train_loss: 0.08693 valid_loss: 0.09551 test_loss: 0.10939 \n",
      "Validation loss decreased (0.096614 --> 0.095512).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08930 valid_loss: 0.09677 test_loss: 0.10992 \n",
      "[ 93/300] train_loss: 0.08695 valid_loss: 0.09670 test_loss: 0.11077 \n",
      "[ 94/300] train_loss: 0.08863 valid_loss: 0.10113 test_loss: 0.11300 \n",
      "[ 95/300] train_loss: 0.08759 valid_loss: 0.09723 test_loss: 0.10963 \n",
      "[ 96/300] train_loss: 0.09045 valid_loss: 0.09706 test_loss: 0.11014 \n",
      "[ 97/300] train_loss: 0.08663 valid_loss: 0.09619 test_loss: 0.10919 \n",
      "[ 98/300] train_loss: 0.08623 valid_loss: 0.09596 test_loss: 0.10781 \n",
      "[ 99/300] train_loss: 0.08555 valid_loss: 0.09435 test_loss: 0.10877 \n",
      "Validation loss decreased (0.095512 --> 0.094354).  Saving model ...\n",
      "[100/300] train_loss: 0.08784 valid_loss: 0.09288 test_loss: 0.10677 \n",
      "Validation loss decreased (0.094354 --> 0.092883).  Saving model ...\n",
      "[101/300] train_loss: 0.08546 valid_loss: 0.09503 test_loss: 0.10737 \n",
      "[102/300] train_loss: 0.08874 valid_loss: 0.09331 test_loss: 0.10661 \n",
      "[103/300] train_loss: 0.08648 valid_loss: 0.09343 test_loss: 0.10778 \n",
      "[104/300] train_loss: 0.08652 valid_loss: 0.09364 test_loss: 0.10779 \n",
      "[105/300] train_loss: 0.08306 valid_loss: 0.09339 test_loss: 0.10708 \n",
      "[106/300] train_loss: 0.08549 valid_loss: 0.09368 test_loss: 0.10630 \n",
      "[107/300] train_loss: 0.08474 valid_loss: 0.09250 test_loss: 0.10515 \n",
      "Validation loss decreased (0.092883 --> 0.092505).  Saving model ...\n",
      "[108/300] train_loss: 0.08679 valid_loss: 0.09273 test_loss: 0.10550 \n",
      "[109/300] train_loss: 0.08500 valid_loss: 0.09138 test_loss: 0.10401 \n",
      "Validation loss decreased (0.092505 --> 0.091382).  Saving model ...\n",
      "[110/300] train_loss: 0.08350 valid_loss: 0.09343 test_loss: 0.10633 \n",
      "[111/300] train_loss: 0.08166 valid_loss: 0.09352 test_loss: 0.10661 \n",
      "[112/300] train_loss: 0.08460 valid_loss: 0.09093 test_loss: 0.10450 \n",
      "Validation loss decreased (0.091382 --> 0.090928).  Saving model ...\n",
      "[113/300] train_loss: 0.08468 valid_loss: 0.09301 test_loss: 0.10495 \n",
      "[114/300] train_loss: 0.08308 valid_loss: 0.09152 test_loss: 0.10442 \n",
      "[115/300] train_loss: 0.08328 valid_loss: 0.09262 test_loss: 0.10567 \n",
      "[116/300] train_loss: 0.08237 valid_loss: 0.09287 test_loss: 0.10571 \n",
      "[117/300] train_loss: 0.08152 valid_loss: 0.09212 test_loss: 0.10525 \n",
      "[118/300] train_loss: 0.08114 valid_loss: 0.09113 test_loss: 0.10446 \n",
      "[119/300] train_loss: 0.08350 valid_loss: 0.09330 test_loss: 0.10570 \n",
      "[120/300] train_loss: 0.08194 valid_loss: 0.09027 test_loss: 0.10373 \n",
      "Validation loss decreased (0.090928 --> 0.090274).  Saving model ...\n",
      "[121/300] train_loss: 0.08327 valid_loss: 0.09124 test_loss: 0.10504 \n",
      "[122/300] train_loss: 0.08059 valid_loss: 0.09066 test_loss: 0.10326 \n",
      "[123/300] train_loss: 0.08132 valid_loss: 0.09009 test_loss: 0.10450 \n",
      "Validation loss decreased (0.090274 --> 0.090092).  Saving model ...\n",
      "[124/300] train_loss: 0.08299 valid_loss: 0.09128 test_loss: 0.10479 \n",
      "[125/300] train_loss: 0.08302 valid_loss: 0.09174 test_loss: 0.10425 \n",
      "[126/300] train_loss: 0.08130 valid_loss: 0.09015 test_loss: 0.10420 \n",
      "[127/300] train_loss: 0.07880 valid_loss: 0.09129 test_loss: 0.10463 \n",
      "[128/300] train_loss: 0.08039 valid_loss: 0.09038 test_loss: 0.10316 \n",
      "[129/300] train_loss: 0.07900 valid_loss: 0.08975 test_loss: 0.10266 \n",
      "Validation loss decreased (0.090092 --> 0.089747).  Saving model ...\n",
      "[130/300] train_loss: 0.08183 valid_loss: 0.08851 test_loss: 0.10077 \n",
      "Validation loss decreased (0.089747 --> 0.088509).  Saving model ...\n",
      "[131/300] train_loss: 0.08107 valid_loss: 0.09024 test_loss: 0.10371 \n",
      "[132/300] train_loss: 0.07766 valid_loss: 0.08976 test_loss: 0.10175 \n",
      "[133/300] train_loss: 0.07901 valid_loss: 0.08979 test_loss: 0.10328 \n",
      "[134/300] train_loss: 0.07804 valid_loss: 0.09079 test_loss: 0.10357 \n",
      "[135/300] train_loss: 0.07992 valid_loss: 0.08873 test_loss: 0.10179 \n",
      "[136/300] train_loss: 0.07683 valid_loss: 0.08887 test_loss: 0.10113 \n",
      "[137/300] train_loss: 0.08128 valid_loss: 0.08949 test_loss: 0.10180 \n",
      "[138/300] train_loss: 0.07877 valid_loss: 0.08746 test_loss: 0.10149 \n",
      "Validation loss decreased (0.088509 --> 0.087458).  Saving model ...\n",
      "[139/300] train_loss: 0.07965 valid_loss: 0.08810 test_loss: 0.10137 \n",
      "[140/300] train_loss: 0.08041 valid_loss: 0.08760 test_loss: 0.10098 \n",
      "[141/300] train_loss: 0.07707 valid_loss: 0.08958 test_loss: 0.10308 \n",
      "[142/300] train_loss: 0.07778 valid_loss: 0.09048 test_loss: 0.10269 \n",
      "[143/300] train_loss: 0.07635 valid_loss: 0.08925 test_loss: 0.10138 \n",
      "[144/300] train_loss: 0.07601 valid_loss: 0.09176 test_loss: 0.10385 \n",
      "[145/300] train_loss: 0.07724 valid_loss: 0.08835 test_loss: 0.09969 \n",
      "[146/300] train_loss: 0.07989 valid_loss: 0.08844 test_loss: 0.09929 \n",
      "[147/300] train_loss: 0.07689 valid_loss: 0.08971 test_loss: 0.10045 \n",
      "[148/300] train_loss: 0.07722 valid_loss: 0.08929 test_loss: 0.10123 \n",
      "[149/300] train_loss: 0.07810 valid_loss: 0.09125 test_loss: 0.10131 \n",
      "[150/300] train_loss: 0.07804 valid_loss: 0.08596 test_loss: 0.09927 \n",
      "Validation loss decreased (0.087458 --> 0.085961).  Saving model ...\n",
      "[151/300] train_loss: 0.07614 valid_loss: 0.08614 test_loss: 0.09895 \n",
      "[152/300] train_loss: 0.07777 valid_loss: 0.08897 test_loss: 0.09985 \n",
      "[153/300] train_loss: 0.07546 valid_loss: 0.08678 test_loss: 0.09887 \n",
      "[154/300] train_loss: 0.07860 valid_loss: 0.08571 test_loss: 0.09889 \n",
      "Validation loss decreased (0.085961 --> 0.085713).  Saving model ...\n",
      "[155/300] train_loss: 0.07405 valid_loss: 0.08678 test_loss: 0.09945 \n",
      "[156/300] train_loss: 0.07602 valid_loss: 0.08743 test_loss: 0.10080 \n",
      "[157/300] train_loss: 0.07740 valid_loss: 0.08603 test_loss: 0.09739 \n",
      "[158/300] train_loss: 0.07461 valid_loss: 0.08644 test_loss: 0.09916 \n",
      "[159/300] train_loss: 0.07465 valid_loss: 0.08704 test_loss: 0.09846 \n",
      "[160/300] train_loss: 0.07412 valid_loss: 0.08634 test_loss: 0.09925 \n",
      "[161/300] train_loss: 0.07727 valid_loss: 0.08776 test_loss: 0.09958 \n",
      "[162/300] train_loss: 0.07668 valid_loss: 0.08869 test_loss: 0.10065 \n",
      "[163/300] train_loss: 0.07406 valid_loss: 0.08620 test_loss: 0.09736 \n",
      "[164/300] train_loss: 0.07627 valid_loss: 0.08494 test_loss: 0.09752 \n",
      "Validation loss decreased (0.085713 --> 0.084943).  Saving model ...\n",
      "[165/300] train_loss: 0.07464 valid_loss: 0.08631 test_loss: 0.09883 \n",
      "[166/300] train_loss: 0.07208 valid_loss: 0.08537 test_loss: 0.09746 \n",
      "[167/300] train_loss: 0.07402 valid_loss: 0.08465 test_loss: 0.09773 \n",
      "Validation loss decreased (0.084943 --> 0.084649).  Saving model ...\n",
      "[168/300] train_loss: 0.07516 valid_loss: 0.08567 test_loss: 0.09857 \n",
      "[169/300] train_loss: 0.07233 valid_loss: 0.08458 test_loss: 0.09634 \n",
      "Validation loss decreased (0.084649 --> 0.084576).  Saving model ...\n",
      "[170/300] train_loss: 0.07674 valid_loss: 0.08564 test_loss: 0.09590 \n",
      "[171/300] train_loss: 0.07532 valid_loss: 0.08633 test_loss: 0.09862 \n",
      "[172/300] train_loss: 0.07579 valid_loss: 0.08646 test_loss: 0.09815 \n",
      "[173/300] train_loss: 0.07365 valid_loss: 0.08593 test_loss: 0.09869 \n",
      "[174/300] train_loss: 0.07416 valid_loss: 0.08607 test_loss: 0.09797 \n",
      "[175/300] train_loss: 0.07307 valid_loss: 0.08419 test_loss: 0.09609 \n",
      "Validation loss decreased (0.084576 --> 0.084188).  Saving model ...\n",
      "[176/300] train_loss: 0.07152 valid_loss: 0.08481 test_loss: 0.09614 \n",
      "[177/300] train_loss: 0.07196 valid_loss: 0.08467 test_loss: 0.09561 \n",
      "[178/300] train_loss: 0.07349 valid_loss: 0.08501 test_loss: 0.09670 \n",
      "[179/300] train_loss: 0.07313 valid_loss: 0.08506 test_loss: 0.09656 \n",
      "[180/300] train_loss: 0.07340 valid_loss: 0.08424 test_loss: 0.09727 \n",
      "[181/300] train_loss: 0.07421 valid_loss: 0.08505 test_loss: 0.09592 \n",
      "[182/300] train_loss: 0.07232 valid_loss: 0.08522 test_loss: 0.09816 \n",
      "[183/300] train_loss: 0.07325 valid_loss: 0.08452 test_loss: 0.09718 \n",
      "[184/300] train_loss: 0.07327 valid_loss: 0.08404 test_loss: 0.09513 \n",
      "Validation loss decreased (0.084188 --> 0.084038).  Saving model ...\n",
      "[185/300] train_loss: 0.07041 valid_loss: 0.08409 test_loss: 0.09541 \n",
      "[186/300] train_loss: 0.07154 valid_loss: 0.08379 test_loss: 0.09547 \n",
      "Validation loss decreased (0.084038 --> 0.083789).  Saving model ...\n",
      "[187/300] train_loss: 0.07478 valid_loss: 0.08329 test_loss: 0.09511 \n",
      "Validation loss decreased (0.083789 --> 0.083287).  Saving model ...\n",
      "[188/300] train_loss: 0.07105 valid_loss: 0.08484 test_loss: 0.09613 \n",
      "[189/300] train_loss: 0.07004 valid_loss: 0.08469 test_loss: 0.09637 \n",
      "[190/300] train_loss: 0.06962 valid_loss: 0.08615 test_loss: 0.09586 \n",
      "[191/300] train_loss: 0.07080 valid_loss: 0.08404 test_loss: 0.09458 \n",
      "[192/300] train_loss: 0.07041 valid_loss: 0.08230 test_loss: 0.09470 \n",
      "Validation loss decreased (0.083287 --> 0.082296).  Saving model ...\n",
      "[193/300] train_loss: 0.07343 valid_loss: 0.08416 test_loss: 0.09463 \n",
      "[194/300] train_loss: 0.07286 valid_loss: 0.08226 test_loss: 0.09282 \n",
      "Validation loss decreased (0.082296 --> 0.082265).  Saving model ...\n",
      "[195/300] train_loss: 0.07229 valid_loss: 0.08406 test_loss: 0.09461 \n",
      "[196/300] train_loss: 0.07045 valid_loss: 0.08376 test_loss: 0.09454 \n",
      "[197/300] train_loss: 0.06879 valid_loss: 0.08199 test_loss: 0.09387 \n",
      "Validation loss decreased (0.082265 --> 0.081990).  Saving model ...\n",
      "[198/300] train_loss: 0.07140 valid_loss: 0.08334 test_loss: 0.09406 \n",
      "[199/300] train_loss: 0.06889 valid_loss: 0.08169 test_loss: 0.09451 \n",
      "Validation loss decreased (0.081990 --> 0.081688).  Saving model ...\n",
      "[200/300] train_loss: 0.07063 valid_loss: 0.08258 test_loss: 0.09387 \n",
      "[201/300] train_loss: 0.07028 valid_loss: 0.08259 test_loss: 0.09307 \n",
      "[202/300] train_loss: 0.07024 valid_loss: 0.08292 test_loss: 0.09458 \n",
      "[203/300] train_loss: 0.06823 valid_loss: 0.08238 test_loss: 0.09372 \n",
      "[204/300] train_loss: 0.07065 valid_loss: 0.08462 test_loss: 0.09581 \n",
      "[205/300] train_loss: 0.07239 valid_loss: 0.08272 test_loss: 0.09304 \n",
      "[206/300] train_loss: 0.06977 valid_loss: 0.08185 test_loss: 0.09284 \n",
      "[207/300] train_loss: 0.06907 valid_loss: 0.08116 test_loss: 0.09341 \n",
      "Validation loss decreased (0.081688 --> 0.081163).  Saving model ...\n",
      "[208/300] train_loss: 0.06897 valid_loss: 0.08179 test_loss: 0.09306 \n",
      "[209/300] train_loss: 0.07127 valid_loss: 0.08294 test_loss: 0.09352 \n",
      "[210/300] train_loss: 0.06945 valid_loss: 0.08240 test_loss: 0.09353 \n",
      "[211/300] train_loss: 0.06949 valid_loss: 0.08348 test_loss: 0.09424 \n",
      "[212/300] train_loss: 0.06867 valid_loss: 0.08288 test_loss: 0.09378 \n",
      "[213/300] train_loss: 0.06943 valid_loss: 0.08330 test_loss: 0.09325 \n",
      "[214/300] train_loss: 0.06864 valid_loss: 0.08237 test_loss: 0.09381 \n",
      "[215/300] train_loss: 0.06961 valid_loss: 0.08201 test_loss: 0.09300 \n",
      "[216/300] train_loss: 0.06981 valid_loss: 0.08216 test_loss: 0.09261 \n",
      "[217/300] train_loss: 0.06851 valid_loss: 0.08284 test_loss: 0.09275 \n",
      "[218/300] train_loss: 0.06738 valid_loss: 0.08102 test_loss: 0.09169 \n",
      "Validation loss decreased (0.081163 --> 0.081023).  Saving model ...\n",
      "[219/300] train_loss: 0.06800 valid_loss: 0.08292 test_loss: 0.09356 \n",
      "[220/300] train_loss: 0.06901 valid_loss: 0.08287 test_loss: 0.09415 \n",
      "[221/300] train_loss: 0.07033 valid_loss: 0.08047 test_loss: 0.09151 \n",
      "Validation loss decreased (0.081023 --> 0.080472).  Saving model ...\n",
      "[222/300] train_loss: 0.06918 valid_loss: 0.08030 test_loss: 0.09035 \n",
      "Validation loss decreased (0.080472 --> 0.080303).  Saving model ...\n",
      "[223/300] train_loss: 0.06809 valid_loss: 0.08107 test_loss: 0.09251 \n",
      "[224/300] train_loss: 0.06840 valid_loss: 0.08196 test_loss: 0.09219 \n",
      "[225/300] train_loss: 0.06664 valid_loss: 0.08319 test_loss: 0.09319 \n",
      "[226/300] train_loss: 0.06878 valid_loss: 0.08115 test_loss: 0.09263 \n",
      "[227/300] train_loss: 0.06739 valid_loss: 0.08037 test_loss: 0.09227 \n",
      "[228/300] train_loss: 0.06706 valid_loss: 0.08126 test_loss: 0.09237 \n",
      "[229/300] train_loss: 0.06882 valid_loss: 0.08141 test_loss: 0.09179 \n",
      "[230/300] train_loss: 0.06857 valid_loss: 0.08045 test_loss: 0.09189 \n",
      "[231/300] train_loss: 0.06745 valid_loss: 0.08150 test_loss: 0.09207 \n",
      "[232/300] train_loss: 0.06673 valid_loss: 0.08038 test_loss: 0.09145 \n",
      "[233/300] train_loss: 0.06590 valid_loss: 0.08145 test_loss: 0.09204 \n",
      "[234/300] train_loss: 0.06650 valid_loss: 0.08158 test_loss: 0.09335 \n",
      "[235/300] train_loss: 0.07077 valid_loss: 0.07987 test_loss: 0.09105 \n",
      "Validation loss decreased (0.080303 --> 0.079873).  Saving model ...\n",
      "[236/300] train_loss: 0.06641 valid_loss: 0.08114 test_loss: 0.09121 \n",
      "[237/300] train_loss: 0.06613 valid_loss: 0.08066 test_loss: 0.09234 \n",
      "[238/300] train_loss: 0.06509 valid_loss: 0.07877 test_loss: 0.09095 \n",
      "Validation loss decreased (0.079873 --> 0.078771).  Saving model ...\n",
      "[239/300] train_loss: 0.06492 valid_loss: 0.08040 test_loss: 0.09079 \n",
      "[240/300] train_loss: 0.06601 valid_loss: 0.08055 test_loss: 0.09168 \n",
      "[241/300] train_loss: 0.06746 valid_loss: 0.07986 test_loss: 0.08978 \n",
      "[242/300] train_loss: 0.06651 valid_loss: 0.07928 test_loss: 0.08931 \n",
      "[243/300] train_loss: 0.06807 valid_loss: 0.07975 test_loss: 0.09011 \n",
      "[244/300] train_loss: 0.06524 valid_loss: 0.08052 test_loss: 0.09003 \n",
      "[245/300] train_loss: 0.06426 valid_loss: 0.08068 test_loss: 0.08984 \n",
      "[246/300] train_loss: 0.06510 valid_loss: 0.08077 test_loss: 0.09202 \n",
      "[247/300] train_loss: 0.06705 valid_loss: 0.07922 test_loss: 0.08814 \n",
      "[248/300] train_loss: 0.06734 valid_loss: 0.08087 test_loss: 0.09023 \n",
      "[249/300] train_loss: 0.06736 valid_loss: 0.07847 test_loss: 0.08899 \n",
      "Validation loss decreased (0.078771 --> 0.078466).  Saving model ...\n",
      "[250/300] train_loss: 0.06309 valid_loss: 0.07970 test_loss: 0.09005 \n",
      "[251/300] train_loss: 0.06566 valid_loss: 0.07901 test_loss: 0.08976 \n",
      "[252/300] train_loss: 0.06400 valid_loss: 0.07940 test_loss: 0.08966 \n",
      "[253/300] train_loss: 0.06766 valid_loss: 0.08031 test_loss: 0.09035 \n",
      "[254/300] train_loss: 0.06616 valid_loss: 0.07874 test_loss: 0.09010 \n",
      "[255/300] train_loss: 0.06490 valid_loss: 0.07979 test_loss: 0.08963 \n",
      "[256/300] train_loss: 0.06385 valid_loss: 0.07996 test_loss: 0.09088 \n",
      "[257/300] train_loss: 0.06517 valid_loss: 0.08010 test_loss: 0.09024 \n",
      "[258/300] train_loss: 0.06871 valid_loss: 0.07831 test_loss: 0.08846 \n",
      "Validation loss decreased (0.078466 --> 0.078308).  Saving model ...\n",
      "[259/300] train_loss: 0.06582 valid_loss: 0.07932 test_loss: 0.08978 \n",
      "[260/300] train_loss: 0.06426 valid_loss: 0.07984 test_loss: 0.09094 \n",
      "[261/300] train_loss: 0.06435 valid_loss: 0.08095 test_loss: 0.09072 \n",
      "[262/300] train_loss: 0.06305 valid_loss: 0.07830 test_loss: 0.08887 \n",
      "Validation loss decreased (0.078308 --> 0.078301).  Saving model ...\n",
      "[263/300] train_loss: 0.06519 valid_loss: 0.07825 test_loss: 0.08835 \n",
      "Validation loss decreased (0.078301 --> 0.078248).  Saving model ...\n",
      "[264/300] train_loss: 0.06523 valid_loss: 0.07753 test_loss: 0.08772 \n",
      "Validation loss decreased (0.078248 --> 0.077533).  Saving model ...\n",
      "[265/300] train_loss: 0.06421 valid_loss: 0.07977 test_loss: 0.08939 \n",
      "[266/300] train_loss: 0.06534 valid_loss: 0.07843 test_loss: 0.08909 \n",
      "[267/300] train_loss: 0.06479 valid_loss: 0.07964 test_loss: 0.09114 \n",
      "[268/300] train_loss: 0.06657 valid_loss: 0.07916 test_loss: 0.08951 \n",
      "[269/300] train_loss: 0.06265 valid_loss: 0.07905 test_loss: 0.08931 \n",
      "[270/300] train_loss: 0.06510 valid_loss: 0.07827 test_loss: 0.08705 \n",
      "[271/300] train_loss: 0.06483 valid_loss: 0.08143 test_loss: 0.09044 \n",
      "[272/300] train_loss: 0.06383 valid_loss: 0.08012 test_loss: 0.09041 \n",
      "[273/300] train_loss: 0.06518 valid_loss: 0.07754 test_loss: 0.08792 \n",
      "[274/300] train_loss: 0.06497 valid_loss: 0.07953 test_loss: 0.08872 \n",
      "[275/300] train_loss: 0.06473 valid_loss: 0.07725 test_loss: 0.08624 \n",
      "Validation loss decreased (0.077533 --> 0.077251).  Saving model ...\n",
      "[276/300] train_loss: 0.06265 valid_loss: 0.07836 test_loss: 0.08712 \n",
      "[277/300] train_loss: 0.06399 valid_loss: 0.07863 test_loss: 0.08796 \n",
      "[278/300] train_loss: 0.06177 valid_loss: 0.07843 test_loss: 0.08827 \n",
      "[279/300] train_loss: 0.06326 valid_loss: 0.07806 test_loss: 0.08783 \n",
      "[280/300] train_loss: 0.06355 valid_loss: 0.07962 test_loss: 0.08782 \n",
      "[281/300] train_loss: 0.06263 valid_loss: 0.07912 test_loss: 0.08958 \n",
      "[282/300] train_loss: 0.06368 valid_loss: 0.07788 test_loss: 0.08714 \n",
      "[283/300] train_loss: 0.06349 valid_loss: 0.07910 test_loss: 0.08811 \n",
      "[284/300] train_loss: 0.06183 valid_loss: 0.07758 test_loss: 0.08787 \n",
      "[285/300] train_loss: 0.06219 valid_loss: 0.07793 test_loss: 0.08685 \n",
      "[286/300] train_loss: 0.06231 valid_loss: 0.07857 test_loss: 0.08822 \n",
      "[287/300] train_loss: 0.06183 valid_loss: 0.07902 test_loss: 0.08800 \n",
      "[288/300] train_loss: 0.06238 valid_loss: 0.07912 test_loss: 0.08835 \n",
      "[289/300] train_loss: 0.06328 valid_loss: 0.07755 test_loss: 0.08818 \n",
      "[290/300] train_loss: 0.06318 valid_loss: 0.07808 test_loss: 0.08725 \n",
      "[291/300] train_loss: 0.06151 valid_loss: 0.07729 test_loss: 0.08719 \n",
      "[292/300] train_loss: 0.06369 valid_loss: 0.07820 test_loss: 0.08810 \n",
      "[293/300] train_loss: 0.06233 valid_loss: 0.07744 test_loss: 0.08843 \n",
      "[294/300] train_loss: 0.06292 valid_loss: 0.07605 test_loss: 0.08657 \n",
      "Validation loss decreased (0.077251 --> 0.076048).  Saving model ...\n",
      "[295/300] train_loss: 0.06367 valid_loss: 0.07775 test_loss: 0.08785 \n",
      "[296/300] train_loss: 0.06221 valid_loss: 0.07849 test_loss: 0.08720 \n",
      "[297/300] train_loss: 0.06181 valid_loss: 0.07806 test_loss: 0.08891 \n",
      "[298/300] train_loss: 0.06206 valid_loss: 0.07660 test_loss: 0.08690 \n",
      "[299/300] train_loss: 0.06204 valid_loss: 0.07751 test_loss: 0.08722 \n",
      "[300/300] train_loss: 0.06331 valid_loss: 0.07789 test_loss: 0.08861 \n",
      "TRAINING MODEL 18\n",
      "[  1/300] train_loss: 0.55938 valid_loss: 0.44730 test_loss: 0.45335 \n",
      "Validation loss decreased (inf --> 0.447303).  Saving model ...\n",
      "[  2/300] train_loss: 0.36233 valid_loss: 0.32830 test_loss: 0.33568 \n",
      "Validation loss decreased (0.447303 --> 0.328298).  Saving model ...\n",
      "[  3/300] train_loss: 0.27645 valid_loss: 0.27004 test_loss: 0.28440 \n",
      "Validation loss decreased (0.328298 --> 0.270038).  Saving model ...\n",
      "[  4/300] train_loss: 0.23278 valid_loss: 0.23472 test_loss: 0.24830 \n",
      "Validation loss decreased (0.270038 --> 0.234718).  Saving model ...\n",
      "[  5/300] train_loss: 0.20752 valid_loss: 0.20881 test_loss: 0.22398 \n",
      "Validation loss decreased (0.234718 --> 0.208807).  Saving model ...\n",
      "[  6/300] train_loss: 0.19459 valid_loss: 0.19464 test_loss: 0.20816 \n",
      "Validation loss decreased (0.208807 --> 0.194640).  Saving model ...\n",
      "[  7/300] train_loss: 0.18400 valid_loss: 0.18578 test_loss: 0.19985 \n",
      "Validation loss decreased (0.194640 --> 0.185783).  Saving model ...\n",
      "[  8/300] train_loss: 0.17648 valid_loss: 0.17612 test_loss: 0.19015 \n",
      "Validation loss decreased (0.185783 --> 0.176123).  Saving model ...\n",
      "[  9/300] train_loss: 0.16845 valid_loss: 0.16886 test_loss: 0.18414 \n",
      "Validation loss decreased (0.176123 --> 0.168861).  Saving model ...\n",
      "[ 10/300] train_loss: 0.15937 valid_loss: 0.16380 test_loss: 0.17851 \n",
      "Validation loss decreased (0.168861 --> 0.163800).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15408 valid_loss: 0.15741 test_loss: 0.17159 \n",
      "Validation loss decreased (0.163800 --> 0.157405).  Saving model ...\n",
      "[ 12/300] train_loss: 0.14872 valid_loss: 0.15208 test_loss: 0.16302 \n",
      "Validation loss decreased (0.157405 --> 0.152085).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14564 valid_loss: 0.15127 test_loss: 0.16187 \n",
      "Validation loss decreased (0.152085 --> 0.151267).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14344 valid_loss: 0.15111 test_loss: 0.16021 \n",
      "Validation loss decreased (0.151267 --> 0.151105).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14129 valid_loss: 0.15018 test_loss: 0.15971 \n",
      "Validation loss decreased (0.151105 --> 0.150182).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13288 valid_loss: 0.14874 test_loss: 0.15670 \n",
      "Validation loss decreased (0.150182 --> 0.148736).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13570 valid_loss: 0.14941 test_loss: 0.15835 \n",
      "[ 18/300] train_loss: 0.12988 valid_loss: 0.14699 test_loss: 0.15319 \n",
      "Validation loss decreased (0.148736 --> 0.146987).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13074 valid_loss: 0.14946 test_loss: 0.15567 \n",
      "[ 20/300] train_loss: 0.12593 valid_loss: 0.14311 test_loss: 0.15086 \n",
      "Validation loss decreased (0.146987 --> 0.143112).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12534 valid_loss: 0.14251 test_loss: 0.15052 \n",
      "Validation loss decreased (0.143112 --> 0.142510).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12330 valid_loss: 0.13554 test_loss: 0.14487 \n",
      "Validation loss decreased (0.142510 --> 0.135544).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12151 valid_loss: 0.14120 test_loss: 0.15058 \n",
      "[ 24/300] train_loss: 0.12160 valid_loss: 0.13275 test_loss: 0.14208 \n",
      "Validation loss decreased (0.135544 --> 0.132746).  Saving model ...\n",
      "[ 25/300] train_loss: 0.11886 valid_loss: 0.13498 test_loss: 0.14222 \n",
      "[ 26/300] train_loss: 0.11794 valid_loss: 0.13097 test_loss: 0.14058 \n",
      "Validation loss decreased (0.132746 --> 0.130965).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11406 valid_loss: 0.12850 test_loss: 0.13970 \n",
      "Validation loss decreased (0.130965 --> 0.128496).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11731 valid_loss: 0.13282 test_loss: 0.14066 \n",
      "[ 29/300] train_loss: 0.11605 valid_loss: 0.13114 test_loss: 0.13983 \n",
      "[ 30/300] train_loss: 0.11155 valid_loss: 0.13494 test_loss: 0.14555 \n",
      "[ 31/300] train_loss: 0.11228 valid_loss: 0.12766 test_loss: 0.13749 \n",
      "Validation loss decreased (0.128496 --> 0.127658).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11340 valid_loss: 0.12470 test_loss: 0.13570 \n",
      "Validation loss decreased (0.127658 --> 0.124702).  Saving model ...\n",
      "[ 33/300] train_loss: 0.10935 valid_loss: 0.12443 test_loss: 0.13379 \n",
      "Validation loss decreased (0.124702 --> 0.124432).  Saving model ...\n",
      "[ 34/300] train_loss: 0.10660 valid_loss: 0.12186 test_loss: 0.13365 \n",
      "Validation loss decreased (0.124432 --> 0.121865).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11031 valid_loss: 0.12580 test_loss: 0.13452 \n",
      "[ 36/300] train_loss: 0.10656 valid_loss: 0.12335 test_loss: 0.13273 \n",
      "[ 37/300] train_loss: 0.10998 valid_loss: 0.12254 test_loss: 0.13246 \n",
      "[ 38/300] train_loss: 0.10746 valid_loss: 0.12583 test_loss: 0.13499 \n",
      "[ 39/300] train_loss: 0.10658 valid_loss: 0.12321 test_loss: 0.13343 \n",
      "[ 40/300] train_loss: 0.10619 valid_loss: 0.11940 test_loss: 0.13037 \n",
      "Validation loss decreased (0.121865 --> 0.119401).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10332 valid_loss: 0.11699 test_loss: 0.12799 \n",
      "Validation loss decreased (0.119401 --> 0.116990).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10330 valid_loss: 0.11725 test_loss: 0.12873 \n",
      "[ 43/300] train_loss: 0.10274 valid_loss: 0.11499 test_loss: 0.12549 \n",
      "Validation loss decreased (0.116990 --> 0.114986).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10311 valid_loss: 0.11743 test_loss: 0.12655 \n",
      "[ 45/300] train_loss: 0.10396 valid_loss: 0.11801 test_loss: 0.12668 \n",
      "[ 46/300] train_loss: 0.10083 valid_loss: 0.11455 test_loss: 0.12523 \n",
      "Validation loss decreased (0.114986 --> 0.114550).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10343 valid_loss: 0.11426 test_loss: 0.12636 \n",
      "Validation loss decreased (0.114550 --> 0.114262).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10072 valid_loss: 0.11748 test_loss: 0.12669 \n",
      "[ 49/300] train_loss: 0.10082 valid_loss: 0.11179 test_loss: 0.12412 \n",
      "Validation loss decreased (0.114262 --> 0.111792).  Saving model ...\n",
      "[ 50/300] train_loss: 0.09713 valid_loss: 0.11616 test_loss: 0.12625 \n",
      "[ 51/300] train_loss: 0.09979 valid_loss: 0.11235 test_loss: 0.12182 \n",
      "[ 52/300] train_loss: 0.10291 valid_loss: 0.11162 test_loss: 0.12521 \n",
      "Validation loss decreased (0.111792 --> 0.111617).  Saving model ...\n",
      "[ 53/300] train_loss: 0.09812 valid_loss: 0.11537 test_loss: 0.12662 \n",
      "[ 54/300] train_loss: 0.09855 valid_loss: 0.11015 test_loss: 0.12172 \n",
      "Validation loss decreased (0.111617 --> 0.110146).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09424 valid_loss: 0.10950 test_loss: 0.12252 \n",
      "Validation loss decreased (0.110146 --> 0.109502).  Saving model ...\n",
      "[ 56/300] train_loss: 0.09745 valid_loss: 0.11212 test_loss: 0.12478 \n",
      "[ 57/300] train_loss: 0.09759 valid_loss: 0.11421 test_loss: 0.12424 \n",
      "[ 58/300] train_loss: 0.09794 valid_loss: 0.10993 test_loss: 0.12235 \n",
      "[ 59/300] train_loss: 0.09686 valid_loss: 0.10935 test_loss: 0.12016 \n",
      "Validation loss decreased (0.109502 --> 0.109352).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09542 valid_loss: 0.10888 test_loss: 0.12035 \n",
      "Validation loss decreased (0.109352 --> 0.108876).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09368 valid_loss: 0.10702 test_loss: 0.11944 \n",
      "Validation loss decreased (0.108876 --> 0.107025).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09574 valid_loss: 0.10937 test_loss: 0.11909 \n",
      "[ 63/300] train_loss: 0.09394 valid_loss: 0.10588 test_loss: 0.11837 \n",
      "Validation loss decreased (0.107025 --> 0.105885).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09482 valid_loss: 0.10844 test_loss: 0.12167 \n",
      "[ 65/300] train_loss: 0.09184 valid_loss: 0.10632 test_loss: 0.11864 \n",
      "[ 66/300] train_loss: 0.08907 valid_loss: 0.10453 test_loss: 0.11644 \n",
      "Validation loss decreased (0.105885 --> 0.104527).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09412 valid_loss: 0.10281 test_loss: 0.11592 \n",
      "Validation loss decreased (0.104527 --> 0.102812).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09293 valid_loss: 0.10319 test_loss: 0.11500 \n",
      "[ 69/300] train_loss: 0.09143 valid_loss: 0.10311 test_loss: 0.11475 \n",
      "[ 70/300] train_loss: 0.09135 valid_loss: 0.10602 test_loss: 0.11772 \n",
      "[ 71/300] train_loss: 0.08958 valid_loss: 0.10465 test_loss: 0.11736 \n",
      "[ 72/300] train_loss: 0.09203 valid_loss: 0.10324 test_loss: 0.11411 \n",
      "[ 73/300] train_loss: 0.08913 valid_loss: 0.10344 test_loss: 0.11557 \n",
      "[ 74/300] train_loss: 0.08883 valid_loss: 0.10234 test_loss: 0.11237 \n",
      "Validation loss decreased (0.102812 --> 0.102337).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09113 valid_loss: 0.10137 test_loss: 0.11215 \n",
      "Validation loss decreased (0.102337 --> 0.101367).  Saving model ...\n",
      "[ 76/300] train_loss: 0.08721 valid_loss: 0.10022 test_loss: 0.10980 \n",
      "Validation loss decreased (0.101367 --> 0.100221).  Saving model ...\n",
      "[ 77/300] train_loss: 0.08777 valid_loss: 0.09922 test_loss: 0.11004 \n",
      "Validation loss decreased (0.100221 --> 0.099215).  Saving model ...\n",
      "[ 78/300] train_loss: 0.08777 valid_loss: 0.10415 test_loss: 0.11549 \n",
      "[ 79/300] train_loss: 0.08552 valid_loss: 0.10106 test_loss: 0.11327 \n",
      "[ 80/300] train_loss: 0.08712 valid_loss: 0.09864 test_loss: 0.11076 \n",
      "Validation loss decreased (0.099215 --> 0.098643).  Saving model ...\n",
      "[ 81/300] train_loss: 0.08711 valid_loss: 0.10005 test_loss: 0.11188 \n",
      "[ 82/300] train_loss: 0.08773 valid_loss: 0.09752 test_loss: 0.10948 \n",
      "Validation loss decreased (0.098643 --> 0.097524).  Saving model ...\n",
      "[ 83/300] train_loss: 0.08731 valid_loss: 0.10010 test_loss: 0.11480 \n",
      "[ 84/300] train_loss: 0.08876 valid_loss: 0.09645 test_loss: 0.10730 \n",
      "Validation loss decreased (0.097524 --> 0.096447).  Saving model ...\n",
      "[ 85/300] train_loss: 0.08661 valid_loss: 0.09585 test_loss: 0.10687 \n",
      "Validation loss decreased (0.096447 --> 0.095850).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08711 valid_loss: 0.09924 test_loss: 0.11147 \n",
      "[ 87/300] train_loss: 0.08721 valid_loss: 0.09802 test_loss: 0.10878 \n",
      "[ 88/300] train_loss: 0.08575 valid_loss: 0.09703 test_loss: 0.10731 \n",
      "[ 89/300] train_loss: 0.08600 valid_loss: 0.09665 test_loss: 0.10774 \n",
      "[ 90/300] train_loss: 0.08537 valid_loss: 0.09614 test_loss: 0.10854 \n",
      "[ 91/300] train_loss: 0.08266 valid_loss: 0.09574 test_loss: 0.10756 \n",
      "Validation loss decreased (0.095850 --> 0.095737).  Saving model ...\n",
      "[ 92/300] train_loss: 0.08525 valid_loss: 0.09687 test_loss: 0.10781 \n",
      "[ 93/300] train_loss: 0.08190 valid_loss: 0.09632 test_loss: 0.10780 \n",
      "[ 94/300] train_loss: 0.08486 valid_loss: 0.09565 test_loss: 0.10785 \n",
      "Validation loss decreased (0.095737 --> 0.095653).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08316 valid_loss: 0.09510 test_loss: 0.10657 \n",
      "Validation loss decreased (0.095653 --> 0.095100).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08381 valid_loss: 0.09674 test_loss: 0.10899 \n",
      "[ 97/300] train_loss: 0.08213 valid_loss: 0.09288 test_loss: 0.10566 \n",
      "Validation loss decreased (0.095100 --> 0.092877).  Saving model ...\n",
      "[ 98/300] train_loss: 0.08533 valid_loss: 0.09407 test_loss: 0.10667 \n",
      "[ 99/300] train_loss: 0.08134 valid_loss: 0.09549 test_loss: 0.10779 \n",
      "[100/300] train_loss: 0.08117 valid_loss: 0.09383 test_loss: 0.10587 \n",
      "[101/300] train_loss: 0.08220 valid_loss: 0.09224 test_loss: 0.10385 \n",
      "Validation loss decreased (0.092877 --> 0.092235).  Saving model ...\n",
      "[102/300] train_loss: 0.08300 valid_loss: 0.09364 test_loss: 0.10618 \n",
      "[103/300] train_loss: 0.08232 valid_loss: 0.09131 test_loss: 0.10374 \n",
      "Validation loss decreased (0.092235 --> 0.091307).  Saving model ...\n",
      "[104/300] train_loss: 0.08344 valid_loss: 0.09217 test_loss: 0.10481 \n",
      "[105/300] train_loss: 0.08394 valid_loss: 0.09473 test_loss: 0.10588 \n",
      "[106/300] train_loss: 0.08029 valid_loss: 0.09242 test_loss: 0.10442 \n",
      "[107/300] train_loss: 0.08079 valid_loss: 0.09209 test_loss: 0.10614 \n",
      "[108/300] train_loss: 0.08036 valid_loss: 0.09337 test_loss: 0.10762 \n",
      "[109/300] train_loss: 0.08003 valid_loss: 0.09074 test_loss: 0.10625 \n",
      "Validation loss decreased (0.091307 --> 0.090740).  Saving model ...\n",
      "[110/300] train_loss: 0.08427 valid_loss: 0.09120 test_loss: 0.10543 \n",
      "[111/300] train_loss: 0.08263 valid_loss: 0.09012 test_loss: 0.10310 \n",
      "Validation loss decreased (0.090740 --> 0.090123).  Saving model ...\n",
      "[112/300] train_loss: 0.07868 valid_loss: 0.08979 test_loss: 0.10333 \n",
      "Validation loss decreased (0.090123 --> 0.089793).  Saving model ...\n",
      "[113/300] train_loss: 0.08024 valid_loss: 0.08953 test_loss: 0.10226 \n",
      "Validation loss decreased (0.089793 --> 0.089532).  Saving model ...\n",
      "[114/300] train_loss: 0.08129 valid_loss: 0.08921 test_loss: 0.10151 \n",
      "Validation loss decreased (0.089532 --> 0.089207).  Saving model ...\n",
      "[115/300] train_loss: 0.07830 valid_loss: 0.09127 test_loss: 0.10348 \n",
      "[116/300] train_loss: 0.08081 valid_loss: 0.08895 test_loss: 0.10236 \n",
      "Validation loss decreased (0.089207 --> 0.088948).  Saving model ...\n",
      "[117/300] train_loss: 0.08037 valid_loss: 0.09047 test_loss: 0.10333 \n",
      "[118/300] train_loss: 0.08050 valid_loss: 0.09051 test_loss: 0.10357 \n",
      "[119/300] train_loss: 0.07888 valid_loss: 0.08886 test_loss: 0.10084 \n",
      "Validation loss decreased (0.088948 --> 0.088857).  Saving model ...\n",
      "[120/300] train_loss: 0.07750 valid_loss: 0.08886 test_loss: 0.10252 \n",
      "Validation loss decreased (0.088857 --> 0.088855).  Saving model ...\n",
      "[121/300] train_loss: 0.07814 valid_loss: 0.09370 test_loss: 0.10570 \n",
      "[122/300] train_loss: 0.07969 valid_loss: 0.08951 test_loss: 0.10193 \n",
      "[123/300] train_loss: 0.08007 valid_loss: 0.08975 test_loss: 0.10436 \n",
      "[124/300] train_loss: 0.07853 valid_loss: 0.08877 test_loss: 0.10185 \n",
      "Validation loss decreased (0.088855 --> 0.088771).  Saving model ...\n",
      "[125/300] train_loss: 0.07724 valid_loss: 0.08876 test_loss: 0.10043 \n",
      "Validation loss decreased (0.088771 --> 0.088757).  Saving model ...\n",
      "[126/300] train_loss: 0.07922 valid_loss: 0.09048 test_loss: 0.10271 \n",
      "[127/300] train_loss: 0.08044 valid_loss: 0.08808 test_loss: 0.10058 \n",
      "Validation loss decreased (0.088757 --> 0.088080).  Saving model ...\n",
      "[128/300] train_loss: 0.07647 valid_loss: 0.08863 test_loss: 0.10069 \n",
      "[129/300] train_loss: 0.07797 valid_loss: 0.08830 test_loss: 0.10081 \n",
      "[130/300] train_loss: 0.07777 valid_loss: 0.08970 test_loss: 0.10271 \n",
      "[131/300] train_loss: 0.07795 valid_loss: 0.08925 test_loss: 0.10211 \n",
      "[132/300] train_loss: 0.07566 valid_loss: 0.09023 test_loss: 0.10142 \n",
      "[133/300] train_loss: 0.07738 valid_loss: 0.08745 test_loss: 0.09996 \n",
      "Validation loss decreased (0.088080 --> 0.087454).  Saving model ...\n",
      "[134/300] train_loss: 0.07689 valid_loss: 0.08969 test_loss: 0.10243 \n",
      "[135/300] train_loss: 0.07818 valid_loss: 0.08674 test_loss: 0.09957 \n",
      "Validation loss decreased (0.087454 --> 0.086739).  Saving model ...\n",
      "[136/300] train_loss: 0.07598 valid_loss: 0.08651 test_loss: 0.09936 \n",
      "Validation loss decreased (0.086739 --> 0.086509).  Saving model ...\n",
      "[137/300] train_loss: 0.07826 valid_loss: 0.08892 test_loss: 0.10040 \n",
      "[138/300] train_loss: 0.07726 valid_loss: 0.08750 test_loss: 0.09939 \n",
      "[139/300] train_loss: 0.07525 valid_loss: 0.08691 test_loss: 0.10003 \n",
      "[140/300] train_loss: 0.07508 valid_loss: 0.08725 test_loss: 0.09888 \n",
      "[141/300] train_loss: 0.07592 valid_loss: 0.08633 test_loss: 0.09892 \n",
      "Validation loss decreased (0.086509 --> 0.086326).  Saving model ...\n",
      "[142/300] train_loss: 0.07337 valid_loss: 0.08655 test_loss: 0.10039 \n",
      "[143/300] train_loss: 0.07598 valid_loss: 0.09011 test_loss: 0.10013 \n",
      "[144/300] train_loss: 0.07526 valid_loss: 0.08495 test_loss: 0.09909 \n",
      "Validation loss decreased (0.086326 --> 0.084948).  Saving model ...\n",
      "[145/300] train_loss: 0.07554 valid_loss: 0.08837 test_loss: 0.10098 \n",
      "[146/300] train_loss: 0.07554 valid_loss: 0.08469 test_loss: 0.09801 \n",
      "Validation loss decreased (0.084948 --> 0.084694).  Saving model ...\n",
      "[147/300] train_loss: 0.07189 valid_loss: 0.08695 test_loss: 0.09924 \n",
      "[148/300] train_loss: 0.07539 valid_loss: 0.08409 test_loss: 0.09600 \n",
      "Validation loss decreased (0.084694 --> 0.084093).  Saving model ...\n",
      "[149/300] train_loss: 0.07387 valid_loss: 0.08528 test_loss: 0.09836 \n",
      "[150/300] train_loss: 0.07534 valid_loss: 0.08738 test_loss: 0.09961 \n",
      "[151/300] train_loss: 0.07651 valid_loss: 0.08488 test_loss: 0.09762 \n",
      "[152/300] train_loss: 0.07716 valid_loss: 0.08625 test_loss: 0.09823 \n",
      "[153/300] train_loss: 0.07782 valid_loss: 0.08486 test_loss: 0.09791 \n",
      "[154/300] train_loss: 0.07209 valid_loss: 0.08398 test_loss: 0.09815 \n",
      "Validation loss decreased (0.084093 --> 0.083983).  Saving model ...\n",
      "[155/300] train_loss: 0.07554 valid_loss: 0.08484 test_loss: 0.09783 \n",
      "[156/300] train_loss: 0.07373 valid_loss: 0.08369 test_loss: 0.09723 \n",
      "Validation loss decreased (0.083983 --> 0.083686).  Saving model ...\n",
      "[157/300] train_loss: 0.07531 valid_loss: 0.08514 test_loss: 0.09719 \n",
      "[158/300] train_loss: 0.07081 valid_loss: 0.08390 test_loss: 0.09773 \n",
      "[159/300] train_loss: 0.07306 valid_loss: 0.08441 test_loss: 0.09687 \n",
      "[160/300] train_loss: 0.07271 valid_loss: 0.08485 test_loss: 0.09836 \n",
      "[161/300] train_loss: 0.07057 valid_loss: 0.08322 test_loss: 0.09555 \n",
      "Validation loss decreased (0.083686 --> 0.083217).  Saving model ...\n",
      "[162/300] train_loss: 0.07293 valid_loss: 0.08432 test_loss: 0.09702 \n",
      "[163/300] train_loss: 0.07454 valid_loss: 0.08245 test_loss: 0.09506 \n",
      "Validation loss decreased (0.083217 --> 0.082450).  Saving model ...\n",
      "[164/300] train_loss: 0.07343 valid_loss: 0.08243 test_loss: 0.09718 \n",
      "Validation loss decreased (0.082450 --> 0.082431).  Saving model ...\n",
      "[165/300] train_loss: 0.07061 valid_loss: 0.08356 test_loss: 0.09666 \n",
      "[166/300] train_loss: 0.07392 valid_loss: 0.08390 test_loss: 0.09754 \n",
      "[167/300] train_loss: 0.07292 valid_loss: 0.08287 test_loss: 0.09556 \n",
      "[168/300] train_loss: 0.06974 valid_loss: 0.08466 test_loss: 0.09752 \n",
      "[169/300] train_loss: 0.07106 valid_loss: 0.08267 test_loss: 0.09646 \n",
      "[170/300] train_loss: 0.06975 valid_loss: 0.08429 test_loss: 0.09678 \n",
      "[171/300] train_loss: 0.07216 valid_loss: 0.08220 test_loss: 0.09502 \n",
      "Validation loss decreased (0.082431 --> 0.082204).  Saving model ...\n",
      "[172/300] train_loss: 0.07252 valid_loss: 0.08258 test_loss: 0.09472 \n",
      "[173/300] train_loss: 0.07175 valid_loss: 0.08167 test_loss: 0.09509 \n",
      "Validation loss decreased (0.082204 --> 0.081675).  Saving model ...\n",
      "[174/300] train_loss: 0.07252 valid_loss: 0.08155 test_loss: 0.09586 \n",
      "Validation loss decreased (0.081675 --> 0.081552).  Saving model ...\n",
      "[175/300] train_loss: 0.07256 valid_loss: 0.08445 test_loss: 0.09890 \n",
      "[176/300] train_loss: 0.07075 valid_loss: 0.08239 test_loss: 0.09607 \n",
      "[177/300] train_loss: 0.07267 valid_loss: 0.08279 test_loss: 0.09576 \n",
      "[178/300] train_loss: 0.06872 valid_loss: 0.07995 test_loss: 0.09377 \n",
      "Validation loss decreased (0.081552 --> 0.079949).  Saving model ...\n",
      "[179/300] train_loss: 0.07079 valid_loss: 0.08263 test_loss: 0.09520 \n",
      "[180/300] train_loss: 0.07113 valid_loss: 0.08280 test_loss: 0.09608 \n",
      "[181/300] train_loss: 0.07045 valid_loss: 0.08193 test_loss: 0.09541 \n",
      "[182/300] train_loss: 0.06978 valid_loss: 0.08345 test_loss: 0.09573 \n",
      "[183/300] train_loss: 0.07050 valid_loss: 0.08131 test_loss: 0.09548 \n",
      "[184/300] train_loss: 0.07186 valid_loss: 0.08155 test_loss: 0.09449 \n",
      "[185/300] train_loss: 0.07197 valid_loss: 0.08179 test_loss: 0.09526 \n",
      "[186/300] train_loss: 0.07047 valid_loss: 0.08124 test_loss: 0.09316 \n",
      "[187/300] train_loss: 0.07034 valid_loss: 0.08142 test_loss: 0.09568 \n",
      "[188/300] train_loss: 0.07134 valid_loss: 0.08068 test_loss: 0.09412 \n",
      "[189/300] train_loss: 0.07087 valid_loss: 0.08102 test_loss: 0.09445 \n",
      "[190/300] train_loss: 0.07157 valid_loss: 0.08039 test_loss: 0.09463 \n",
      "[191/300] train_loss: 0.07013 valid_loss: 0.08059 test_loss: 0.09411 \n",
      "[192/300] train_loss: 0.07089 valid_loss: 0.08083 test_loss: 0.09404 \n",
      "[193/300] train_loss: 0.07009 valid_loss: 0.08030 test_loss: 0.09408 \n",
      "[194/300] train_loss: 0.06912 valid_loss: 0.08270 test_loss: 0.09521 \n",
      "[195/300] train_loss: 0.06698 valid_loss: 0.08202 test_loss: 0.09404 \n",
      "[196/300] train_loss: 0.06917 valid_loss: 0.08225 test_loss: 0.09534 \n",
      "[197/300] train_loss: 0.06810 valid_loss: 0.08000 test_loss: 0.09360 \n",
      "[198/300] train_loss: 0.06986 valid_loss: 0.08101 test_loss: 0.09484 \n",
      "[199/300] train_loss: 0.07081 valid_loss: 0.08096 test_loss: 0.09443 \n",
      "[200/300] train_loss: 0.06846 valid_loss: 0.07959 test_loss: 0.09417 \n",
      "Validation loss decreased (0.079949 --> 0.079585).  Saving model ...\n",
      "[201/300] train_loss: 0.07038 valid_loss: 0.07941 test_loss: 0.09358 \n",
      "Validation loss decreased (0.079585 --> 0.079413).  Saving model ...\n",
      "[202/300] train_loss: 0.06815 valid_loss: 0.08061 test_loss: 0.09437 \n",
      "[203/300] train_loss: 0.06746 valid_loss: 0.07885 test_loss: 0.09331 \n",
      "Validation loss decreased (0.079413 --> 0.078853).  Saving model ...\n",
      "[204/300] train_loss: 0.06995 valid_loss: 0.07902 test_loss: 0.09232 \n",
      "[205/300] train_loss: 0.06808 valid_loss: 0.07923 test_loss: 0.09369 \n",
      "[206/300] train_loss: 0.07008 valid_loss: 0.08000 test_loss: 0.09292 \n",
      "[207/300] train_loss: 0.06580 valid_loss: 0.08001 test_loss: 0.09519 \n",
      "[208/300] train_loss: 0.06973 valid_loss: 0.07887 test_loss: 0.09291 \n",
      "[209/300] train_loss: 0.06812 valid_loss: 0.08186 test_loss: 0.09504 \n",
      "[210/300] train_loss: 0.06765 valid_loss: 0.07946 test_loss: 0.09374 \n",
      "[211/300] train_loss: 0.06872 valid_loss: 0.07798 test_loss: 0.09191 \n",
      "Validation loss decreased (0.078853 --> 0.077985).  Saving model ...\n",
      "[212/300] train_loss: 0.07220 valid_loss: 0.07907 test_loss: 0.09249 \n",
      "[213/300] train_loss: 0.06937 valid_loss: 0.07833 test_loss: 0.09191 \n",
      "[214/300] train_loss: 0.06882 valid_loss: 0.07855 test_loss: 0.09277 \n",
      "[215/300] train_loss: 0.06682 valid_loss: 0.07891 test_loss: 0.09281 \n",
      "[216/300] train_loss: 0.06539 valid_loss: 0.07907 test_loss: 0.09345 \n",
      "[217/300] train_loss: 0.06581 valid_loss: 0.07870 test_loss: 0.09238 \n",
      "[218/300] train_loss: 0.06667 valid_loss: 0.07792 test_loss: 0.09256 \n",
      "Validation loss decreased (0.077985 --> 0.077916).  Saving model ...\n",
      "[219/300] train_loss: 0.06879 valid_loss: 0.07906 test_loss: 0.09385 \n",
      "[220/300] train_loss: 0.06703 valid_loss: 0.07894 test_loss: 0.09293 \n",
      "[221/300] train_loss: 0.06866 valid_loss: 0.07810 test_loss: 0.09305 \n",
      "[222/300] train_loss: 0.06594 valid_loss: 0.07732 test_loss: 0.09144 \n",
      "Validation loss decreased (0.077916 --> 0.077317).  Saving model ...\n",
      "[223/300] train_loss: 0.06603 valid_loss: 0.07806 test_loss: 0.09228 \n",
      "[224/300] train_loss: 0.06606 valid_loss: 0.07795 test_loss: 0.09114 \n",
      "[225/300] train_loss: 0.06679 valid_loss: 0.07921 test_loss: 0.09319 \n",
      "[226/300] train_loss: 0.06624 valid_loss: 0.08090 test_loss: 0.09416 \n",
      "[227/300] train_loss: 0.06587 valid_loss: 0.07869 test_loss: 0.09313 \n",
      "[228/300] train_loss: 0.06677 valid_loss: 0.07666 test_loss: 0.09054 \n",
      "Validation loss decreased (0.077317 --> 0.076665).  Saving model ...\n",
      "[229/300] train_loss: 0.06756 valid_loss: 0.07658 test_loss: 0.09036 \n",
      "Validation loss decreased (0.076665 --> 0.076584).  Saving model ...\n",
      "[230/300] train_loss: 0.06451 valid_loss: 0.07760 test_loss: 0.09240 \n",
      "[231/300] train_loss: 0.06559 valid_loss: 0.07802 test_loss: 0.09262 \n",
      "[232/300] train_loss: 0.06646 valid_loss: 0.07772 test_loss: 0.09385 \n",
      "[233/300] train_loss: 0.06592 valid_loss: 0.07653 test_loss: 0.09237 \n",
      "Validation loss decreased (0.076584 --> 0.076532).  Saving model ...\n",
      "[234/300] train_loss: 0.06559 valid_loss: 0.07736 test_loss: 0.09241 \n",
      "[235/300] train_loss: 0.06457 valid_loss: 0.07739 test_loss: 0.09174 \n",
      "[236/300] train_loss: 0.06692 valid_loss: 0.07709 test_loss: 0.09254 \n",
      "[237/300] train_loss: 0.06552 valid_loss: 0.07623 test_loss: 0.09190 \n",
      "Validation loss decreased (0.076532 --> 0.076230).  Saving model ...\n",
      "[238/300] train_loss: 0.06324 valid_loss: 0.07574 test_loss: 0.09146 \n",
      "Validation loss decreased (0.076230 --> 0.075741).  Saving model ...\n",
      "[239/300] train_loss: 0.06550 valid_loss: 0.07672 test_loss: 0.09109 \n",
      "[240/300] train_loss: 0.06647 valid_loss: 0.07671 test_loss: 0.09172 \n",
      "[241/300] train_loss: 0.06567 valid_loss: 0.07587 test_loss: 0.09044 \n",
      "[242/300] train_loss: 0.06669 valid_loss: 0.07915 test_loss: 0.09278 \n",
      "[243/300] train_loss: 0.06482 valid_loss: 0.07741 test_loss: 0.09105 \n",
      "[244/300] train_loss: 0.06525 valid_loss: 0.07638 test_loss: 0.09049 \n",
      "[245/300] train_loss: 0.06497 valid_loss: 0.07877 test_loss: 0.09293 \n",
      "[246/300] train_loss: 0.06512 valid_loss: 0.07715 test_loss: 0.09120 \n",
      "[247/300] train_loss: 0.06491 valid_loss: 0.07565 test_loss: 0.09114 \n",
      "Validation loss decreased (0.075741 --> 0.075651).  Saving model ...\n",
      "[248/300] train_loss: 0.06256 valid_loss: 0.07610 test_loss: 0.09082 \n",
      "[249/300] train_loss: 0.06592 valid_loss: 0.07692 test_loss: 0.09129 \n",
      "[250/300] train_loss: 0.06403 valid_loss: 0.07646 test_loss: 0.09142 \n",
      "[251/300] train_loss: 0.06330 valid_loss: 0.07569 test_loss: 0.09050 \n",
      "[252/300] train_loss: 0.06556 valid_loss: 0.07667 test_loss: 0.09098 \n",
      "[253/300] train_loss: 0.06459 valid_loss: 0.07625 test_loss: 0.09147 \n",
      "[254/300] train_loss: 0.06165 valid_loss: 0.07677 test_loss: 0.09111 \n",
      "[255/300] train_loss: 0.06600 valid_loss: 0.07693 test_loss: 0.09097 \n",
      "[256/300] train_loss: 0.06393 valid_loss: 0.07657 test_loss: 0.09087 \n",
      "[257/300] train_loss: 0.06500 valid_loss: 0.07550 test_loss: 0.09123 \n",
      "Validation loss decreased (0.075651 --> 0.075497).  Saving model ...\n",
      "[258/300] train_loss: 0.06335 valid_loss: 0.07641 test_loss: 0.09118 \n",
      "[259/300] train_loss: 0.06416 valid_loss: 0.07552 test_loss: 0.09047 \n",
      "[260/300] train_loss: 0.06356 valid_loss: 0.07589 test_loss: 0.09047 \n",
      "[261/300] train_loss: 0.06540 valid_loss: 0.07649 test_loss: 0.09117 \n",
      "[262/300] train_loss: 0.06311 valid_loss: 0.07736 test_loss: 0.09189 \n",
      "[263/300] train_loss: 0.06312 valid_loss: 0.07611 test_loss: 0.09219 \n",
      "[264/300] train_loss: 0.06297 valid_loss: 0.07452 test_loss: 0.09006 \n",
      "Validation loss decreased (0.075497 --> 0.074521).  Saving model ...\n",
      "[265/300] train_loss: 0.06308 valid_loss: 0.07748 test_loss: 0.09164 \n",
      "[266/300] train_loss: 0.06255 valid_loss: 0.07628 test_loss: 0.09204 \n",
      "[267/300] train_loss: 0.06426 valid_loss: 0.07483 test_loss: 0.08953 \n",
      "[268/300] train_loss: 0.06411 valid_loss: 0.07518 test_loss: 0.08960 \n",
      "[269/300] train_loss: 0.06260 valid_loss: 0.07453 test_loss: 0.09048 \n",
      "[270/300] train_loss: 0.06385 valid_loss: 0.07442 test_loss: 0.08969 \n",
      "Validation loss decreased (0.074521 --> 0.074422).  Saving model ...\n",
      "[271/300] train_loss: 0.06279 valid_loss: 0.07745 test_loss: 0.09193 \n",
      "[272/300] train_loss: 0.06396 valid_loss: 0.07469 test_loss: 0.08972 \n",
      "[273/300] train_loss: 0.06415 valid_loss: 0.07519 test_loss: 0.09147 \n",
      "[274/300] train_loss: 0.06291 valid_loss: 0.07455 test_loss: 0.08950 \n",
      "[275/300] train_loss: 0.06255 valid_loss: 0.07528 test_loss: 0.08981 \n",
      "[276/300] train_loss: 0.06234 valid_loss: 0.07504 test_loss: 0.09122 \n",
      "[277/300] train_loss: 0.06337 valid_loss: 0.07440 test_loss: 0.08970 \n",
      "Validation loss decreased (0.074422 --> 0.074402).  Saving model ...\n",
      "[278/300] train_loss: 0.06159 valid_loss: 0.07502 test_loss: 0.09023 \n",
      "[279/300] train_loss: 0.06043 valid_loss: 0.07421 test_loss: 0.08999 \n",
      "Validation loss decreased (0.074402 --> 0.074205).  Saving model ...\n",
      "[280/300] train_loss: 0.06181 valid_loss: 0.07511 test_loss: 0.09016 \n",
      "[281/300] train_loss: 0.06268 valid_loss: 0.07539 test_loss: 0.09059 \n",
      "[282/300] train_loss: 0.06086 valid_loss: 0.07406 test_loss: 0.09002 \n",
      "Validation loss decreased (0.074205 --> 0.074060).  Saving model ...\n",
      "[283/300] train_loss: 0.06400 valid_loss: 0.07439 test_loss: 0.08978 \n",
      "[284/300] train_loss: 0.06199 valid_loss: 0.07372 test_loss: 0.08938 \n",
      "Validation loss decreased (0.074060 --> 0.073716).  Saving model ...\n",
      "[285/300] train_loss: 0.06124 valid_loss: 0.07335 test_loss: 0.08902 \n",
      "Validation loss decreased (0.073716 --> 0.073349).  Saving model ...\n",
      "[286/300] train_loss: 0.06138 valid_loss: 0.07430 test_loss: 0.09048 \n",
      "[287/300] train_loss: 0.06379 valid_loss: 0.07441 test_loss: 0.08988 \n",
      "[288/300] train_loss: 0.06359 valid_loss: 0.07365 test_loss: 0.08909 \n",
      "[289/300] train_loss: 0.06333 valid_loss: 0.07511 test_loss: 0.09138 \n",
      "[290/300] train_loss: 0.06188 valid_loss: 0.07567 test_loss: 0.09026 \n",
      "[291/300] train_loss: 0.06102 valid_loss: 0.07435 test_loss: 0.09057 \n",
      "[292/300] train_loss: 0.06247 valid_loss: 0.07322 test_loss: 0.08910 \n",
      "Validation loss decreased (0.073349 --> 0.073220).  Saving model ...\n",
      "[293/300] train_loss: 0.06275 valid_loss: 0.07413 test_loss: 0.09098 \n",
      "[294/300] train_loss: 0.06196 valid_loss: 0.07428 test_loss: 0.09101 \n",
      "[295/300] train_loss: 0.06347 valid_loss: 0.07353 test_loss: 0.09025 \n",
      "[296/300] train_loss: 0.06188 valid_loss: 0.07252 test_loss: 0.08841 \n",
      "Validation loss decreased (0.073220 --> 0.072519).  Saving model ...\n",
      "[297/300] train_loss: 0.06174 valid_loss: 0.07324 test_loss: 0.09111 \n",
      "[298/300] train_loss: 0.06149 valid_loss: 0.07364 test_loss: 0.08985 \n",
      "[299/300] train_loss: 0.05968 valid_loss: 0.07329 test_loss: 0.08963 \n",
      "[300/300] train_loss: 0.06001 valid_loss: 0.07268 test_loss: 0.08907 \n",
      "TRAINING MODEL 19\n",
      "[  1/300] train_loss: 0.62121 valid_loss: 0.50998 test_loss: 0.51661 \n",
      "Validation loss decreased (inf --> 0.509984).  Saving model ...\n",
      "[  2/300] train_loss: 0.41220 valid_loss: 0.36707 test_loss: 0.37391 \n",
      "Validation loss decreased (0.509984 --> 0.367072).  Saving model ...\n",
      "[  3/300] train_loss: 0.30280 valid_loss: 0.29240 test_loss: 0.30472 \n",
      "Validation loss decreased (0.367072 --> 0.292399).  Saving model ...\n",
      "[  4/300] train_loss: 0.25100 valid_loss: 0.24457 test_loss: 0.25869 \n",
      "Validation loss decreased (0.292399 --> 0.244567).  Saving model ...\n",
      "[  5/300] train_loss: 0.21952 valid_loss: 0.21923 test_loss: 0.23453 \n",
      "Validation loss decreased (0.244567 --> 0.219232).  Saving model ...\n",
      "[  6/300] train_loss: 0.20331 valid_loss: 0.20309 test_loss: 0.21813 \n",
      "Validation loss decreased (0.219232 --> 0.203087).  Saving model ...\n",
      "[  7/300] train_loss: 0.19144 valid_loss: 0.19292 test_loss: 0.20992 \n",
      "Validation loss decreased (0.203087 --> 0.192919).  Saving model ...\n",
      "[  8/300] train_loss: 0.18101 valid_loss: 0.18113 test_loss: 0.19422 \n",
      "Validation loss decreased (0.192919 --> 0.181130).  Saving model ...\n",
      "[  9/300] train_loss: 0.17341 valid_loss: 0.17500 test_loss: 0.18653 \n",
      "Validation loss decreased (0.181130 --> 0.174998).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16528 valid_loss: 0.16702 test_loss: 0.17877 \n",
      "Validation loss decreased (0.174998 --> 0.167024).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16145 valid_loss: 0.16183 test_loss: 0.17459 \n",
      "Validation loss decreased (0.167024 --> 0.161830).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15362 valid_loss: 0.15842 test_loss: 0.17020 \n",
      "Validation loss decreased (0.161830 --> 0.158424).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15109 valid_loss: 0.15295 test_loss: 0.16442 \n",
      "Validation loss decreased (0.158424 --> 0.152952).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14274 valid_loss: 0.15029 test_loss: 0.16321 \n",
      "Validation loss decreased (0.152952 --> 0.150290).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14198 valid_loss: 0.14929 test_loss: 0.16017 \n",
      "Validation loss decreased (0.150290 --> 0.149293).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13983 valid_loss: 0.14396 test_loss: 0.15506 \n",
      "Validation loss decreased (0.149293 --> 0.143958).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13506 valid_loss: 0.14264 test_loss: 0.15378 \n",
      "Validation loss decreased (0.143958 --> 0.142643).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13465 valid_loss: 0.14240 test_loss: 0.15359 \n",
      "Validation loss decreased (0.142643 --> 0.142395).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13332 valid_loss: 0.14086 test_loss: 0.15189 \n",
      "Validation loss decreased (0.142395 --> 0.140863).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13048 valid_loss: 0.13902 test_loss: 0.15045 \n",
      "Validation loss decreased (0.140863 --> 0.139022).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12859 valid_loss: 0.13626 test_loss: 0.14821 \n",
      "Validation loss decreased (0.139022 --> 0.136264).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12629 valid_loss: 0.13399 test_loss: 0.14720 \n",
      "Validation loss decreased (0.136264 --> 0.133994).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12573 valid_loss: 0.13153 test_loss: 0.14382 \n",
      "Validation loss decreased (0.133994 --> 0.131532).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12408 valid_loss: 0.13167 test_loss: 0.14614 \n",
      "[ 25/300] train_loss: 0.12192 valid_loss: 0.13102 test_loss: 0.14313 \n",
      "Validation loss decreased (0.131532 --> 0.131019).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12296 valid_loss: 0.12848 test_loss: 0.14133 \n",
      "Validation loss decreased (0.131019 --> 0.128484).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11965 valid_loss: 0.12691 test_loss: 0.14043 \n",
      "Validation loss decreased (0.128484 --> 0.126909).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11895 valid_loss: 0.12431 test_loss: 0.13760 \n",
      "Validation loss decreased (0.126909 --> 0.124306).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11766 valid_loss: 0.12356 test_loss: 0.13778 \n",
      "Validation loss decreased (0.124306 --> 0.123564).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11562 valid_loss: 0.12387 test_loss: 0.13839 \n",
      "[ 31/300] train_loss: 0.11728 valid_loss: 0.12083 test_loss: 0.13391 \n",
      "Validation loss decreased (0.123564 --> 0.120828).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11517 valid_loss: 0.11935 test_loss: 0.13330 \n",
      "Validation loss decreased (0.120828 --> 0.119353).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11568 valid_loss: 0.12052 test_loss: 0.13456 \n",
      "[ 34/300] train_loss: 0.11131 valid_loss: 0.11939 test_loss: 0.13295 \n",
      "[ 35/300] train_loss: 0.11393 valid_loss: 0.11736 test_loss: 0.13083 \n",
      "Validation loss decreased (0.119353 --> 0.117359).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11463 valid_loss: 0.11733 test_loss: 0.13153 \n",
      "Validation loss decreased (0.117359 --> 0.117330).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11092 valid_loss: 0.11572 test_loss: 0.13036 \n",
      "Validation loss decreased (0.117330 --> 0.115716).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11235 valid_loss: 0.11460 test_loss: 0.12767 \n",
      "Validation loss decreased (0.115716 --> 0.114603).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11011 valid_loss: 0.11429 test_loss: 0.12836 \n",
      "Validation loss decreased (0.114603 --> 0.114289).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10842 valid_loss: 0.11362 test_loss: 0.12696 \n",
      "Validation loss decreased (0.114289 --> 0.113615).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11093 valid_loss: 0.11225 test_loss: 0.12682 \n",
      "Validation loss decreased (0.113615 --> 0.112245).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10771 valid_loss: 0.11360 test_loss: 0.12699 \n",
      "[ 43/300] train_loss: 0.10549 valid_loss: 0.11375 test_loss: 0.12717 \n",
      "[ 44/300] train_loss: 0.10588 valid_loss: 0.10975 test_loss: 0.12363 \n",
      "Validation loss decreased (0.112245 --> 0.109748).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10770 valid_loss: 0.11018 test_loss: 0.12369 \n",
      "[ 46/300] train_loss: 0.10377 valid_loss: 0.10977 test_loss: 0.12308 \n",
      "[ 47/300] train_loss: 0.10527 valid_loss: 0.11150 test_loss: 0.12548 \n",
      "[ 48/300] train_loss: 0.10349 valid_loss: 0.10996 test_loss: 0.12408 \n",
      "[ 49/300] train_loss: 0.10228 valid_loss: 0.10846 test_loss: 0.12174 \n",
      "Validation loss decreased (0.109748 --> 0.108464).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10352 valid_loss: 0.10894 test_loss: 0.12203 \n",
      "[ 51/300] train_loss: 0.10374 valid_loss: 0.10628 test_loss: 0.12096 \n",
      "Validation loss decreased (0.108464 --> 0.106278).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10223 valid_loss: 0.10891 test_loss: 0.12060 \n",
      "[ 53/300] train_loss: 0.09948 valid_loss: 0.10612 test_loss: 0.12010 \n",
      "Validation loss decreased (0.106278 --> 0.106122).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10096 valid_loss: 0.10622 test_loss: 0.12170 \n",
      "[ 55/300] train_loss: 0.10144 valid_loss: 0.10558 test_loss: 0.11940 \n",
      "Validation loss decreased (0.106122 --> 0.105585).  Saving model ...\n",
      "[ 56/300] train_loss: 0.09861 valid_loss: 0.10683 test_loss: 0.11951 \n",
      "[ 57/300] train_loss: 0.10064 valid_loss: 0.10673 test_loss: 0.11945 \n",
      "[ 58/300] train_loss: 0.09746 valid_loss: 0.10274 test_loss: 0.11692 \n",
      "Validation loss decreased (0.105585 --> 0.102741).  Saving model ...\n",
      "[ 59/300] train_loss: 0.10110 valid_loss: 0.10592 test_loss: 0.11837 \n",
      "[ 60/300] train_loss: 0.09822 valid_loss: 0.10307 test_loss: 0.11684 \n",
      "[ 61/300] train_loss: 0.09639 valid_loss: 0.10458 test_loss: 0.11833 \n",
      "[ 62/300] train_loss: 0.09546 valid_loss: 0.10514 test_loss: 0.11684 \n",
      "[ 63/300] train_loss: 0.09773 valid_loss: 0.10312 test_loss: 0.11603 \n",
      "[ 64/300] train_loss: 0.09628 valid_loss: 0.10486 test_loss: 0.11794 \n",
      "[ 65/300] train_loss: 0.09707 valid_loss: 0.10333 test_loss: 0.11584 \n",
      "[ 66/300] train_loss: 0.09443 valid_loss: 0.10189 test_loss: 0.11619 \n",
      "Validation loss decreased (0.102741 --> 0.101892).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09620 valid_loss: 0.10120 test_loss: 0.11363 \n",
      "Validation loss decreased (0.101892 --> 0.101201).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09529 valid_loss: 0.10224 test_loss: 0.11492 \n",
      "[ 69/300] train_loss: 0.09601 valid_loss: 0.10215 test_loss: 0.11427 \n",
      "[ 70/300] train_loss: 0.09429 valid_loss: 0.10211 test_loss: 0.11470 \n",
      "[ 71/300] train_loss: 0.09225 valid_loss: 0.10100 test_loss: 0.11263 \n",
      "Validation loss decreased (0.101201 --> 0.101001).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09204 valid_loss: 0.10086 test_loss: 0.11304 \n",
      "Validation loss decreased (0.101001 --> 0.100856).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09412 valid_loss: 0.09859 test_loss: 0.11250 \n",
      "Validation loss decreased (0.100856 --> 0.098594).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09264 valid_loss: 0.09965 test_loss: 0.11262 \n",
      "[ 75/300] train_loss: 0.09423 valid_loss: 0.10008 test_loss: 0.11244 \n",
      "[ 76/300] train_loss: 0.09385 valid_loss: 0.09788 test_loss: 0.11018 \n",
      "Validation loss decreased (0.098594 --> 0.097884).  Saving model ...\n",
      "[ 77/300] train_loss: 0.08888 valid_loss: 0.10028 test_loss: 0.11177 \n",
      "[ 78/300] train_loss: 0.09113 valid_loss: 0.09867 test_loss: 0.11052 \n",
      "[ 79/300] train_loss: 0.09172 valid_loss: 0.10165 test_loss: 0.11298 \n",
      "[ 80/300] train_loss: 0.09033 valid_loss: 0.10030 test_loss: 0.11058 \n",
      "[ 81/300] train_loss: 0.09091 valid_loss: 0.09862 test_loss: 0.10962 \n",
      "[ 82/300] train_loss: 0.09202 valid_loss: 0.09858 test_loss: 0.10944 \n",
      "[ 83/300] train_loss: 0.09079 valid_loss: 0.10107 test_loss: 0.11074 \n",
      "[ 84/300] train_loss: 0.09026 valid_loss: 0.09665 test_loss: 0.10792 \n",
      "Validation loss decreased (0.097884 --> 0.096651).  Saving model ...\n",
      "[ 85/300] train_loss: 0.09109 valid_loss: 0.09655 test_loss: 0.10767 \n",
      "Validation loss decreased (0.096651 --> 0.096550).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08848 valid_loss: 0.09671 test_loss: 0.10745 \n",
      "[ 87/300] train_loss: 0.08911 valid_loss: 0.09645 test_loss: 0.10658 \n",
      "Validation loss decreased (0.096550 --> 0.096446).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08853 valid_loss: 0.09768 test_loss: 0.10981 \n",
      "[ 89/300] train_loss: 0.08766 valid_loss: 0.09709 test_loss: 0.10725 \n",
      "[ 90/300] train_loss: 0.08939 valid_loss: 0.09486 test_loss: 0.10733 \n",
      "Validation loss decreased (0.096446 --> 0.094859).  Saving model ...\n",
      "[ 91/300] train_loss: 0.09305 valid_loss: 0.09580 test_loss: 0.10731 \n",
      "[ 92/300] train_loss: 0.08807 valid_loss: 0.09485 test_loss: 0.10583 \n",
      "Validation loss decreased (0.094859 --> 0.094846).  Saving model ...\n",
      "[ 93/300] train_loss: 0.08618 valid_loss: 0.09532 test_loss: 0.10565 \n",
      "[ 94/300] train_loss: 0.08743 valid_loss: 0.09525 test_loss: 0.10581 \n",
      "[ 95/300] train_loss: 0.08523 valid_loss: 0.09695 test_loss: 0.10610 \n",
      "[ 96/300] train_loss: 0.08944 valid_loss: 0.09552 test_loss: 0.10663 \n",
      "[ 97/300] train_loss: 0.08465 valid_loss: 0.09481 test_loss: 0.10563 \n",
      "Validation loss decreased (0.094846 --> 0.094812).  Saving model ...\n",
      "[ 98/300] train_loss: 0.09023 valid_loss: 0.09465 test_loss: 0.10590 \n",
      "Validation loss decreased (0.094812 --> 0.094648).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08404 valid_loss: 0.09305 test_loss: 0.10526 \n",
      "Validation loss decreased (0.094648 --> 0.093050).  Saving model ...\n",
      "[100/300] train_loss: 0.08661 valid_loss: 0.09411 test_loss: 0.10435 \n",
      "[101/300] train_loss: 0.08573 valid_loss: 0.09346 test_loss: 0.10610 \n",
      "[102/300] train_loss: 0.08523 valid_loss: 0.09431 test_loss: 0.10446 \n",
      "[103/300] train_loss: 0.08643 valid_loss: 0.09336 test_loss: 0.10524 \n",
      "[104/300] train_loss: 0.08483 valid_loss: 0.09407 test_loss: 0.10478 \n",
      "[105/300] train_loss: 0.08402 valid_loss: 0.09357 test_loss: 0.10271 \n",
      "[106/300] train_loss: 0.08304 valid_loss: 0.09261 test_loss: 0.10376 \n",
      "Validation loss decreased (0.093050 --> 0.092606).  Saving model ...\n",
      "[107/300] train_loss: 0.08352 valid_loss: 0.09239 test_loss: 0.10326 \n",
      "Validation loss decreased (0.092606 --> 0.092389).  Saving model ...\n",
      "[108/300] train_loss: 0.08361 valid_loss: 0.09100 test_loss: 0.10252 \n",
      "Validation loss decreased (0.092389 --> 0.091000).  Saving model ...\n",
      "[109/300] train_loss: 0.08454 valid_loss: 0.09111 test_loss: 0.10240 \n",
      "[110/300] train_loss: 0.08550 valid_loss: 0.09273 test_loss: 0.10420 \n",
      "[111/300] train_loss: 0.08627 valid_loss: 0.09027 test_loss: 0.10298 \n",
      "Validation loss decreased (0.091000 --> 0.090273).  Saving model ...\n",
      "[112/300] train_loss: 0.08398 valid_loss: 0.09003 test_loss: 0.10237 \n",
      "Validation loss decreased (0.090273 --> 0.090027).  Saving model ...\n",
      "[113/300] train_loss: 0.08172 valid_loss: 0.09290 test_loss: 0.10337 \n",
      "[114/300] train_loss: 0.08230 valid_loss: 0.09151 test_loss: 0.10261 \n",
      "[115/300] train_loss: 0.08278 valid_loss: 0.09205 test_loss: 0.10197 \n",
      "[116/300] train_loss: 0.08168 valid_loss: 0.09190 test_loss: 0.10275 \n",
      "[117/300] train_loss: 0.08294 valid_loss: 0.09036 test_loss: 0.10129 \n",
      "[118/300] train_loss: 0.08288 valid_loss: 0.09058 test_loss: 0.10177 \n",
      "[119/300] train_loss: 0.08015 valid_loss: 0.09070 test_loss: 0.10163 \n",
      "[120/300] train_loss: 0.07975 valid_loss: 0.08882 test_loss: 0.10121 \n",
      "Validation loss decreased (0.090027 --> 0.088818).  Saving model ...\n",
      "[121/300] train_loss: 0.08219 valid_loss: 0.08955 test_loss: 0.10152 \n",
      "[122/300] train_loss: 0.08216 valid_loss: 0.08866 test_loss: 0.09999 \n",
      "Validation loss decreased (0.088818 --> 0.088665).  Saving model ...\n",
      "[123/300] train_loss: 0.07875 valid_loss: 0.09059 test_loss: 0.10024 \n",
      "[124/300] train_loss: 0.08058 valid_loss: 0.09003 test_loss: 0.10055 \n",
      "[125/300] train_loss: 0.07901 valid_loss: 0.08924 test_loss: 0.09997 \n",
      "[126/300] train_loss: 0.08012 valid_loss: 0.08987 test_loss: 0.10042 \n",
      "[127/300] train_loss: 0.08150 valid_loss: 0.08901 test_loss: 0.09964 \n",
      "[128/300] train_loss: 0.08205 valid_loss: 0.08900 test_loss: 0.10026 \n",
      "[129/300] train_loss: 0.07883 valid_loss: 0.08813 test_loss: 0.09997 \n",
      "Validation loss decreased (0.088665 --> 0.088131).  Saving model ...\n",
      "[130/300] train_loss: 0.08114 valid_loss: 0.08983 test_loss: 0.10020 \n",
      "[131/300] train_loss: 0.07959 valid_loss: 0.08816 test_loss: 0.10007 \n",
      "[132/300] train_loss: 0.07872 valid_loss: 0.09125 test_loss: 0.10195 \n",
      "[133/300] train_loss: 0.07912 valid_loss: 0.08922 test_loss: 0.09918 \n",
      "[134/300] train_loss: 0.08042 valid_loss: 0.08921 test_loss: 0.09899 \n",
      "[135/300] train_loss: 0.08204 valid_loss: 0.08769 test_loss: 0.09835 \n",
      "Validation loss decreased (0.088131 --> 0.087691).  Saving model ...\n",
      "[136/300] train_loss: 0.07882 valid_loss: 0.08846 test_loss: 0.09994 \n",
      "[137/300] train_loss: 0.07856 valid_loss: 0.08623 test_loss: 0.09756 \n",
      "Validation loss decreased (0.087691 --> 0.086234).  Saving model ...\n",
      "[138/300] train_loss: 0.07877 valid_loss: 0.08825 test_loss: 0.09740 \n",
      "[139/300] train_loss: 0.08233 valid_loss: 0.08774 test_loss: 0.09852 \n",
      "[140/300] train_loss: 0.07955 valid_loss: 0.08804 test_loss: 0.09889 \n",
      "[141/300] train_loss: 0.07839 valid_loss: 0.08677 test_loss: 0.09776 \n",
      "[142/300] train_loss: 0.08077 valid_loss: 0.08638 test_loss: 0.09843 \n",
      "[143/300] train_loss: 0.08073 valid_loss: 0.08695 test_loss: 0.09782 \n",
      "[144/300] train_loss: 0.07895 valid_loss: 0.08674 test_loss: 0.09775 \n",
      "[145/300] train_loss: 0.08127 valid_loss: 0.08792 test_loss: 0.09787 \n",
      "[146/300] train_loss: 0.07577 valid_loss: 0.08661 test_loss: 0.09774 \n",
      "[147/300] train_loss: 0.07775 valid_loss: 0.08609 test_loss: 0.09698 \n",
      "Validation loss decreased (0.086234 --> 0.086091).  Saving model ...\n",
      "[148/300] train_loss: 0.07699 valid_loss: 0.08799 test_loss: 0.09869 \n",
      "[149/300] train_loss: 0.07719 valid_loss: 0.08510 test_loss: 0.09625 \n",
      "Validation loss decreased (0.086091 --> 0.085098).  Saving model ...\n",
      "[150/300] train_loss: 0.07652 valid_loss: 0.08490 test_loss: 0.09732 \n",
      "Validation loss decreased (0.085098 --> 0.084899).  Saving model ...\n",
      "[151/300] train_loss: 0.07771 valid_loss: 0.08763 test_loss: 0.09639 \n",
      "[152/300] train_loss: 0.07648 valid_loss: 0.08772 test_loss: 0.09684 \n",
      "[153/300] train_loss: 0.07592 valid_loss: 0.08573 test_loss: 0.09810 \n",
      "[154/300] train_loss: 0.07560 valid_loss: 0.08555 test_loss: 0.09701 \n",
      "[155/300] train_loss: 0.07703 valid_loss: 0.08552 test_loss: 0.09535 \n",
      "[156/300] train_loss: 0.07702 valid_loss: 0.08606 test_loss: 0.09560 \n",
      "[157/300] train_loss: 0.07757 valid_loss: 0.08560 test_loss: 0.09766 \n",
      "[158/300] train_loss: 0.07375 valid_loss: 0.08422 test_loss: 0.09491 \n",
      "Validation loss decreased (0.084899 --> 0.084225).  Saving model ...\n",
      "[159/300] train_loss: 0.07666 valid_loss: 0.08623 test_loss: 0.09620 \n",
      "[160/300] train_loss: 0.07426 valid_loss: 0.08488 test_loss: 0.09604 \n",
      "[161/300] train_loss: 0.07476 valid_loss: 0.08464 test_loss: 0.09624 \n",
      "[162/300] train_loss: 0.07417 valid_loss: 0.08537 test_loss: 0.09647 \n",
      "[163/300] train_loss: 0.07483 valid_loss: 0.08415 test_loss: 0.09503 \n",
      "Validation loss decreased (0.084225 --> 0.084151).  Saving model ...\n",
      "[164/300] train_loss: 0.07608 valid_loss: 0.08440 test_loss: 0.09441 \n",
      "[165/300] train_loss: 0.07495 valid_loss: 0.08509 test_loss: 0.09589 \n",
      "[166/300] train_loss: 0.07571 valid_loss: 0.08519 test_loss: 0.09615 \n",
      "[167/300] train_loss: 0.07354 valid_loss: 0.08363 test_loss: 0.09452 \n",
      "Validation loss decreased (0.084151 --> 0.083627).  Saving model ...\n",
      "[168/300] train_loss: 0.07316 valid_loss: 0.08482 test_loss: 0.09479 \n",
      "[169/300] train_loss: 0.07349 valid_loss: 0.08434 test_loss: 0.09491 \n",
      "[170/300] train_loss: 0.07422 valid_loss: 0.08333 test_loss: 0.09511 \n",
      "Validation loss decreased (0.083627 --> 0.083326).  Saving model ...\n",
      "[171/300] train_loss: 0.07458 valid_loss: 0.08468 test_loss: 0.09454 \n",
      "[172/300] train_loss: 0.07326 valid_loss: 0.08344 test_loss: 0.09548 \n",
      "[173/300] train_loss: 0.07366 valid_loss: 0.08355 test_loss: 0.09373 \n",
      "[174/300] train_loss: 0.07622 valid_loss: 0.08447 test_loss: 0.09401 \n",
      "[175/300] train_loss: 0.07269 valid_loss: 0.08494 test_loss: 0.09483 \n",
      "[176/300] train_loss: 0.07390 valid_loss: 0.08420 test_loss: 0.09480 \n",
      "[177/300] train_loss: 0.07296 valid_loss: 0.08278 test_loss: 0.09622 \n",
      "Validation loss decreased (0.083326 --> 0.082781).  Saving model ...\n",
      "[178/300] train_loss: 0.07446 valid_loss: 0.08453 test_loss: 0.09421 \n",
      "[179/300] train_loss: 0.07386 valid_loss: 0.08373 test_loss: 0.09405 \n",
      "[180/300] train_loss: 0.07233 valid_loss: 0.08328 test_loss: 0.09473 \n",
      "[181/300] train_loss: 0.07003 valid_loss: 0.08445 test_loss: 0.09468 \n",
      "[182/300] train_loss: 0.07304 valid_loss: 0.08368 test_loss: 0.09439 \n",
      "[183/300] train_loss: 0.07292 valid_loss: 0.08359 test_loss: 0.09457 \n",
      "[184/300] train_loss: 0.07237 valid_loss: 0.08366 test_loss: 0.09629 \n",
      "[185/300] train_loss: 0.07220 valid_loss: 0.08173 test_loss: 0.09261 \n",
      "Validation loss decreased (0.082781 --> 0.081729).  Saving model ...\n",
      "[186/300] train_loss: 0.07096 valid_loss: 0.08468 test_loss: 0.09434 \n",
      "[187/300] train_loss: 0.06888 valid_loss: 0.08241 test_loss: 0.09418 \n",
      "[188/300] train_loss: 0.07368 valid_loss: 0.08221 test_loss: 0.09160 \n",
      "[189/300] train_loss: 0.07130 valid_loss: 0.08406 test_loss: 0.09378 \n",
      "[190/300] train_loss: 0.07131 valid_loss: 0.08146 test_loss: 0.09222 \n",
      "Validation loss decreased (0.081729 --> 0.081459).  Saving model ...\n",
      "[191/300] train_loss: 0.06994 valid_loss: 0.08378 test_loss: 0.09389 \n",
      "[192/300] train_loss: 0.07006 valid_loss: 0.08271 test_loss: 0.09360 \n",
      "[193/300] train_loss: 0.07214 valid_loss: 0.08408 test_loss: 0.09420 \n",
      "[194/300] train_loss: 0.07309 valid_loss: 0.08148 test_loss: 0.09131 \n",
      "[195/300] train_loss: 0.07126 valid_loss: 0.08319 test_loss: 0.09303 \n",
      "[196/300] train_loss: 0.07029 valid_loss: 0.08436 test_loss: 0.09375 \n",
      "[197/300] train_loss: 0.07214 valid_loss: 0.08209 test_loss: 0.09240 \n",
      "[198/300] train_loss: 0.07055 valid_loss: 0.08369 test_loss: 0.09409 \n",
      "[199/300] train_loss: 0.07114 valid_loss: 0.08176 test_loss: 0.09200 \n",
      "[200/300] train_loss: 0.07122 valid_loss: 0.08181 test_loss: 0.09205 \n",
      "[201/300] train_loss: 0.06961 valid_loss: 0.08105 test_loss: 0.09306 \n",
      "Validation loss decreased (0.081459 --> 0.081054).  Saving model ...\n",
      "[202/300] train_loss: 0.07242 valid_loss: 0.08302 test_loss: 0.09306 \n",
      "[203/300] train_loss: 0.06950 valid_loss: 0.08077 test_loss: 0.09169 \n",
      "Validation loss decreased (0.081054 --> 0.080774).  Saving model ...\n",
      "[204/300] train_loss: 0.06835 valid_loss: 0.08206 test_loss: 0.09340 \n",
      "[205/300] train_loss: 0.06967 valid_loss: 0.08136 test_loss: 0.09343 \n",
      "[206/300] train_loss: 0.07173 valid_loss: 0.08217 test_loss: 0.09165 \n",
      "[207/300] train_loss: 0.07120 valid_loss: 0.08218 test_loss: 0.09141 \n",
      "[208/300] train_loss: 0.06906 valid_loss: 0.08217 test_loss: 0.09287 \n",
      "[209/300] train_loss: 0.07015 valid_loss: 0.08118 test_loss: 0.09195 \n",
      "[210/300] train_loss: 0.07014 valid_loss: 0.08176 test_loss: 0.09170 \n",
      "[211/300] train_loss: 0.06942 valid_loss: 0.08098 test_loss: 0.09143 \n",
      "[212/300] train_loss: 0.06762 valid_loss: 0.08236 test_loss: 0.09187 \n",
      "[213/300] train_loss: 0.06869 valid_loss: 0.07947 test_loss: 0.09197 \n",
      "Validation loss decreased (0.080774 --> 0.079471).  Saving model ...\n",
      "[214/300] train_loss: 0.06779 valid_loss: 0.08103 test_loss: 0.09096 \n",
      "[215/300] train_loss: 0.06964 valid_loss: 0.08094 test_loss: 0.09127 \n",
      "[216/300] train_loss: 0.07022 valid_loss: 0.08108 test_loss: 0.09050 \n",
      "[217/300] train_loss: 0.06985 valid_loss: 0.08195 test_loss: 0.09203 \n",
      "[218/300] train_loss: 0.07104 valid_loss: 0.08125 test_loss: 0.09236 \n",
      "[219/300] train_loss: 0.07001 valid_loss: 0.08158 test_loss: 0.09265 \n",
      "[220/300] train_loss: 0.06949 valid_loss: 0.07977 test_loss: 0.09078 \n",
      "[221/300] train_loss: 0.06753 valid_loss: 0.08164 test_loss: 0.09306 \n",
      "[222/300] train_loss: 0.07064 valid_loss: 0.08009 test_loss: 0.09024 \n",
      "[223/300] train_loss: 0.07071 valid_loss: 0.08026 test_loss: 0.09033 \n",
      "[224/300] train_loss: 0.06898 valid_loss: 0.08161 test_loss: 0.09063 \n",
      "[225/300] train_loss: 0.06995 valid_loss: 0.08069 test_loss: 0.09089 \n",
      "[226/300] train_loss: 0.06969 valid_loss: 0.08035 test_loss: 0.09053 \n",
      "[227/300] train_loss: 0.07006 valid_loss: 0.08142 test_loss: 0.09111 \n",
      "[228/300] train_loss: 0.06823 valid_loss: 0.07951 test_loss: 0.09094 \n",
      "[229/300] train_loss: 0.06995 valid_loss: 0.08134 test_loss: 0.09169 \n",
      "[230/300] train_loss: 0.06777 valid_loss: 0.08130 test_loss: 0.09108 \n",
      "[231/300] train_loss: 0.06849 valid_loss: 0.07936 test_loss: 0.09090 \n",
      "Validation loss decreased (0.079471 --> 0.079364).  Saving model ...\n",
      "[232/300] train_loss: 0.06895 valid_loss: 0.08039 test_loss: 0.09076 \n",
      "[233/300] train_loss: 0.06709 valid_loss: 0.07987 test_loss: 0.09042 \n",
      "[234/300] train_loss: 0.06966 valid_loss: 0.08301 test_loss: 0.09040 \n",
      "[235/300] train_loss: 0.06730 valid_loss: 0.08320 test_loss: 0.09245 \n",
      "[236/300] train_loss: 0.06755 valid_loss: 0.08022 test_loss: 0.09101 \n",
      "[237/300] train_loss: 0.06710 valid_loss: 0.07830 test_loss: 0.09017 \n",
      "Validation loss decreased (0.079364 --> 0.078297).  Saving model ...\n",
      "[238/300] train_loss: 0.06777 valid_loss: 0.07916 test_loss: 0.08932 \n",
      "[239/300] train_loss: 0.06812 valid_loss: 0.07906 test_loss: 0.08948 \n",
      "[240/300] train_loss: 0.06581 valid_loss: 0.07982 test_loss: 0.09063 \n",
      "[241/300] train_loss: 0.06400 valid_loss: 0.07813 test_loss: 0.08985 \n",
      "Validation loss decreased (0.078297 --> 0.078125).  Saving model ...\n",
      "[242/300] train_loss: 0.06695 valid_loss: 0.08208 test_loss: 0.09261 \n",
      "[243/300] train_loss: 0.06677 valid_loss: 0.07972 test_loss: 0.09068 \n",
      "[244/300] train_loss: 0.06780 valid_loss: 0.07962 test_loss: 0.09033 \n",
      "[245/300] train_loss: 0.06598 valid_loss: 0.07931 test_loss: 0.09020 \n",
      "[246/300] train_loss: 0.06637 valid_loss: 0.07892 test_loss: 0.09067 \n",
      "[247/300] train_loss: 0.06514 valid_loss: 0.07785 test_loss: 0.08980 \n",
      "Validation loss decreased (0.078125 --> 0.077850).  Saving model ...\n",
      "[248/300] train_loss: 0.06429 valid_loss: 0.07775 test_loss: 0.08946 \n",
      "Validation loss decreased (0.077850 --> 0.077750).  Saving model ...\n",
      "[249/300] train_loss: 0.06622 valid_loss: 0.07910 test_loss: 0.09034 \n",
      "[250/300] train_loss: 0.06760 valid_loss: 0.07872 test_loss: 0.08985 \n",
      "[251/300] train_loss: 0.06714 valid_loss: 0.07972 test_loss: 0.09023 \n",
      "[252/300] train_loss: 0.06487 valid_loss: 0.07768 test_loss: 0.08938 \n",
      "Validation loss decreased (0.077750 --> 0.077676).  Saving model ...\n",
      "[253/300] train_loss: 0.06729 valid_loss: 0.07751 test_loss: 0.08943 \n",
      "Validation loss decreased (0.077676 --> 0.077514).  Saving model ...\n",
      "[254/300] train_loss: 0.06633 valid_loss: 0.07828 test_loss: 0.08956 \n",
      "[255/300] train_loss: 0.06656 valid_loss: 0.07797 test_loss: 0.08867 \n",
      "[256/300] train_loss: 0.06683 valid_loss: 0.07708 test_loss: 0.08915 \n",
      "Validation loss decreased (0.077514 --> 0.077076).  Saving model ...\n",
      "[257/300] train_loss: 0.06544 valid_loss: 0.07867 test_loss: 0.08935 \n",
      "[258/300] train_loss: 0.06646 valid_loss: 0.07893 test_loss: 0.08920 \n",
      "[259/300] train_loss: 0.06552 valid_loss: 0.07975 test_loss: 0.09011 \n",
      "[260/300] train_loss: 0.06541 valid_loss: 0.08017 test_loss: 0.09036 \n",
      "[261/300] train_loss: 0.06509 valid_loss: 0.07930 test_loss: 0.09006 \n",
      "[262/300] train_loss: 0.06560 valid_loss: 0.07909 test_loss: 0.08922 \n",
      "[263/300] train_loss: 0.06560 valid_loss: 0.07824 test_loss: 0.08866 \n",
      "[264/300] train_loss: 0.06606 valid_loss: 0.07631 test_loss: 0.08913 \n",
      "Validation loss decreased (0.077076 --> 0.076309).  Saving model ...\n",
      "[265/300] train_loss: 0.06518 valid_loss: 0.07640 test_loss: 0.08902 \n",
      "[266/300] train_loss: 0.06699 valid_loss: 0.07776 test_loss: 0.08968 \n",
      "[267/300] train_loss: 0.06596 valid_loss: 0.07790 test_loss: 0.08919 \n",
      "[268/300] train_loss: 0.06389 valid_loss: 0.07804 test_loss: 0.08961 \n",
      "[269/300] train_loss: 0.06600 valid_loss: 0.07702 test_loss: 0.09036 \n",
      "[270/300] train_loss: 0.06562 valid_loss: 0.07795 test_loss: 0.08923 \n",
      "[271/300] train_loss: 0.06416 valid_loss: 0.07751 test_loss: 0.08902 \n",
      "[272/300] train_loss: 0.06466 valid_loss: 0.07837 test_loss: 0.08973 \n",
      "[273/300] train_loss: 0.06702 valid_loss: 0.07908 test_loss: 0.08928 \n",
      "[274/300] train_loss: 0.06561 valid_loss: 0.07634 test_loss: 0.08905 \n",
      "[275/300] train_loss: 0.06281 valid_loss: 0.07946 test_loss: 0.08995 \n",
      "[276/300] train_loss: 0.06282 valid_loss: 0.07720 test_loss: 0.08763 \n",
      "[277/300] train_loss: 0.06502 valid_loss: 0.07792 test_loss: 0.09077 \n",
      "[278/300] train_loss: 0.06452 valid_loss: 0.07758 test_loss: 0.08907 \n",
      "[279/300] train_loss: 0.06406 valid_loss: 0.07774 test_loss: 0.08920 \n",
      "[280/300] train_loss: 0.06458 valid_loss: 0.07821 test_loss: 0.08825 \n",
      "[281/300] train_loss: 0.06319 valid_loss: 0.07667 test_loss: 0.08889 \n",
      "[282/300] train_loss: 0.06244 valid_loss: 0.07611 test_loss: 0.08782 \n",
      "Validation loss decreased (0.076309 --> 0.076113).  Saving model ...\n",
      "[283/300] train_loss: 0.06505 valid_loss: 0.07723 test_loss: 0.08949 \n",
      "[284/300] train_loss: 0.06328 valid_loss: 0.07685 test_loss: 0.08887 \n",
      "[285/300] train_loss: 0.06368 valid_loss: 0.07622 test_loss: 0.08916 \n",
      "[286/300] train_loss: 0.06245 valid_loss: 0.07803 test_loss: 0.09074 \n",
      "[287/300] train_loss: 0.06327 valid_loss: 0.07740 test_loss: 0.08927 \n",
      "[288/300] train_loss: 0.06465 valid_loss: 0.07656 test_loss: 0.08781 \n",
      "[289/300] train_loss: 0.06087 valid_loss: 0.07744 test_loss: 0.08986 \n",
      "[290/300] train_loss: 0.06408 valid_loss: 0.07618 test_loss: 0.08903 \n",
      "[291/300] train_loss: 0.06325 valid_loss: 0.07883 test_loss: 0.08994 \n",
      "[292/300] train_loss: 0.06403 valid_loss: 0.07718 test_loss: 0.08872 \n",
      "[293/300] train_loss: 0.06393 valid_loss: 0.07756 test_loss: 0.08981 \n",
      "[294/300] train_loss: 0.06401 valid_loss: 0.07714 test_loss: 0.08888 \n",
      "[295/300] train_loss: 0.06536 valid_loss: 0.07618 test_loss: 0.08746 \n",
      "[296/300] train_loss: 0.06304 valid_loss: 0.07681 test_loss: 0.08793 \n",
      "[297/300] train_loss: 0.06283 valid_loss: 0.07683 test_loss: 0.08838 \n",
      "[298/300] train_loss: 0.06327 valid_loss: 0.07631 test_loss: 0.08771 \n",
      "[299/300] train_loss: 0.06400 valid_loss: 0.07612 test_loss: 0.08760 \n",
      "[300/300] train_loss: 0.06267 valid_loss: 0.07684 test_loss: 0.08839 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 300\n",
    "\n",
    "train_loader = dl_train_seen\n",
    "valid_loader = dl_valid_seen\n",
    "test_loader = dl_test_seen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model = PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_seen_status_%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.plot(test_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/100] train_loss: 0.63111 valid_loss: 0.53670 test_loss: 0.51249 \n",
      "Validation loss decreased (inf --> 0.536702).  Saving model ...\n",
      "[  2/100] train_loss: 0.45673 valid_loss: 0.40395 test_loss: 0.35558 \n",
      "Validation loss decreased (0.536702 --> 0.403945).  Saving model ...\n",
      "[  3/100] train_loss: 0.35668 valid_loss: 0.33894 test_loss: 0.28313 \n",
      "Validation loss decreased (0.403945 --> 0.338943).  Saving model ...\n",
      "[  4/100] train_loss: 0.29483 valid_loss: 0.29183 test_loss: 0.23517 \n",
      "Validation loss decreased (0.338943 --> 0.291834).  Saving model ...\n",
      "[  5/100] train_loss: 0.25620 valid_loss: 0.26312 test_loss: 0.19780 \n",
      "Validation loss decreased (0.291834 --> 0.263116).  Saving model ...\n",
      "[  6/100] train_loss: 0.22819 valid_loss: 0.24475 test_loss: 0.17537 \n",
      "Validation loss decreased (0.263116 --> 0.244754).  Saving model ...\n",
      "[  7/100] train_loss: 0.21174 valid_loss: 0.21968 test_loss: 0.15815 \n",
      "Validation loss decreased (0.244754 --> 0.219684).  Saving model ...\n",
      "[  8/100] train_loss: 0.20010 valid_loss: 0.21072 test_loss: 0.15043 \n",
      "Validation loss decreased (0.219684 --> 0.210715).  Saving model ...\n",
      "[  9/100] train_loss: 0.18920 valid_loss: 0.20499 test_loss: 0.14185 \n",
      "Validation loss decreased (0.210715 --> 0.204995).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17951 valid_loss: 0.19754 test_loss: 0.13721 \n",
      "Validation loss decreased (0.204995 --> 0.197539).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16820 valid_loss: 0.18928 test_loss: 0.13683 \n",
      "Validation loss decreased (0.197539 --> 0.189278).  Saving model ...\n",
      "[ 12/100] train_loss: 0.17101 valid_loss: 0.18388 test_loss: 0.12668 \n",
      "Validation loss decreased (0.189278 --> 0.183882).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15961 valid_loss: 0.17171 test_loss: 0.12190 \n",
      "Validation loss decreased (0.183882 --> 0.171714).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15698 valid_loss: 0.16615 test_loss: 0.11927 \n",
      "Validation loss decreased (0.171714 --> 0.166154).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14939 valid_loss: 0.16587 test_loss: 0.11710 \n",
      "Validation loss decreased (0.166154 --> 0.165875).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14610 valid_loss: 0.16423 test_loss: 0.11474 \n",
      "Validation loss decreased (0.165875 --> 0.164234).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14620 valid_loss: 0.16186 test_loss: 0.11316 \n",
      "Validation loss decreased (0.164234 --> 0.161864).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14312 valid_loss: 0.15857 test_loss: 0.11722 \n",
      "Validation loss decreased (0.161864 --> 0.158572).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14188 valid_loss: 0.15601 test_loss: 0.11267 \n",
      "Validation loss decreased (0.158572 --> 0.156011).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14000 valid_loss: 0.15945 test_loss: 0.11686 \n",
      "[ 21/100] train_loss: 0.13855 valid_loss: 0.15524 test_loss: 0.11300 \n",
      "Validation loss decreased (0.156011 --> 0.155244).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13679 valid_loss: 0.15843 test_loss: 0.11831 \n",
      "[ 23/100] train_loss: 0.13667 valid_loss: 0.15897 test_loss: 0.12030 \n",
      "[ 24/100] train_loss: 0.13326 valid_loss: 0.14812 test_loss: 0.11088 \n",
      "Validation loss decreased (0.155244 --> 0.148122).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12907 valid_loss: 0.14806 test_loss: 0.11377 \n",
      "Validation loss decreased (0.148122 --> 0.148059).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12855 valid_loss: 0.15072 test_loss: 0.11312 \n",
      "[ 27/100] train_loss: 0.12900 valid_loss: 0.14777 test_loss: 0.11424 \n",
      "Validation loss decreased (0.148059 --> 0.147771).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12644 valid_loss: 0.14527 test_loss: 0.10372 \n",
      "Validation loss decreased (0.147771 --> 0.145269).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12651 valid_loss: 0.14516 test_loss: 0.10818 \n",
      "Validation loss decreased (0.145269 --> 0.145164).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12428 valid_loss: 0.14019 test_loss: 0.10422 \n",
      "Validation loss decreased (0.145164 --> 0.140189).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12398 valid_loss: 0.14238 test_loss: 0.10857 \n",
      "[ 32/100] train_loss: 0.12272 valid_loss: 0.13960 test_loss: 0.09833 \n",
      "Validation loss decreased (0.140189 --> 0.139602).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11992 valid_loss: 0.13971 test_loss: 0.10535 \n",
      "[ 34/100] train_loss: 0.11986 valid_loss: 0.13767 test_loss: 0.10235 \n",
      "Validation loss decreased (0.139602 --> 0.137673).  Saving model ...\n",
      "[ 35/100] train_loss: 0.12072 valid_loss: 0.13707 test_loss: 0.10140 \n",
      "Validation loss decreased (0.137673 --> 0.137068).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11862 valid_loss: 0.13498 test_loss: 0.09660 \n",
      "Validation loss decreased (0.137068 --> 0.134981).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11929 valid_loss: 0.13493 test_loss: 0.10139 \n",
      "Validation loss decreased (0.134981 --> 0.134932).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11765 valid_loss: 0.13297 test_loss: 0.10265 \n",
      "Validation loss decreased (0.134932 --> 0.132972).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11324 valid_loss: 0.13578 test_loss: 0.09853 \n",
      "[ 40/100] train_loss: 0.11551 valid_loss: 0.13494 test_loss: 0.09640 \n",
      "[ 41/100] train_loss: 0.11278 valid_loss: 0.13115 test_loss: 0.10139 \n",
      "Validation loss decreased (0.132972 --> 0.131147).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11463 valid_loss: 0.13064 test_loss: 0.09904 \n",
      "Validation loss decreased (0.131147 --> 0.130640).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11301 valid_loss: 0.13069 test_loss: 0.10165 \n",
      "[ 44/100] train_loss: 0.11064 valid_loss: 0.13188 test_loss: 0.10107 \n",
      "[ 45/100] train_loss: 0.11331 valid_loss: 0.13659 test_loss: 0.10555 \n",
      "[ 46/100] train_loss: 0.10976 valid_loss: 0.13043 test_loss: 0.09769 \n",
      "Validation loss decreased (0.130640 --> 0.130429).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11189 valid_loss: 0.12912 test_loss: 0.09726 \n",
      "Validation loss decreased (0.130429 --> 0.129124).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11036 valid_loss: 0.12666 test_loss: 0.09782 \n",
      "Validation loss decreased (0.129124 --> 0.126659).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10893 valid_loss: 0.12640 test_loss: 0.09666 \n",
      "Validation loss decreased (0.126659 --> 0.126396).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10983 valid_loss: 0.12737 test_loss: 0.09960 \n",
      "[ 51/100] train_loss: 0.10909 valid_loss: 0.13024 test_loss: 0.09976 \n",
      "[ 52/100] train_loss: 0.10688 valid_loss: 0.12543 test_loss: 0.09479 \n",
      "Validation loss decreased (0.126396 --> 0.125434).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10734 valid_loss: 0.12466 test_loss: 0.09605 \n",
      "Validation loss decreased (0.125434 --> 0.124662).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10295 valid_loss: 0.12148 test_loss: 0.09049 \n",
      "Validation loss decreased (0.124662 --> 0.121478).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10521 valid_loss: 0.12289 test_loss: 0.09147 \n",
      "[ 56/100] train_loss: 0.10800 valid_loss: 0.12052 test_loss: 0.09050 \n",
      "Validation loss decreased (0.121478 --> 0.120518).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10525 valid_loss: 0.11874 test_loss: 0.09073 \n",
      "Validation loss decreased (0.120518 --> 0.118743).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10611 valid_loss: 0.12113 test_loss: 0.09266 \n",
      "[ 59/100] train_loss: 0.10545 valid_loss: 0.12085 test_loss: 0.09017 \n",
      "[ 60/100] train_loss: 0.10187 valid_loss: 0.11725 test_loss: 0.08691 \n",
      "Validation loss decreased (0.118743 --> 0.117245).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10196 valid_loss: 0.11776 test_loss: 0.08925 \n",
      "[ 62/100] train_loss: 0.10099 valid_loss: 0.11634 test_loss: 0.08313 \n",
      "Validation loss decreased (0.117245 --> 0.116338).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10082 valid_loss: 0.11515 test_loss: 0.08438 \n",
      "Validation loss decreased (0.116338 --> 0.115153).  Saving model ...\n",
      "[ 64/100] train_loss: 0.09789 valid_loss: 0.11768 test_loss: 0.08703 \n",
      "[ 65/100] train_loss: 0.09930 valid_loss: 0.11563 test_loss: 0.08360 \n",
      "[ 66/100] train_loss: 0.10079 valid_loss: 0.11380 test_loss: 0.08683 \n",
      "Validation loss decreased (0.115153 --> 0.113797).  Saving model ...\n",
      "[ 67/100] train_loss: 0.09868 valid_loss: 0.11621 test_loss: 0.08412 \n",
      "[ 68/100] train_loss: 0.09748 valid_loss: 0.11671 test_loss: 0.09256 \n",
      "[ 69/100] train_loss: 0.09920 valid_loss: 0.11539 test_loss: 0.08776 \n",
      "[ 70/100] train_loss: 0.09882 valid_loss: 0.11177 test_loss: 0.08296 \n",
      "Validation loss decreased (0.113797 --> 0.111768).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09833 valid_loss: 0.11291 test_loss: 0.08581 \n",
      "[ 72/100] train_loss: 0.09611 valid_loss: 0.11620 test_loss: 0.08437 \n",
      "[ 73/100] train_loss: 0.09836 valid_loss: 0.11090 test_loss: 0.08304 \n",
      "Validation loss decreased (0.111768 --> 0.110903).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09679 valid_loss: 0.11397 test_loss: 0.08391 \n",
      "[ 75/100] train_loss: 0.09764 valid_loss: 0.11254 test_loss: 0.08340 \n",
      "[ 76/100] train_loss: 0.09568 valid_loss: 0.11067 test_loss: 0.08173 \n",
      "Validation loss decreased (0.110903 --> 0.110668).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09495 valid_loss: 0.10867 test_loss: 0.08057 \n",
      "Validation loss decreased (0.110668 --> 0.108668).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09542 valid_loss: 0.11221 test_loss: 0.08176 \n",
      "[ 79/100] train_loss: 0.09742 valid_loss: 0.10912 test_loss: 0.08222 \n",
      "[ 80/100] train_loss: 0.09438 valid_loss: 0.11086 test_loss: 0.08205 \n",
      "[ 81/100] train_loss: 0.09404 valid_loss: 0.11082 test_loss: 0.08106 \n",
      "[ 82/100] train_loss: 0.09287 valid_loss: 0.10877 test_loss: 0.07870 \n",
      "[ 83/100] train_loss: 0.09319 valid_loss: 0.10708 test_loss: 0.07739 \n",
      "Validation loss decreased (0.108668 --> 0.107079).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09646 valid_loss: 0.10723 test_loss: 0.07930 \n",
      "[ 85/100] train_loss: 0.09312 valid_loss: 0.10817 test_loss: 0.08057 \n",
      "[ 86/100] train_loss: 0.09251 valid_loss: 0.10744 test_loss: 0.07983 \n",
      "[ 87/100] train_loss: 0.09245 valid_loss: 0.10735 test_loss: 0.07835 \n",
      "[ 88/100] train_loss: 0.09320 valid_loss: 0.10485 test_loss: 0.07897 \n",
      "Validation loss decreased (0.107079 --> 0.104853).  Saving model ...\n",
      "[ 89/100] train_loss: 0.08822 valid_loss: 0.10456 test_loss: 0.08019 \n",
      "Validation loss decreased (0.104853 --> 0.104555).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09129 valid_loss: 0.10605 test_loss: 0.08011 \n",
      "[ 91/100] train_loss: 0.08836 valid_loss: 0.10619 test_loss: 0.07894 \n",
      "[ 92/100] train_loss: 0.09288 valid_loss: 0.10705 test_loss: 0.07961 \n",
      "[ 93/100] train_loss: 0.09153 valid_loss: 0.10461 test_loss: 0.07878 \n",
      "[ 94/100] train_loss: 0.09139 valid_loss: 0.10371 test_loss: 0.07918 \n",
      "Validation loss decreased (0.104555 --> 0.103713).  Saving model ...\n",
      "[ 95/100] train_loss: 0.08991 valid_loss: 0.10677 test_loss: 0.07877 \n",
      "[ 96/100] train_loss: 0.08972 valid_loss: 0.10560 test_loss: 0.07798 \n",
      "[ 97/100] train_loss: 0.09027 valid_loss: 0.10377 test_loss: 0.07835 \n",
      "[ 98/100] train_loss: 0.08851 valid_loss: 0.10201 test_loss: 0.07808 \n",
      "Validation loss decreased (0.103713 --> 0.102006).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09080 valid_loss: 0.10446 test_loss: 0.07872 \n",
      "[100/100] train_loss: 0.08913 valid_loss: 0.10251 test_loss: 0.07780 \n",
      "TRAINING MODEL 1\n",
      "[  1/100] train_loss: 0.55029 valid_loss: 0.45674 test_loss: 0.41544 \n",
      "Validation loss decreased (inf --> 0.456735).  Saving model ...\n",
      "[  2/100] train_loss: 0.37629 valid_loss: 0.34554 test_loss: 0.28015 \n",
      "Validation loss decreased (0.456735 --> 0.345544).  Saving model ...\n",
      "[  3/100] train_loss: 0.29361 valid_loss: 0.29437 test_loss: 0.22555 \n",
      "Validation loss decreased (0.345544 --> 0.294372).  Saving model ...\n",
      "[  4/100] train_loss: 0.24681 valid_loss: 0.25584 test_loss: 0.18723 \n",
      "Validation loss decreased (0.294372 --> 0.255840).  Saving model ...\n",
      "[  5/100] train_loss: 0.21993 valid_loss: 0.24145 test_loss: 0.17525 \n",
      "Validation loss decreased (0.255840 --> 0.241448).  Saving model ...\n",
      "[  6/100] train_loss: 0.21166 valid_loss: 0.22442 test_loss: 0.15805 \n",
      "Validation loss decreased (0.241448 --> 0.224416).  Saving model ...\n",
      "[  7/100] train_loss: 0.19536 valid_loss: 0.20798 test_loss: 0.14755 \n",
      "Validation loss decreased (0.224416 --> 0.207982).  Saving model ...\n",
      "[  8/100] train_loss: 0.18569 valid_loss: 0.19873 test_loss: 0.13765 \n",
      "Validation loss decreased (0.207982 --> 0.198732).  Saving model ...\n",
      "[  9/100] train_loss: 0.17950 valid_loss: 0.20803 test_loss: 0.14142 \n",
      "[ 10/100] train_loss: 0.17071 valid_loss: 0.18979 test_loss: 0.13951 \n",
      "Validation loss decreased (0.198732 --> 0.189789).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16431 valid_loss: 0.19602 test_loss: 0.13093 \n",
      "[ 12/100] train_loss: 0.16054 valid_loss: 0.17816 test_loss: 0.12459 \n",
      "Validation loss decreased (0.189789 --> 0.178158).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15385 valid_loss: 0.17212 test_loss: 0.12237 \n",
      "Validation loss decreased (0.178158 --> 0.172121).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15366 valid_loss: 0.17495 test_loss: 0.12378 \n",
      "[ 15/100] train_loss: 0.14959 valid_loss: 0.16524 test_loss: 0.12076 \n",
      "Validation loss decreased (0.172121 --> 0.165239).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14396 valid_loss: 0.16110 test_loss: 0.11591 \n",
      "Validation loss decreased (0.165239 --> 0.161096).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14569 valid_loss: 0.15899 test_loss: 0.11487 \n",
      "Validation loss decreased (0.161096 --> 0.158988).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14147 valid_loss: 0.15930 test_loss: 0.11625 \n",
      "[ 19/100] train_loss: 0.13599 valid_loss: 0.15406 test_loss: 0.11332 \n",
      "Validation loss decreased (0.158988 --> 0.154064).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13892 valid_loss: 0.15329 test_loss: 0.10908 \n",
      "Validation loss decreased (0.154064 --> 0.153289).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13602 valid_loss: 0.14912 test_loss: 0.10908 \n",
      "Validation loss decreased (0.153289 --> 0.149124).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13417 valid_loss: 0.15369 test_loss: 0.11296 \n",
      "[ 23/100] train_loss: 0.13127 valid_loss: 0.14644 test_loss: 0.11077 \n",
      "Validation loss decreased (0.149124 --> 0.146440).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12693 valid_loss: 0.14800 test_loss: 0.11090 \n",
      "[ 25/100] train_loss: 0.12634 valid_loss: 0.14433 test_loss: 0.10329 \n",
      "Validation loss decreased (0.146440 --> 0.144333).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12799 valid_loss: 0.14403 test_loss: 0.10477 \n",
      "Validation loss decreased (0.144333 --> 0.144027).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12782 valid_loss: 0.14103 test_loss: 0.10744 \n",
      "Validation loss decreased (0.144027 --> 0.141032).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12543 valid_loss: 0.14358 test_loss: 0.11257 \n",
      "[ 29/100] train_loss: 0.12308 valid_loss: 0.14079 test_loss: 0.11255 \n",
      "Validation loss decreased (0.141032 --> 0.140792).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12649 valid_loss: 0.13782 test_loss: 0.10535 \n",
      "Validation loss decreased (0.140792 --> 0.137823).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12646 valid_loss: 0.14036 test_loss: 0.11178 \n",
      "[ 32/100] train_loss: 0.12261 valid_loss: 0.13826 test_loss: 0.10587 \n",
      "[ 33/100] train_loss: 0.12250 valid_loss: 0.13358 test_loss: 0.09730 \n",
      "Validation loss decreased (0.137823 --> 0.133585).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11630 valid_loss: 0.13150 test_loss: 0.10071 \n",
      "Validation loss decreased (0.133585 --> 0.131505).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11957 valid_loss: 0.13233 test_loss: 0.10029 \n",
      "[ 36/100] train_loss: 0.11565 valid_loss: 0.13538 test_loss: 0.10138 \n",
      "[ 37/100] train_loss: 0.12098 valid_loss: 0.13498 test_loss: 0.10373 \n",
      "[ 38/100] train_loss: 0.11769 valid_loss: 0.13090 test_loss: 0.10398 \n",
      "Validation loss decreased (0.131505 --> 0.130902).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11731 valid_loss: 0.13147 test_loss: 0.09803 \n",
      "[ 40/100] train_loss: 0.11505 valid_loss: 0.13123 test_loss: 0.09999 \n",
      "[ 41/100] train_loss: 0.11510 valid_loss: 0.13125 test_loss: 0.09841 \n",
      "[ 42/100] train_loss: 0.11088 valid_loss: 0.12809 test_loss: 0.09791 \n",
      "Validation loss decreased (0.130902 --> 0.128090).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11312 valid_loss: 0.12964 test_loss: 0.09534 \n",
      "[ 44/100] train_loss: 0.11352 valid_loss: 0.12505 test_loss: 0.09814 \n",
      "Validation loss decreased (0.128090 --> 0.125048).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11324 valid_loss: 0.12739 test_loss: 0.09785 \n",
      "[ 46/100] train_loss: 0.11276 valid_loss: 0.12735 test_loss: 0.09611 \n",
      "[ 47/100] train_loss: 0.10795 valid_loss: 0.12467 test_loss: 0.09695 \n",
      "Validation loss decreased (0.125048 --> 0.124666).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10913 valid_loss: 0.12236 test_loss: 0.09317 \n",
      "Validation loss decreased (0.124666 --> 0.122360).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10907 valid_loss: 0.12339 test_loss: 0.09445 \n",
      "[ 50/100] train_loss: 0.10877 valid_loss: 0.12206 test_loss: 0.09294 \n",
      "Validation loss decreased (0.122360 --> 0.122065).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10928 valid_loss: 0.12171 test_loss: 0.09524 \n",
      "Validation loss decreased (0.122065 --> 0.121713).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10759 valid_loss: 0.12142 test_loss: 0.09631 \n",
      "Validation loss decreased (0.121713 --> 0.121421).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10711 valid_loss: 0.12112 test_loss: 0.08854 \n",
      "Validation loss decreased (0.121421 --> 0.121118).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10669 valid_loss: 0.12172 test_loss: 0.09361 \n",
      "[ 55/100] train_loss: 0.10720 valid_loss: 0.12033 test_loss: 0.09764 \n",
      "Validation loss decreased (0.121118 --> 0.120328).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10658 valid_loss: 0.12135 test_loss: 0.09414 \n",
      "[ 57/100] train_loss: 0.10739 valid_loss: 0.12006 test_loss: 0.09330 \n",
      "Validation loss decreased (0.120328 --> 0.120056).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10607 valid_loss: 0.12204 test_loss: 0.09787 \n",
      "[ 59/100] train_loss: 0.10262 valid_loss: 0.11843 test_loss: 0.09037 \n",
      "Validation loss decreased (0.120056 --> 0.118433).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10384 valid_loss: 0.11440 test_loss: 0.08744 \n",
      "Validation loss decreased (0.118433 --> 0.114401).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10203 valid_loss: 0.12029 test_loss: 0.09060 \n",
      "[ 62/100] train_loss: 0.10275 valid_loss: 0.11453 test_loss: 0.08836 \n",
      "[ 63/100] train_loss: 0.10003 valid_loss: 0.11713 test_loss: 0.09210 \n",
      "[ 64/100] train_loss: 0.10197 valid_loss: 0.11403 test_loss: 0.08855 \n",
      "Validation loss decreased (0.114401 --> 0.114032).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10003 valid_loss: 0.11412 test_loss: 0.09174 \n",
      "[ 66/100] train_loss: 0.10138 valid_loss: 0.11449 test_loss: 0.09149 \n",
      "[ 67/100] train_loss: 0.10029 valid_loss: 0.11286 test_loss: 0.08919 \n",
      "Validation loss decreased (0.114032 --> 0.112856).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10003 valid_loss: 0.11785 test_loss: 0.09223 \n",
      "[ 69/100] train_loss: 0.09786 valid_loss: 0.11305 test_loss: 0.08639 \n",
      "[ 70/100] train_loss: 0.10014 valid_loss: 0.11256 test_loss: 0.08744 \n",
      "Validation loss decreased (0.112856 --> 0.112559).  Saving model ...\n",
      "[ 71/100] train_loss: 0.10009 valid_loss: 0.11178 test_loss: 0.08818 \n",
      "Validation loss decreased (0.112559 --> 0.111776).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09813 valid_loss: 0.11019 test_loss: 0.08533 \n",
      "Validation loss decreased (0.111776 --> 0.110186).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09637 valid_loss: 0.10973 test_loss: 0.08479 \n",
      "Validation loss decreased (0.110186 --> 0.109728).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09773 valid_loss: 0.11075 test_loss: 0.08610 \n",
      "[ 75/100] train_loss: 0.09360 valid_loss: 0.11071 test_loss: 0.08481 \n",
      "[ 76/100] train_loss: 0.09543 valid_loss: 0.11219 test_loss: 0.08810 \n",
      "[ 77/100] train_loss: 0.09728 valid_loss: 0.10952 test_loss: 0.08749 \n",
      "Validation loss decreased (0.109728 --> 0.109523).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09638 valid_loss: 0.11118 test_loss: 0.08574 \n",
      "[ 79/100] train_loss: 0.09541 valid_loss: 0.10833 test_loss: 0.08669 \n",
      "Validation loss decreased (0.109523 --> 0.108328).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09389 valid_loss: 0.10901 test_loss: 0.08671 \n",
      "[ 81/100] train_loss: 0.09581 valid_loss: 0.10918 test_loss: 0.08472 \n",
      "[ 82/100] train_loss: 0.09676 valid_loss: 0.10731 test_loss: 0.08497 \n",
      "Validation loss decreased (0.108328 --> 0.107307).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09340 valid_loss: 0.11170 test_loss: 0.08643 \n",
      "[ 84/100] train_loss: 0.09389 valid_loss: 0.10693 test_loss: 0.08716 \n",
      "Validation loss decreased (0.107307 --> 0.106934).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09153 valid_loss: 0.10683 test_loss: 0.08737 \n",
      "Validation loss decreased (0.106934 --> 0.106833).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09008 valid_loss: 0.11003 test_loss: 0.08408 \n",
      "[ 87/100] train_loss: 0.09131 valid_loss: 0.11612 test_loss: 0.08508 \n",
      "[ 88/100] train_loss: 0.09455 valid_loss: 0.10473 test_loss: 0.08508 \n",
      "Validation loss decreased (0.106833 --> 0.104734).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09254 valid_loss: 0.11614 test_loss: 0.09198 \n",
      "[ 90/100] train_loss: 0.09205 valid_loss: 0.10615 test_loss: 0.08831 \n",
      "[ 91/100] train_loss: 0.09253 valid_loss: 0.10591 test_loss: 0.08499 \n",
      "[ 92/100] train_loss: 0.09232 valid_loss: 0.10351 test_loss: 0.08312 \n",
      "Validation loss decreased (0.104734 --> 0.103509).  Saving model ...\n",
      "[ 93/100] train_loss: 0.08924 valid_loss: 0.10260 test_loss: 0.08268 \n",
      "Validation loss decreased (0.103509 --> 0.102598).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09196 valid_loss: 0.10501 test_loss: 0.08444 \n",
      "[ 95/100] train_loss: 0.08729 valid_loss: 0.10457 test_loss: 0.08298 \n",
      "[ 96/100] train_loss: 0.09109 valid_loss: 0.10567 test_loss: 0.08432 \n",
      "[ 97/100] train_loss: 0.09205 valid_loss: 0.11625 test_loss: 0.08638 \n",
      "[ 98/100] train_loss: 0.09242 valid_loss: 0.10233 test_loss: 0.08293 \n",
      "Validation loss decreased (0.102598 --> 0.102327).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09048 valid_loss: 0.10282 test_loss: 0.08440 \n",
      "[100/100] train_loss: 0.08951 valid_loss: 0.10156 test_loss: 0.08141 \n",
      "Validation loss decreased (0.102327 --> 0.101560).  Saving model ...\n",
      "TRAINING MODEL 2\n",
      "[  1/100] train_loss: 0.58178 valid_loss: 0.48381 test_loss: 0.44241 \n",
      "Validation loss decreased (inf --> 0.483810).  Saving model ...\n",
      "[  2/100] train_loss: 0.39231 valid_loss: 0.35576 test_loss: 0.29255 \n",
      "Validation loss decreased (0.483810 --> 0.355759).  Saving model ...\n",
      "[  3/100] train_loss: 0.30570 valid_loss: 0.29885 test_loss: 0.23040 \n",
      "Validation loss decreased (0.355759 --> 0.298845).  Saving model ...\n",
      "[  4/100] train_loss: 0.25571 valid_loss: 0.26479 test_loss: 0.19483 \n",
      "Validation loss decreased (0.298845 --> 0.264790).  Saving model ...\n",
      "[  5/100] train_loss: 0.22531 valid_loss: 0.23895 test_loss: 0.17252 \n",
      "Validation loss decreased (0.264790 --> 0.238949).  Saving model ...\n",
      "[  6/100] train_loss: 0.20683 valid_loss: 0.22906 test_loss: 0.16484 \n",
      "Validation loss decreased (0.238949 --> 0.229058).  Saving model ...\n",
      "[  7/100] train_loss: 0.19550 valid_loss: 0.21241 test_loss: 0.14822 \n",
      "Validation loss decreased (0.229058 --> 0.212414).  Saving model ...\n",
      "[  8/100] train_loss: 0.18821 valid_loss: 0.20564 test_loss: 0.14068 \n",
      "Validation loss decreased (0.212414 --> 0.205635).  Saving model ...\n",
      "[  9/100] train_loss: 0.17860 valid_loss: 0.20334 test_loss: 0.14706 \n",
      "Validation loss decreased (0.205635 --> 0.203335).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17755 valid_loss: 0.19878 test_loss: 0.13303 \n",
      "Validation loss decreased (0.203335 --> 0.198778).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16927 valid_loss: 0.19078 test_loss: 0.13151 \n",
      "Validation loss decreased (0.198778 --> 0.190779).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16841 valid_loss: 0.18037 test_loss: 0.13151 \n",
      "Validation loss decreased (0.190779 --> 0.180370).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16132 valid_loss: 0.17541 test_loss: 0.12685 \n",
      "Validation loss decreased (0.180370 --> 0.175407).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15790 valid_loss: 0.17311 test_loss: 0.12148 \n",
      "Validation loss decreased (0.175407 --> 0.173112).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15483 valid_loss: 0.17294 test_loss: 0.12478 \n",
      "Validation loss decreased (0.173112 --> 0.172941).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15379 valid_loss: 0.16796 test_loss: 0.12349 \n",
      "Validation loss decreased (0.172941 --> 0.167957).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14673 valid_loss: 0.16990 test_loss: 0.11957 \n",
      "[ 18/100] train_loss: 0.14492 valid_loss: 0.16968 test_loss: 0.12677 \n",
      "[ 19/100] train_loss: 0.14106 valid_loss: 0.15746 test_loss: 0.12188 \n",
      "Validation loss decreased (0.167957 --> 0.157455).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14095 valid_loss: 0.16441 test_loss: 0.11557 \n",
      "[ 21/100] train_loss: 0.13623 valid_loss: 0.16471 test_loss: 0.11879 \n",
      "[ 22/100] train_loss: 0.13864 valid_loss: 0.16002 test_loss: 0.11693 \n",
      "[ 23/100] train_loss: 0.13782 valid_loss: 0.15758 test_loss: 0.11649 \n",
      "[ 24/100] train_loss: 0.13361 valid_loss: 0.16141 test_loss: 0.11703 \n",
      "[ 25/100] train_loss: 0.13233 valid_loss: 0.15151 test_loss: 0.11685 \n",
      "Validation loss decreased (0.157455 --> 0.151511).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13007 valid_loss: 0.15215 test_loss: 0.11606 \n",
      "[ 27/100] train_loss: 0.12978 valid_loss: 0.15211 test_loss: 0.11486 \n",
      "[ 28/100] train_loss: 0.13125 valid_loss: 0.14562 test_loss: 0.10741 \n",
      "Validation loss decreased (0.151511 --> 0.145625).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12796 valid_loss: 0.15352 test_loss: 0.11125 \n",
      "[ 30/100] train_loss: 0.12922 valid_loss: 0.14351 test_loss: 0.10951 \n",
      "Validation loss decreased (0.145625 --> 0.143514).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12511 valid_loss: 0.14207 test_loss: 0.10374 \n",
      "Validation loss decreased (0.143514 --> 0.142069).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12192 valid_loss: 0.14703 test_loss: 0.10555 \n",
      "[ 33/100] train_loss: 0.12533 valid_loss: 0.14146 test_loss: 0.10474 \n",
      "Validation loss decreased (0.142069 --> 0.141463).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12597 valid_loss: 0.14189 test_loss: 0.10922 \n",
      "[ 35/100] train_loss: 0.12327 valid_loss: 0.13784 test_loss: 0.10328 \n",
      "Validation loss decreased (0.141463 --> 0.137841).  Saving model ...\n",
      "[ 36/100] train_loss: 0.12375 valid_loss: 0.13714 test_loss: 0.10615 \n",
      "Validation loss decreased (0.137841 --> 0.137141).  Saving model ...\n",
      "[ 37/100] train_loss: 0.12185 valid_loss: 0.13540 test_loss: 0.10419 \n",
      "Validation loss decreased (0.137141 --> 0.135402).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11822 valid_loss: 0.13472 test_loss: 0.09564 \n",
      "Validation loss decreased (0.135402 --> 0.134715).  Saving model ...\n",
      "[ 39/100] train_loss: 0.12083 valid_loss: 0.13513 test_loss: 0.10360 \n",
      "[ 40/100] train_loss: 0.11575 valid_loss: 0.13074 test_loss: 0.09942 \n",
      "Validation loss decreased (0.134715 --> 0.130738).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11887 valid_loss: 0.13085 test_loss: 0.09402 \n",
      "[ 42/100] train_loss: 0.11610 valid_loss: 0.14131 test_loss: 0.10057 \n",
      "[ 43/100] train_loss: 0.11414 valid_loss: 0.13130 test_loss: 0.09673 \n",
      "[ 44/100] train_loss: 0.11355 valid_loss: 0.13130 test_loss: 0.09773 \n",
      "[ 45/100] train_loss: 0.11514 valid_loss: 0.13569 test_loss: 0.09777 \n",
      "[ 46/100] train_loss: 0.11196 valid_loss: 0.12822 test_loss: 0.09415 \n",
      "Validation loss decreased (0.130738 --> 0.128223).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11091 valid_loss: 0.12619 test_loss: 0.09442 \n",
      "Validation loss decreased (0.128223 --> 0.126190).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11394 valid_loss: 0.12452 test_loss: 0.09365 \n",
      "Validation loss decreased (0.126190 --> 0.124519).  Saving model ...\n",
      "[ 49/100] train_loss: 0.11431 valid_loss: 0.12755 test_loss: 0.09644 \n",
      "[ 50/100] train_loss: 0.11006 valid_loss: 0.12657 test_loss: 0.09363 \n",
      "[ 51/100] train_loss: 0.10766 valid_loss: 0.12885 test_loss: 0.09665 \n",
      "[ 52/100] train_loss: 0.10953 valid_loss: 0.12160 test_loss: 0.09206 \n",
      "Validation loss decreased (0.124519 --> 0.121599).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10774 valid_loss: 0.12326 test_loss: 0.09630 \n",
      "[ 54/100] train_loss: 0.10545 valid_loss: 0.12290 test_loss: 0.09322 \n",
      "[ 55/100] train_loss: 0.10944 valid_loss: 0.12237 test_loss: 0.09435 \n",
      "[ 56/100] train_loss: 0.10512 valid_loss: 0.12189 test_loss: 0.08936 \n",
      "[ 57/100] train_loss: 0.10531 valid_loss: 0.12414 test_loss: 0.09088 \n",
      "[ 58/100] train_loss: 0.10710 valid_loss: 0.12343 test_loss: 0.09296 \n",
      "[ 59/100] train_loss: 0.10741 valid_loss: 0.11891 test_loss: 0.08864 \n",
      "Validation loss decreased (0.121599 --> 0.118910).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10558 valid_loss: 0.12074 test_loss: 0.08941 \n",
      "[ 61/100] train_loss: 0.10288 valid_loss: 0.12081 test_loss: 0.09073 \n",
      "[ 62/100] train_loss: 0.10425 valid_loss: 0.11579 test_loss: 0.09009 \n",
      "Validation loss decreased (0.118910 --> 0.115794).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10136 valid_loss: 0.11557 test_loss: 0.08786 \n",
      "Validation loss decreased (0.115794 --> 0.115572).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10468 valid_loss: 0.11803 test_loss: 0.08524 \n",
      "[ 65/100] train_loss: 0.10352 valid_loss: 0.11563 test_loss: 0.08693 \n",
      "[ 66/100] train_loss: 0.10173 valid_loss: 0.11364 test_loss: 0.09025 \n",
      "Validation loss decreased (0.115572 --> 0.113639).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10197 valid_loss: 0.11403 test_loss: 0.08848 \n",
      "[ 68/100] train_loss: 0.10027 valid_loss: 0.11543 test_loss: 0.08634 \n",
      "[ 69/100] train_loss: 0.10013 valid_loss: 0.11601 test_loss: 0.08688 \n",
      "[ 70/100] train_loss: 0.10079 valid_loss: 0.11314 test_loss: 0.08311 \n",
      "Validation loss decreased (0.113639 --> 0.113145).  Saving model ...\n",
      "[ 71/100] train_loss: 0.10169 valid_loss: 0.11366 test_loss: 0.08402 \n",
      "[ 72/100] train_loss: 0.09831 valid_loss: 0.11298 test_loss: 0.08529 \n",
      "Validation loss decreased (0.113145 --> 0.112978).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09777 valid_loss: 0.11193 test_loss: 0.08673 \n",
      "Validation loss decreased (0.112978 --> 0.111927).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09760 valid_loss: 0.11008 test_loss: 0.08439 \n",
      "Validation loss decreased (0.111927 --> 0.110076).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09987 valid_loss: 0.11205 test_loss: 0.08804 \n",
      "[ 76/100] train_loss: 0.09765 valid_loss: 0.11241 test_loss: 0.08297 \n",
      "[ 77/100] train_loss: 0.09844 valid_loss: 0.11090 test_loss: 0.08425 \n",
      "[ 78/100] train_loss: 0.09747 valid_loss: 0.10918 test_loss: 0.08218 \n",
      "Validation loss decreased (0.110076 --> 0.109179).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09519 valid_loss: 0.11112 test_loss: 0.08425 \n",
      "[ 80/100] train_loss: 0.09713 valid_loss: 0.11026 test_loss: 0.08329 \n",
      "[ 81/100] train_loss: 0.09706 valid_loss: 0.11094 test_loss: 0.08447 \n",
      "[ 82/100] train_loss: 0.09716 valid_loss: 0.10700 test_loss: 0.08192 \n",
      "Validation loss decreased (0.109179 --> 0.107000).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09640 valid_loss: 0.10717 test_loss: 0.08214 \n",
      "[ 84/100] train_loss: 0.09613 valid_loss: 0.11117 test_loss: 0.08162 \n",
      "[ 85/100] train_loss: 0.09213 valid_loss: 0.11653 test_loss: 0.08773 \n",
      "[ 86/100] train_loss: 0.09414 valid_loss: 0.10697 test_loss: 0.08251 \n",
      "Validation loss decreased (0.107000 --> 0.106974).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09413 valid_loss: 0.10524 test_loss: 0.08364 \n",
      "Validation loss decreased (0.106974 --> 0.105240).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09512 valid_loss: 0.10794 test_loss: 0.08301 \n",
      "[ 89/100] train_loss: 0.09226 valid_loss: 0.10644 test_loss: 0.08167 \n",
      "[ 90/100] train_loss: 0.09351 valid_loss: 0.10729 test_loss: 0.08317 \n",
      "[ 91/100] train_loss: 0.09467 valid_loss: 0.10698 test_loss: 0.08141 \n",
      "[ 92/100] train_loss: 0.09190 valid_loss: 0.10715 test_loss: 0.08146 \n",
      "[ 93/100] train_loss: 0.09218 valid_loss: 0.10531 test_loss: 0.08149 \n",
      "[ 94/100] train_loss: 0.09309 valid_loss: 0.10475 test_loss: 0.08312 \n",
      "Validation loss decreased (0.105240 --> 0.104750).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09242 valid_loss: 0.10630 test_loss: 0.07905 \n",
      "[ 96/100] train_loss: 0.09302 valid_loss: 0.10430 test_loss: 0.07881 \n",
      "Validation loss decreased (0.104750 --> 0.104301).  Saving model ...\n",
      "[ 97/100] train_loss: 0.08968 valid_loss: 0.10739 test_loss: 0.08016 \n",
      "[ 98/100] train_loss: 0.09093 valid_loss: 0.10489 test_loss: 0.08031 \n",
      "[ 99/100] train_loss: 0.08928 valid_loss: 0.10627 test_loss: 0.08041 \n",
      "[100/100] train_loss: 0.09136 valid_loss: 0.10684 test_loss: 0.08051 \n",
      "TRAINING MODEL 3\n",
      "[  1/100] train_loss: 0.56319 valid_loss: 0.47432 test_loss: 0.43171 \n",
      "Validation loss decreased (inf --> 0.474317).  Saving model ...\n",
      "[  2/100] train_loss: 0.38224 valid_loss: 0.35289 test_loss: 0.28624 \n",
      "Validation loss decreased (0.474317 --> 0.352888).  Saving model ...\n",
      "[  3/100] train_loss: 0.29513 valid_loss: 0.29588 test_loss: 0.21889 \n",
      "Validation loss decreased (0.352888 --> 0.295876).  Saving model ...\n",
      "[  4/100] train_loss: 0.24904 valid_loss: 0.25771 test_loss: 0.18701 \n",
      "Validation loss decreased (0.295876 --> 0.257712).  Saving model ...\n",
      "[  5/100] train_loss: 0.22089 valid_loss: 0.23448 test_loss: 0.16346 \n",
      "Validation loss decreased (0.257712 --> 0.234479).  Saving model ...\n",
      "[  6/100] train_loss: 0.20397 valid_loss: 0.21805 test_loss: 0.15446 \n",
      "Validation loss decreased (0.234479 --> 0.218052).  Saving model ...\n",
      "[  7/100] train_loss: 0.19580 valid_loss: 0.21229 test_loss: 0.14517 \n",
      "Validation loss decreased (0.218052 --> 0.212289).  Saving model ...\n",
      "[  8/100] train_loss: 0.18632 valid_loss: 0.19878 test_loss: 0.13719 \n",
      "Validation loss decreased (0.212289 --> 0.198781).  Saving model ...\n",
      "[  9/100] train_loss: 0.17740 valid_loss: 0.19437 test_loss: 0.13706 \n",
      "Validation loss decreased (0.198781 --> 0.194368).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16783 valid_loss: 0.19251 test_loss: 0.13486 \n",
      "Validation loss decreased (0.194368 --> 0.192509).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16529 valid_loss: 0.18522 test_loss: 0.13447 \n",
      "Validation loss decreased (0.192509 --> 0.185224).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16050 valid_loss: 0.17954 test_loss: 0.13238 \n",
      "Validation loss decreased (0.185224 --> 0.179543).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15357 valid_loss: 0.17089 test_loss: 0.12275 \n",
      "Validation loss decreased (0.179543 --> 0.170886).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15573 valid_loss: 0.17208 test_loss: 0.12114 \n",
      "[ 15/100] train_loss: 0.15183 valid_loss: 0.17132 test_loss: 0.12943 \n",
      "[ 16/100] train_loss: 0.14726 valid_loss: 0.17981 test_loss: 0.13024 \n",
      "[ 17/100] train_loss: 0.14576 valid_loss: 0.16703 test_loss: 0.12731 \n",
      "Validation loss decreased (0.170886 --> 0.167025).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14267 valid_loss: 0.16952 test_loss: 0.13504 \n",
      "[ 19/100] train_loss: 0.13913 valid_loss: 0.16090 test_loss: 0.12288 \n",
      "Validation loss decreased (0.167025 --> 0.160897).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14239 valid_loss: 0.15897 test_loss: 0.13005 \n",
      "Validation loss decreased (0.160897 --> 0.158967).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13496 valid_loss: 0.15508 test_loss: 0.11426 \n",
      "Validation loss decreased (0.158967 --> 0.155082).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13308 valid_loss: 0.16093 test_loss: 0.12962 \n",
      "[ 23/100] train_loss: 0.13472 valid_loss: 0.15112 test_loss: 0.11395 \n",
      "Validation loss decreased (0.155082 --> 0.151125).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13040 valid_loss: 0.15626 test_loss: 0.12239 \n",
      "[ 25/100] train_loss: 0.13170 valid_loss: 0.15606 test_loss: 0.12437 \n",
      "[ 26/100] train_loss: 0.13013 valid_loss: 0.14302 test_loss: 0.10548 \n",
      "Validation loss decreased (0.151125 --> 0.143022).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12622 valid_loss: 0.14588 test_loss: 0.11426 \n",
      "[ 28/100] train_loss: 0.12330 valid_loss: 0.14071 test_loss: 0.10578 \n",
      "Validation loss decreased (0.143022 --> 0.140707).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12709 valid_loss: 0.14864 test_loss: 0.11648 \n",
      "[ 30/100] train_loss: 0.12132 valid_loss: 0.14265 test_loss: 0.10758 \n",
      "[ 31/100] train_loss: 0.12393 valid_loss: 0.14194 test_loss: 0.10187 \n",
      "[ 32/100] train_loss: 0.12240 valid_loss: 0.13828 test_loss: 0.10560 \n",
      "Validation loss decreased (0.140707 --> 0.138282).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11922 valid_loss: 0.13654 test_loss: 0.10269 \n",
      "Validation loss decreased (0.138282 --> 0.136544).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11886 valid_loss: 0.13788 test_loss: 0.10347 \n",
      "[ 35/100] train_loss: 0.11686 valid_loss: 0.13593 test_loss: 0.09696 \n",
      "Validation loss decreased (0.136544 --> 0.135933).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11915 valid_loss: 0.13579 test_loss: 0.10251 \n",
      "Validation loss decreased (0.135933 --> 0.135786).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11573 valid_loss: 0.13431 test_loss: 0.09989 \n",
      "Validation loss decreased (0.135786 --> 0.134311).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11473 valid_loss: 0.13382 test_loss: 0.09716 \n",
      "Validation loss decreased (0.134311 --> 0.133818).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11399 valid_loss: 0.13164 test_loss: 0.09708 \n",
      "Validation loss decreased (0.133818 --> 0.131640).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11403 valid_loss: 0.13278 test_loss: 0.09997 \n",
      "[ 41/100] train_loss: 0.11333 valid_loss: 0.13066 test_loss: 0.09484 \n",
      "Validation loss decreased (0.131640 --> 0.130658).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11249 valid_loss: 0.13432 test_loss: 0.09759 \n",
      "[ 43/100] train_loss: 0.11047 valid_loss: 0.12672 test_loss: 0.09583 \n",
      "Validation loss decreased (0.130658 --> 0.126722).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11272 valid_loss: 0.13059 test_loss: 0.09630 \n",
      "[ 45/100] train_loss: 0.11389 valid_loss: 0.13120 test_loss: 0.09834 \n",
      "[ 46/100] train_loss: 0.11136 valid_loss: 0.12792 test_loss: 0.09420 \n",
      "[ 47/100] train_loss: 0.10980 valid_loss: 0.12565 test_loss: 0.09403 \n",
      "Validation loss decreased (0.126722 --> 0.125646).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10593 valid_loss: 0.12682 test_loss: 0.09677 \n",
      "[ 49/100] train_loss: 0.10724 valid_loss: 0.12765 test_loss: 0.09930 \n",
      "[ 50/100] train_loss: 0.10882 valid_loss: 0.12562 test_loss: 0.09478 \n",
      "Validation loss decreased (0.125646 --> 0.125623).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10338 valid_loss: 0.12571 test_loss: 0.09655 \n",
      "[ 52/100] train_loss: 0.10542 valid_loss: 0.12420 test_loss: 0.09262 \n",
      "Validation loss decreased (0.125623 --> 0.124198).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10795 valid_loss: 0.12228 test_loss: 0.09252 \n",
      "Validation loss decreased (0.124198 --> 0.122278).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10609 valid_loss: 0.12033 test_loss: 0.09065 \n",
      "Validation loss decreased (0.122278 --> 0.120326).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10414 valid_loss: 0.12440 test_loss: 0.09242 \n",
      "[ 56/100] train_loss: 0.10424 valid_loss: 0.12376 test_loss: 0.09359 \n",
      "[ 57/100] train_loss: 0.10201 valid_loss: 0.12252 test_loss: 0.09043 \n",
      "[ 58/100] train_loss: 0.10714 valid_loss: 0.11973 test_loss: 0.08845 \n",
      "Validation loss decreased (0.120326 --> 0.119727).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10301 valid_loss: 0.11963 test_loss: 0.08905 \n",
      "Validation loss decreased (0.119727 --> 0.119635).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10249 valid_loss: 0.11974 test_loss: 0.08772 \n",
      "[ 61/100] train_loss: 0.10161 valid_loss: 0.12036 test_loss: 0.08978 \n",
      "[ 62/100] train_loss: 0.10141 valid_loss: 0.11874 test_loss: 0.09428 \n",
      "Validation loss decreased (0.119635 --> 0.118742).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10092 valid_loss: 0.11615 test_loss: 0.08588 \n",
      "Validation loss decreased (0.118742 --> 0.116148).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10131 valid_loss: 0.11722 test_loss: 0.08859 \n",
      "[ 65/100] train_loss: 0.09820 valid_loss: 0.11556 test_loss: 0.08546 \n",
      "Validation loss decreased (0.116148 --> 0.115561).  Saving model ...\n",
      "[ 66/100] train_loss: 0.09939 valid_loss: 0.11611 test_loss: 0.08308 \n",
      "[ 67/100] train_loss: 0.09954 valid_loss: 0.11408 test_loss: 0.08673 \n",
      "Validation loss decreased (0.115561 --> 0.114083).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10014 valid_loss: 0.11503 test_loss: 0.08640 \n",
      "[ 69/100] train_loss: 0.09873 valid_loss: 0.11484 test_loss: 0.08428 \n",
      "[ 70/100] train_loss: 0.09777 valid_loss: 0.11369 test_loss: 0.08611 \n",
      "Validation loss decreased (0.114083 --> 0.113690).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09993 valid_loss: 0.11478 test_loss: 0.08530 \n",
      "[ 72/100] train_loss: 0.09807 valid_loss: 0.11320 test_loss: 0.08428 \n",
      "Validation loss decreased (0.113690 --> 0.113205).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09845 valid_loss: 0.11122 test_loss: 0.08629 \n",
      "Validation loss decreased (0.113205 --> 0.111220).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09464 valid_loss: 0.11063 test_loss: 0.08415 \n",
      "Validation loss decreased (0.111220 --> 0.110629).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09504 valid_loss: 0.11289 test_loss: 0.08069 \n",
      "[ 76/100] train_loss: 0.09341 valid_loss: 0.11218 test_loss: 0.08104 \n",
      "[ 77/100] train_loss: 0.09499 valid_loss: 0.10914 test_loss: 0.08295 \n",
      "Validation loss decreased (0.110629 --> 0.109144).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09570 valid_loss: 0.11008 test_loss: 0.08281 \n",
      "[ 79/100] train_loss: 0.09572 valid_loss: 0.10966 test_loss: 0.08172 \n",
      "[ 80/100] train_loss: 0.09447 valid_loss: 0.10955 test_loss: 0.07932 \n",
      "[ 81/100] train_loss: 0.09487 valid_loss: 0.11023 test_loss: 0.08118 \n",
      "[ 82/100] train_loss: 0.09449 valid_loss: 0.11094 test_loss: 0.08168 \n",
      "[ 83/100] train_loss: 0.09329 valid_loss: 0.11155 test_loss: 0.08285 \n",
      "[ 84/100] train_loss: 0.09154 valid_loss: 0.10940 test_loss: 0.08410 \n",
      "[ 85/100] train_loss: 0.09374 valid_loss: 0.10821 test_loss: 0.07921 \n",
      "Validation loss decreased (0.109144 --> 0.108208).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09536 valid_loss: 0.10931 test_loss: 0.08282 \n",
      "[ 87/100] train_loss: 0.09035 valid_loss: 0.10612 test_loss: 0.08219 \n",
      "Validation loss decreased (0.108208 --> 0.106115).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09200 valid_loss: 0.10510 test_loss: 0.07894 \n",
      "Validation loss decreased (0.106115 --> 0.105104).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09079 valid_loss: 0.10631 test_loss: 0.07764 \n",
      "[ 90/100] train_loss: 0.09232 valid_loss: 0.10674 test_loss: 0.07946 \n",
      "[ 91/100] train_loss: 0.08858 valid_loss: 0.10569 test_loss: 0.07954 \n",
      "[ 92/100] train_loss: 0.09129 valid_loss: 0.10689 test_loss: 0.07956 \n",
      "[ 93/100] train_loss: 0.08807 valid_loss: 0.10890 test_loss: 0.08227 \n",
      "[ 94/100] train_loss: 0.08765 valid_loss: 0.10672 test_loss: 0.07951 \n",
      "[ 95/100] train_loss: 0.08984 valid_loss: 0.10470 test_loss: 0.07802 \n",
      "Validation loss decreased (0.105104 --> 0.104703).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09082 valid_loss: 0.10504 test_loss: 0.08067 \n",
      "[ 97/100] train_loss: 0.09338 valid_loss: 0.10722 test_loss: 0.07943 \n",
      "[ 98/100] train_loss: 0.09012 valid_loss: 0.10220 test_loss: 0.07857 \n",
      "Validation loss decreased (0.104703 --> 0.102196).  Saving model ...\n",
      "[ 99/100] train_loss: 0.08704 valid_loss: 0.10724 test_loss: 0.07888 \n",
      "[100/100] train_loss: 0.08829 valid_loss: 0.10404 test_loss: 0.07713 \n",
      "TRAINING MODEL 4\n",
      "[  1/100] train_loss: 0.57545 valid_loss: 0.47218 test_loss: 0.42772 \n",
      "Validation loss decreased (inf --> 0.472182).  Saving model ...\n",
      "[  2/100] train_loss: 0.38097 valid_loss: 0.34559 test_loss: 0.28100 \n",
      "Validation loss decreased (0.472182 --> 0.345591).  Saving model ...\n",
      "[  3/100] train_loss: 0.29262 valid_loss: 0.29687 test_loss: 0.21965 \n",
      "Validation loss decreased (0.345591 --> 0.296868).  Saving model ...\n",
      "[  4/100] train_loss: 0.25065 valid_loss: 0.26305 test_loss: 0.18851 \n",
      "Validation loss decreased (0.296868 --> 0.263048).  Saving model ...\n",
      "[  5/100] train_loss: 0.22374 valid_loss: 0.24135 test_loss: 0.16666 \n",
      "Validation loss decreased (0.263048 --> 0.241345).  Saving model ...\n",
      "[  6/100] train_loss: 0.21159 valid_loss: 0.22417 test_loss: 0.15833 \n",
      "Validation loss decreased (0.241345 --> 0.224174).  Saving model ...\n",
      "[  7/100] train_loss: 0.20211 valid_loss: 0.21931 test_loss: 0.14606 \n",
      "Validation loss decreased (0.224174 --> 0.219313).  Saving model ...\n",
      "[  8/100] train_loss: 0.18497 valid_loss: 0.21560 test_loss: 0.14444 \n",
      "Validation loss decreased (0.219313 --> 0.215602).  Saving model ...\n",
      "[  9/100] train_loss: 0.18158 valid_loss: 0.20618 test_loss: 0.13756 \n",
      "Validation loss decreased (0.215602 --> 0.206180).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17320 valid_loss: 0.20322 test_loss: 0.13165 \n",
      "Validation loss decreased (0.206180 --> 0.203221).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16577 valid_loss: 0.18542 test_loss: 0.13308 \n",
      "Validation loss decreased (0.203221 --> 0.185415).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16094 valid_loss: 0.17577 test_loss: 0.12604 \n",
      "Validation loss decreased (0.185415 --> 0.175772).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15602 valid_loss: 0.17845 test_loss: 0.12440 \n",
      "[ 14/100] train_loss: 0.15346 valid_loss: 0.17128 test_loss: 0.12274 \n",
      "Validation loss decreased (0.175772 --> 0.171283).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14803 valid_loss: 0.16810 test_loss: 0.12070 \n",
      "Validation loss decreased (0.171283 --> 0.168098).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14835 valid_loss: 0.16154 test_loss: 0.11652 \n",
      "Validation loss decreased (0.168098 --> 0.161536).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14450 valid_loss: 0.16158 test_loss: 0.11308 \n",
      "[ 18/100] train_loss: 0.14613 valid_loss: 0.16956 test_loss: 0.11560 \n",
      "[ 19/100] train_loss: 0.14337 valid_loss: 0.15429 test_loss: 0.11118 \n",
      "Validation loss decreased (0.161536 --> 0.154288).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13903 valid_loss: 0.15897 test_loss: 0.11257 \n",
      "[ 21/100] train_loss: 0.14061 valid_loss: 0.15950 test_loss: 0.11164 \n",
      "[ 22/100] train_loss: 0.13586 valid_loss: 0.15159 test_loss: 0.10865 \n",
      "Validation loss decreased (0.154288 --> 0.151590).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13685 valid_loss: 0.15269 test_loss: 0.10693 \n",
      "[ 24/100] train_loss: 0.13121 valid_loss: 0.15121 test_loss: 0.10862 \n",
      "Validation loss decreased (0.151590 --> 0.151212).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13588 valid_loss: 0.14677 test_loss: 0.10413 \n",
      "Validation loss decreased (0.151212 --> 0.146773).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13412 valid_loss: 0.14559 test_loss: 0.10609 \n",
      "Validation loss decreased (0.146773 --> 0.145590).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13174 valid_loss: 0.15260 test_loss: 0.10999 \n",
      "[ 28/100] train_loss: 0.13059 valid_loss: 0.14844 test_loss: 0.10899 \n",
      "[ 29/100] train_loss: 0.13221 valid_loss: 0.14708 test_loss: 0.10364 \n",
      "[ 30/100] train_loss: 0.12865 valid_loss: 0.14236 test_loss: 0.10244 \n",
      "Validation loss decreased (0.145590 --> 0.142360).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12798 valid_loss: 0.14246 test_loss: 0.10749 \n",
      "[ 32/100] train_loss: 0.12576 valid_loss: 0.14153 test_loss: 0.10094 \n",
      "Validation loss decreased (0.142360 --> 0.141531).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12650 valid_loss: 0.13968 test_loss: 0.10476 \n",
      "Validation loss decreased (0.141531 --> 0.139680).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12659 valid_loss: 0.13994 test_loss: 0.10628 \n",
      "[ 35/100] train_loss: 0.12229 valid_loss: 0.14853 test_loss: 0.10480 \n",
      "[ 36/100] train_loss: 0.11988 valid_loss: 0.13463 test_loss: 0.10129 \n",
      "Validation loss decreased (0.139680 --> 0.134628).  Saving model ...\n",
      "[ 37/100] train_loss: 0.12180 valid_loss: 0.13932 test_loss: 0.10193 \n",
      "[ 38/100] train_loss: 0.12042 valid_loss: 0.13735 test_loss: 0.09697 \n",
      "[ 39/100] train_loss: 0.12213 valid_loss: 0.13524 test_loss: 0.09865 \n",
      "[ 40/100] train_loss: 0.12276 valid_loss: 0.13444 test_loss: 0.09780 \n",
      "Validation loss decreased (0.134628 --> 0.134440).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11884 valid_loss: 0.13002 test_loss: 0.09755 \n",
      "Validation loss decreased (0.134440 --> 0.130020).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11856 valid_loss: 0.13521 test_loss: 0.09342 \n",
      "[ 43/100] train_loss: 0.11543 valid_loss: 0.13402 test_loss: 0.09570 \n",
      "[ 44/100] train_loss: 0.11627 valid_loss: 0.13266 test_loss: 0.09914 \n",
      "[ 45/100] train_loss: 0.11613 valid_loss: 0.12971 test_loss: 0.09511 \n",
      "Validation loss decreased (0.130020 --> 0.129709).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11458 valid_loss: 0.13283 test_loss: 0.09622 \n",
      "[ 47/100] train_loss: 0.11393 valid_loss: 0.12599 test_loss: 0.09327 \n",
      "Validation loss decreased (0.129709 --> 0.125995).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11427 valid_loss: 0.12808 test_loss: 0.09451 \n",
      "[ 49/100] train_loss: 0.11608 valid_loss: 0.12996 test_loss: 0.09408 \n",
      "[ 50/100] train_loss: 0.10917 valid_loss: 0.12742 test_loss: 0.09158 \n",
      "[ 51/100] train_loss: 0.10957 valid_loss: 0.12697 test_loss: 0.09460 \n",
      "[ 52/100] train_loss: 0.11015 valid_loss: 0.12266 test_loss: 0.09092 \n",
      "Validation loss decreased (0.125995 --> 0.122663).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10960 valid_loss: 0.12679 test_loss: 0.09388 \n",
      "[ 54/100] train_loss: 0.10948 valid_loss: 0.12493 test_loss: 0.09051 \n",
      "[ 55/100] train_loss: 0.10847 valid_loss: 0.12453 test_loss: 0.09429 \n",
      "[ 56/100] train_loss: 0.11049 valid_loss: 0.12298 test_loss: 0.09204 \n",
      "[ 57/100] train_loss: 0.11004 valid_loss: 0.12343 test_loss: 0.09196 \n",
      "[ 58/100] train_loss: 0.10418 valid_loss: 0.12615 test_loss: 0.08942 \n",
      "[ 59/100] train_loss: 0.10451 valid_loss: 0.12130 test_loss: 0.09054 \n",
      "Validation loss decreased (0.122663 --> 0.121304).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10671 valid_loss: 0.12336 test_loss: 0.09139 \n",
      "[ 61/100] train_loss: 0.10445 valid_loss: 0.12431 test_loss: 0.08975 \n",
      "[ 62/100] train_loss: 0.10615 valid_loss: 0.12172 test_loss: 0.08943 \n",
      "[ 63/100] train_loss: 0.10349 valid_loss: 0.12286 test_loss: 0.08898 \n",
      "[ 64/100] train_loss: 0.10317 valid_loss: 0.11601 test_loss: 0.08804 \n",
      "Validation loss decreased (0.121304 --> 0.116008).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10299 valid_loss: 0.11722 test_loss: 0.09226 \n",
      "[ 66/100] train_loss: 0.10443 valid_loss: 0.11844 test_loss: 0.08606 \n",
      "[ 67/100] train_loss: 0.10209 valid_loss: 0.11724 test_loss: 0.08841 \n",
      "[ 68/100] train_loss: 0.10382 valid_loss: 0.11407 test_loss: 0.08384 \n",
      "Validation loss decreased (0.116008 --> 0.114073).  Saving model ...\n",
      "[ 69/100] train_loss: 0.09955 valid_loss: 0.11611 test_loss: 0.08713 \n",
      "[ 70/100] train_loss: 0.09952 valid_loss: 0.11576 test_loss: 0.08378 \n",
      "[ 71/100] train_loss: 0.10454 valid_loss: 0.11589 test_loss: 0.08498 \n",
      "[ 72/100] train_loss: 0.09959 valid_loss: 0.11645 test_loss: 0.08521 \n",
      "[ 73/100] train_loss: 0.09802 valid_loss: 0.11267 test_loss: 0.08606 \n",
      "Validation loss decreased (0.114073 --> 0.112665).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09854 valid_loss: 0.11749 test_loss: 0.08561 \n",
      "[ 75/100] train_loss: 0.10106 valid_loss: 0.11424 test_loss: 0.08541 \n",
      "[ 76/100] train_loss: 0.09771 valid_loss: 0.11397 test_loss: 0.08942 \n",
      "[ 77/100] train_loss: 0.10014 valid_loss: 0.12138 test_loss: 0.08911 \n",
      "[ 78/100] train_loss: 0.10013 valid_loss: 0.11457 test_loss: 0.08873 \n",
      "[ 79/100] train_loss: 0.09922 valid_loss: 0.11408 test_loss: 0.08295 \n",
      "[ 80/100] train_loss: 0.09716 valid_loss: 0.11320 test_loss: 0.08651 \n",
      "[ 81/100] train_loss: 0.09542 valid_loss: 0.11725 test_loss: 0.08703 \n",
      "[ 82/100] train_loss: 0.09465 valid_loss: 0.11620 test_loss: 0.08547 \n",
      "[ 83/100] train_loss: 0.09991 valid_loss: 0.11234 test_loss: 0.08345 \n",
      "Validation loss decreased (0.112665 --> 0.112340).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09774 valid_loss: 0.10990 test_loss: 0.08493 \n",
      "Validation loss decreased (0.112340 --> 0.109905).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09633 valid_loss: 0.11302 test_loss: 0.08610 \n",
      "[ 86/100] train_loss: 0.09510 valid_loss: 0.10943 test_loss: 0.08166 \n",
      "Validation loss decreased (0.109905 --> 0.109429).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09175 valid_loss: 0.10879 test_loss: 0.08267 \n",
      "Validation loss decreased (0.109429 --> 0.108791).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09317 valid_loss: 0.11084 test_loss: 0.08288 \n",
      "[ 89/100] train_loss: 0.09229 valid_loss: 0.10850 test_loss: 0.08230 \n",
      "Validation loss decreased (0.108791 --> 0.108504).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09181 valid_loss: 0.11415 test_loss: 0.08886 \n",
      "[ 91/100] train_loss: 0.09527 valid_loss: 0.10953 test_loss: 0.08348 \n",
      "[ 92/100] train_loss: 0.09466 valid_loss: 0.10912 test_loss: 0.08148 \n",
      "[ 93/100] train_loss: 0.09464 valid_loss: 0.11430 test_loss: 0.08750 \n",
      "[ 94/100] train_loss: 0.09408 valid_loss: 0.10959 test_loss: 0.08476 \n",
      "[ 95/100] train_loss: 0.09478 valid_loss: 0.11466 test_loss: 0.08720 \n",
      "[ 96/100] train_loss: 0.09351 valid_loss: 0.10604 test_loss: 0.08125 \n",
      "Validation loss decreased (0.108504 --> 0.106038).  Saving model ...\n",
      "[ 97/100] train_loss: 0.09607 valid_loss: 0.10753 test_loss: 0.08368 \n",
      "[ 98/100] train_loss: 0.09022 valid_loss: 0.10723 test_loss: 0.08369 \n",
      "[ 99/100] train_loss: 0.09076 valid_loss: 0.10974 test_loss: 0.08183 \n",
      "[100/100] train_loss: 0.08920 valid_loss: 0.10713 test_loss: 0.08078 \n",
      "TRAINING MODEL 5\n",
      "[  1/100] train_loss: 0.61736 valid_loss: 0.52571 test_loss: 0.50197 \n",
      "Validation loss decreased (inf --> 0.525709).  Saving model ...\n",
      "[  2/100] train_loss: 0.44579 valid_loss: 0.39689 test_loss: 0.34132 \n",
      "Validation loss decreased (0.525709 --> 0.396895).  Saving model ...\n",
      "[  3/100] train_loss: 0.34084 valid_loss: 0.33548 test_loss: 0.26746 \n",
      "Validation loss decreased (0.396895 --> 0.335481).  Saving model ...\n",
      "[  4/100] train_loss: 0.27860 valid_loss: 0.29228 test_loss: 0.22152 \n",
      "Validation loss decreased (0.335481 --> 0.292284).  Saving model ...\n",
      "[  5/100] train_loss: 0.24621 valid_loss: 0.26179 test_loss: 0.18927 \n",
      "Validation loss decreased (0.292284 --> 0.261794).  Saving model ...\n",
      "[  6/100] train_loss: 0.22374 valid_loss: 0.24915 test_loss: 0.17150 \n",
      "Validation loss decreased (0.261794 --> 0.249153).  Saving model ...\n",
      "[  7/100] train_loss: 0.20683 valid_loss: 0.23185 test_loss: 0.16148 \n",
      "Validation loss decreased (0.249153 --> 0.231846).  Saving model ...\n",
      "[  8/100] train_loss: 0.19807 valid_loss: 0.22520 test_loss: 0.14942 \n",
      "Validation loss decreased (0.231846 --> 0.225202).  Saving model ...\n",
      "[  9/100] train_loss: 0.18807 valid_loss: 0.20965 test_loss: 0.14014 \n",
      "Validation loss decreased (0.225202 --> 0.209655).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18138 valid_loss: 0.19942 test_loss: 0.13733 \n",
      "Validation loss decreased (0.209655 --> 0.199418).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17059 valid_loss: 0.19555 test_loss: 0.13072 \n",
      "Validation loss decreased (0.199418 --> 0.195549).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16455 valid_loss: 0.19448 test_loss: 0.13203 \n",
      "Validation loss decreased (0.195549 --> 0.194483).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16180 valid_loss: 0.19367 test_loss: 0.12622 \n",
      "Validation loss decreased (0.194483 --> 0.193668).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15910 valid_loss: 0.17656 test_loss: 0.12554 \n",
      "Validation loss decreased (0.193668 --> 0.176561).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15348 valid_loss: 0.16662 test_loss: 0.12109 \n",
      "Validation loss decreased (0.176561 --> 0.166622).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15387 valid_loss: 0.16619 test_loss: 0.12034 \n",
      "Validation loss decreased (0.166622 --> 0.166187).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14898 valid_loss: 0.16714 test_loss: 0.11866 \n",
      "[ 18/100] train_loss: 0.14781 valid_loss: 0.16838 test_loss: 0.11741 \n",
      "[ 19/100] train_loss: 0.14553 valid_loss: 0.15949 test_loss: 0.11706 \n",
      "Validation loss decreased (0.166187 --> 0.159488).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14478 valid_loss: 0.15587 test_loss: 0.11387 \n",
      "Validation loss decreased (0.159488 --> 0.155866).  Saving model ...\n",
      "[ 21/100] train_loss: 0.14037 valid_loss: 0.15618 test_loss: 0.11573 \n",
      "[ 22/100] train_loss: 0.13764 valid_loss: 0.15787 test_loss: 0.11270 \n",
      "[ 23/100] train_loss: 0.13973 valid_loss: 0.16350 test_loss: 0.11536 \n",
      "[ 24/100] train_loss: 0.13798 valid_loss: 0.15514 test_loss: 0.11031 \n",
      "Validation loss decreased (0.155866 --> 0.155136).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13600 valid_loss: 0.14719 test_loss: 0.10739 \n",
      "Validation loss decreased (0.155136 --> 0.147194).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13112 valid_loss: 0.15368 test_loss: 0.11885 \n",
      "[ 27/100] train_loss: 0.12939 valid_loss: 0.14837 test_loss: 0.11550 \n",
      "[ 28/100] train_loss: 0.13027 valid_loss: 0.15126 test_loss: 0.11606 \n",
      "[ 29/100] train_loss: 0.12794 valid_loss: 0.14978 test_loss: 0.11449 \n",
      "[ 30/100] train_loss: 0.12728 valid_loss: 0.14302 test_loss: 0.10811 \n",
      "Validation loss decreased (0.147194 --> 0.143015).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12645 valid_loss: 0.14713 test_loss: 0.10488 \n",
      "[ 32/100] train_loss: 0.12701 valid_loss: 0.14430 test_loss: 0.10892 \n",
      "[ 33/100] train_loss: 0.12704 valid_loss: 0.14080 test_loss: 0.10507 \n",
      "Validation loss decreased (0.143015 --> 0.140796).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12206 valid_loss: 0.14167 test_loss: 0.10722 \n",
      "[ 35/100] train_loss: 0.12430 valid_loss: 0.14881 test_loss: 0.11181 \n",
      "[ 36/100] train_loss: 0.12147 valid_loss: 0.13824 test_loss: 0.10497 \n",
      "Validation loss decreased (0.140796 --> 0.138242).  Saving model ...\n",
      "[ 37/100] train_loss: 0.12365 valid_loss: 0.14064 test_loss: 0.10116 \n",
      "[ 38/100] train_loss: 0.12052 valid_loss: 0.13485 test_loss: 0.10514 \n",
      "Validation loss decreased (0.138242 --> 0.134853).  Saving model ...\n",
      "[ 39/100] train_loss: 0.12202 valid_loss: 0.13358 test_loss: 0.10420 \n",
      "Validation loss decreased (0.134853 --> 0.133584).  Saving model ...\n",
      "[ 40/100] train_loss: 0.12072 valid_loss: 0.13690 test_loss: 0.09773 \n",
      "[ 41/100] train_loss: 0.11721 valid_loss: 0.14259 test_loss: 0.10302 \n",
      "[ 42/100] train_loss: 0.11441 valid_loss: 0.13114 test_loss: 0.10005 \n",
      "Validation loss decreased (0.133584 --> 0.131138).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11764 valid_loss: 0.13544 test_loss: 0.09978 \n",
      "[ 44/100] train_loss: 0.11862 valid_loss: 0.13160 test_loss: 0.09738 \n",
      "[ 45/100] train_loss: 0.11481 valid_loss: 0.13101 test_loss: 0.09716 \n",
      "Validation loss decreased (0.131138 --> 0.131015).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11379 valid_loss: 0.13189 test_loss: 0.10017 \n",
      "[ 47/100] train_loss: 0.11196 valid_loss: 0.12989 test_loss: 0.10006 \n",
      "Validation loss decreased (0.131015 --> 0.129889).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11409 valid_loss: 0.13790 test_loss: 0.09872 \n",
      "[ 49/100] train_loss: 0.10883 valid_loss: 0.12859 test_loss: 0.09704 \n",
      "Validation loss decreased (0.129889 --> 0.128595).  Saving model ...\n",
      "[ 50/100] train_loss: 0.11179 valid_loss: 0.12729 test_loss: 0.10030 \n",
      "Validation loss decreased (0.128595 --> 0.127293).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11320 valid_loss: 0.12968 test_loss: 0.10065 \n",
      "[ 52/100] train_loss: 0.11139 valid_loss: 0.13164 test_loss: 0.09764 \n",
      "[ 53/100] train_loss: 0.11028 valid_loss: 0.12486 test_loss: 0.09523 \n",
      "Validation loss decreased (0.127293 --> 0.124857).  Saving model ...\n",
      "[ 54/100] train_loss: 0.11182 valid_loss: 0.12669 test_loss: 0.09716 \n",
      "[ 55/100] train_loss: 0.10895 valid_loss: 0.12834 test_loss: 0.09851 \n",
      "[ 56/100] train_loss: 0.10566 valid_loss: 0.12262 test_loss: 0.09774 \n",
      "Validation loss decreased (0.124857 --> 0.122618).  Saving model ...\n",
      "[ 57/100] train_loss: 0.11091 valid_loss: 0.12431 test_loss: 0.09675 \n",
      "[ 58/100] train_loss: 0.10810 valid_loss: 0.12240 test_loss: 0.09251 \n",
      "Validation loss decreased (0.122618 --> 0.122405).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10755 valid_loss: 0.12546 test_loss: 0.09496 \n",
      "[ 60/100] train_loss: 0.10765 valid_loss: 0.12400 test_loss: 0.09062 \n",
      "[ 61/100] train_loss: 0.11069 valid_loss: 0.12137 test_loss: 0.09345 \n",
      "Validation loss decreased (0.122405 --> 0.121366).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10556 valid_loss: 0.12104 test_loss: 0.09435 \n",
      "Validation loss decreased (0.121366 --> 0.121039).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10490 valid_loss: 0.12416 test_loss: 0.09437 \n",
      "[ 64/100] train_loss: 0.10306 valid_loss: 0.12903 test_loss: 0.09168 \n",
      "[ 65/100] train_loss: 0.10234 valid_loss: 0.11874 test_loss: 0.08782 \n",
      "Validation loss decreased (0.121039 --> 0.118745).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10221 valid_loss: 0.11961 test_loss: 0.09260 \n",
      "[ 67/100] train_loss: 0.10367 valid_loss: 0.12128 test_loss: 0.09349 \n",
      "[ 68/100] train_loss: 0.10142 valid_loss: 0.11912 test_loss: 0.09342 \n",
      "[ 69/100] train_loss: 0.10113 valid_loss: 0.11932 test_loss: 0.08961 \n",
      "[ 70/100] train_loss: 0.10092 valid_loss: 0.12182 test_loss: 0.08805 \n",
      "[ 71/100] train_loss: 0.10001 valid_loss: 0.11597 test_loss: 0.08794 \n",
      "Validation loss decreased (0.118745 --> 0.115972).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09845 valid_loss: 0.11690 test_loss: 0.08858 \n",
      "[ 73/100] train_loss: 0.10061 valid_loss: 0.11836 test_loss: 0.09161 \n",
      "[ 74/100] train_loss: 0.09798 valid_loss: 0.11221 test_loss: 0.08771 \n",
      "Validation loss decreased (0.115972 --> 0.112210).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09883 valid_loss: 0.11368 test_loss: 0.08671 \n",
      "[ 76/100] train_loss: 0.09893 valid_loss: 0.11568 test_loss: 0.08621 \n",
      "[ 77/100] train_loss: 0.09690 valid_loss: 0.11462 test_loss: 0.08554 \n",
      "[ 78/100] train_loss: 0.10050 valid_loss: 0.11301 test_loss: 0.08798 \n",
      "[ 79/100] train_loss: 0.09717 valid_loss: 0.11257 test_loss: 0.08717 \n",
      "[ 80/100] train_loss: 0.09714 valid_loss: 0.11023 test_loss: 0.08328 \n",
      "Validation loss decreased (0.112210 --> 0.110228).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09450 valid_loss: 0.11123 test_loss: 0.08524 \n",
      "[ 82/100] train_loss: 0.09617 valid_loss: 0.11209 test_loss: 0.08593 \n",
      "[ 83/100] train_loss: 0.09913 valid_loss: 0.10874 test_loss: 0.08560 \n",
      "Validation loss decreased (0.110228 --> 0.108743).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09417 valid_loss: 0.11087 test_loss: 0.08379 \n",
      "[ 85/100] train_loss: 0.09576 valid_loss: 0.11255 test_loss: 0.08438 \n",
      "[ 86/100] train_loss: 0.09530 valid_loss: 0.10915 test_loss: 0.08142 \n",
      "[ 87/100] train_loss: 0.09521 valid_loss: 0.10942 test_loss: 0.08440 \n",
      "[ 88/100] train_loss: 0.09531 valid_loss: 0.11124 test_loss: 0.08410 \n",
      "[ 89/100] train_loss: 0.09621 valid_loss: 0.10953 test_loss: 0.08308 \n",
      "[ 90/100] train_loss: 0.09289 valid_loss: 0.10703 test_loss: 0.08317 \n",
      "Validation loss decreased (0.108743 --> 0.107034).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09232 valid_loss: 0.10921 test_loss: 0.08295 \n",
      "[ 92/100] train_loss: 0.09342 valid_loss: 0.10759 test_loss: 0.08393 \n",
      "[ 93/100] train_loss: 0.09421 valid_loss: 0.10988 test_loss: 0.08119 \n",
      "[ 94/100] train_loss: 0.09587 valid_loss: 0.10642 test_loss: 0.08293 \n",
      "Validation loss decreased (0.107034 --> 0.106416).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09323 valid_loss: 0.10900 test_loss: 0.08114 \n",
      "[ 96/100] train_loss: 0.09521 valid_loss: 0.11083 test_loss: 0.08198 \n",
      "[ 97/100] train_loss: 0.09154 valid_loss: 0.10699 test_loss: 0.08220 \n",
      "[ 98/100] train_loss: 0.09561 valid_loss: 0.10700 test_loss: 0.08336 \n",
      "[ 99/100] train_loss: 0.09333 valid_loss: 0.10658 test_loss: 0.07895 \n",
      "[100/100] train_loss: 0.09222 valid_loss: 0.10814 test_loss: 0.08074 \n",
      "TRAINING MODEL 6\n",
      "[  1/100] train_loss: 0.58667 valid_loss: 0.48197 test_loss: 0.44209 \n",
      "Validation loss decreased (inf --> 0.481970).  Saving model ...\n",
      "[  2/100] train_loss: 0.40186 valid_loss: 0.36144 test_loss: 0.29670 \n",
      "Validation loss decreased (0.481970 --> 0.361441).  Saving model ...\n",
      "[  3/100] train_loss: 0.30381 valid_loss: 0.30258 test_loss: 0.23281 \n",
      "Validation loss decreased (0.361441 --> 0.302582).  Saving model ...\n",
      "[  4/100] train_loss: 0.25437 valid_loss: 0.26308 test_loss: 0.19363 \n",
      "Validation loss decreased (0.302582 --> 0.263079).  Saving model ...\n",
      "[  5/100] train_loss: 0.22885 valid_loss: 0.24887 test_loss: 0.17256 \n",
      "Validation loss decreased (0.263079 --> 0.248866).  Saving model ...\n",
      "[  6/100] train_loss: 0.21062 valid_loss: 0.22864 test_loss: 0.15993 \n",
      "Validation loss decreased (0.248866 --> 0.228637).  Saving model ...\n",
      "[  7/100] train_loss: 0.19583 valid_loss: 0.20887 test_loss: 0.14575 \n",
      "Validation loss decreased (0.228637 --> 0.208874).  Saving model ...\n",
      "[  8/100] train_loss: 0.18783 valid_loss: 0.20237 test_loss: 0.14492 \n",
      "Validation loss decreased (0.208874 --> 0.202373).  Saving model ...\n",
      "[  9/100] train_loss: 0.18180 valid_loss: 0.20502 test_loss: 0.14021 \n",
      "[ 10/100] train_loss: 0.17412 valid_loss: 0.19577 test_loss: 0.13717 \n",
      "Validation loss decreased (0.202373 --> 0.195773).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16874 valid_loss: 0.18881 test_loss: 0.13159 \n",
      "Validation loss decreased (0.195773 --> 0.188807).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16510 valid_loss: 0.18343 test_loss: 0.12483 \n",
      "Validation loss decreased (0.188807 --> 0.183428).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16029 valid_loss: 0.17764 test_loss: 0.12664 \n",
      "Validation loss decreased (0.183428 --> 0.177644).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15668 valid_loss: 0.17521 test_loss: 0.12537 \n",
      "Validation loss decreased (0.177644 --> 0.175210).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15412 valid_loss: 0.17016 test_loss: 0.12354 \n",
      "Validation loss decreased (0.175210 --> 0.170158).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15041 valid_loss: 0.16979 test_loss: 0.12716 \n",
      "Validation loss decreased (0.170158 --> 0.169790).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14469 valid_loss: 0.16435 test_loss: 0.11763 \n",
      "Validation loss decreased (0.169790 --> 0.164346).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14610 valid_loss: 0.16458 test_loss: 0.11713 \n",
      "[ 19/100] train_loss: 0.14241 valid_loss: 0.15809 test_loss: 0.11989 \n",
      "Validation loss decreased (0.164346 --> 0.158088).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14052 valid_loss: 0.16482 test_loss: 0.11480 \n",
      "[ 21/100] train_loss: 0.13977 valid_loss: 0.16299 test_loss: 0.12579 \n",
      "[ 22/100] train_loss: 0.13805 valid_loss: 0.17279 test_loss: 0.12597 \n",
      "[ 23/100] train_loss: 0.13151 valid_loss: 0.15258 test_loss: 0.10893 \n",
      "Validation loss decreased (0.158088 --> 0.152582).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13237 valid_loss: 0.14934 test_loss: 0.11569 \n",
      "Validation loss decreased (0.152582 --> 0.149338).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13346 valid_loss: 0.14949 test_loss: 0.11117 \n",
      "[ 26/100] train_loss: 0.13154 valid_loss: 0.14984 test_loss: 0.11158 \n",
      "[ 27/100] train_loss: 0.13108 valid_loss: 0.15189 test_loss: 0.12136 \n",
      "[ 28/100] train_loss: 0.13158 valid_loss: 0.14817 test_loss: 0.10258 \n",
      "Validation loss decreased (0.149338 --> 0.148174).  Saving model ...\n",
      "[ 29/100] train_loss: 0.13154 valid_loss: 0.14219 test_loss: 0.10564 \n",
      "Validation loss decreased (0.148174 --> 0.142189).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12424 valid_loss: 0.14568 test_loss: 0.10329 \n",
      "[ 31/100] train_loss: 0.12561 valid_loss: 0.14118 test_loss: 0.10029 \n",
      "Validation loss decreased (0.142189 --> 0.141177).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12738 valid_loss: 0.14003 test_loss: 0.10280 \n",
      "Validation loss decreased (0.141177 --> 0.140028).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12112 valid_loss: 0.14220 test_loss: 0.10822 \n",
      "[ 34/100] train_loss: 0.12417 valid_loss: 0.13868 test_loss: 0.10260 \n",
      "Validation loss decreased (0.140028 --> 0.138680).  Saving model ...\n",
      "[ 35/100] train_loss: 0.12262 valid_loss: 0.13620 test_loss: 0.10215 \n",
      "Validation loss decreased (0.138680 --> 0.136203).  Saving model ...\n",
      "[ 36/100] train_loss: 0.12280 valid_loss: 0.13637 test_loss: 0.09708 \n",
      "[ 37/100] train_loss: 0.11732 valid_loss: 0.13290 test_loss: 0.10051 \n",
      "Validation loss decreased (0.136203 --> 0.132900).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11748 valid_loss: 0.13732 test_loss: 0.10216 \n",
      "[ 39/100] train_loss: 0.12002 valid_loss: 0.13082 test_loss: 0.09617 \n",
      "Validation loss decreased (0.132900 --> 0.130816).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11588 valid_loss: 0.13272 test_loss: 0.09630 \n",
      "[ 41/100] train_loss: 0.11450 valid_loss: 0.13112 test_loss: 0.09781 \n",
      "[ 42/100] train_loss: 0.11559 valid_loss: 0.12694 test_loss: 0.09394 \n",
      "Validation loss decreased (0.130816 --> 0.126943).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11341 valid_loss: 0.12806 test_loss: 0.09264 \n",
      "[ 44/100] train_loss: 0.11849 valid_loss: 0.12727 test_loss: 0.09765 \n",
      "[ 45/100] train_loss: 0.11054 valid_loss: 0.12779 test_loss: 0.09643 \n",
      "[ 46/100] train_loss: 0.10927 valid_loss: 0.12533 test_loss: 0.09390 \n",
      "Validation loss decreased (0.126943 --> 0.125328).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11100 valid_loss: 0.12529 test_loss: 0.09236 \n",
      "Validation loss decreased (0.125328 --> 0.125287).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11061 valid_loss: 0.12628 test_loss: 0.09354 \n",
      "[ 49/100] train_loss: 0.11032 valid_loss: 0.12138 test_loss: 0.08789 \n",
      "Validation loss decreased (0.125287 --> 0.121376).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10986 valid_loss: 0.12469 test_loss: 0.09145 \n",
      "[ 51/100] train_loss: 0.11024 valid_loss: 0.12776 test_loss: 0.09115 \n",
      "[ 52/100] train_loss: 0.10700 valid_loss: 0.12184 test_loss: 0.09200 \n",
      "[ 53/100] train_loss: 0.10928 valid_loss: 0.12167 test_loss: 0.09382 \n",
      "[ 54/100] train_loss: 0.10608 valid_loss: 0.12233 test_loss: 0.09067 \n",
      "[ 55/100] train_loss: 0.10796 valid_loss: 0.12620 test_loss: 0.09280 \n",
      "[ 56/100] train_loss: 0.10439 valid_loss: 0.11948 test_loss: 0.08765 \n",
      "Validation loss decreased (0.121376 --> 0.119477).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10288 valid_loss: 0.12231 test_loss: 0.08532 \n",
      "[ 58/100] train_loss: 0.10554 valid_loss: 0.12255 test_loss: 0.08959 \n",
      "[ 59/100] train_loss: 0.10404 valid_loss: 0.11623 test_loss: 0.08742 \n",
      "Validation loss decreased (0.119477 --> 0.116225).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10153 valid_loss: 0.12141 test_loss: 0.08495 \n",
      "[ 61/100] train_loss: 0.10195 valid_loss: 0.12063 test_loss: 0.08714 \n",
      "[ 62/100] train_loss: 0.10425 valid_loss: 0.11632 test_loss: 0.08758 \n",
      "[ 63/100] train_loss: 0.10071 valid_loss: 0.11467 test_loss: 0.08481 \n",
      "Validation loss decreased (0.116225 --> 0.114671).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10148 valid_loss: 0.11288 test_loss: 0.08612 \n",
      "Validation loss decreased (0.114671 --> 0.112882).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10101 valid_loss: 0.11688 test_loss: 0.08768 \n",
      "[ 66/100] train_loss: 0.09848 valid_loss: 0.11795 test_loss: 0.08597 \n",
      "[ 67/100] train_loss: 0.10114 valid_loss: 0.11491 test_loss: 0.08247 \n",
      "[ 68/100] train_loss: 0.09989 valid_loss: 0.11462 test_loss: 0.08464 \n",
      "[ 69/100] train_loss: 0.09937 valid_loss: 0.11500 test_loss: 0.08299 \n",
      "[ 70/100] train_loss: 0.10026 valid_loss: 0.10990 test_loss: 0.08381 \n",
      "Validation loss decreased (0.112882 --> 0.109902).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09717 valid_loss: 0.11183 test_loss: 0.08252 \n",
      "[ 72/100] train_loss: 0.09888 valid_loss: 0.11430 test_loss: 0.08262 \n",
      "[ 73/100] train_loss: 0.09868 valid_loss: 0.11019 test_loss: 0.08158 \n",
      "[ 74/100] train_loss: 0.09983 valid_loss: 0.11362 test_loss: 0.08510 \n",
      "[ 75/100] train_loss: 0.09549 valid_loss: 0.10881 test_loss: 0.08206 \n",
      "Validation loss decreased (0.109902 --> 0.108807).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09690 valid_loss: 0.11206 test_loss: 0.08158 \n",
      "[ 77/100] train_loss: 0.09496 valid_loss: 0.10953 test_loss: 0.08179 \n",
      "[ 78/100] train_loss: 0.09545 valid_loss: 0.10968 test_loss: 0.08180 \n",
      "[ 79/100] train_loss: 0.09744 valid_loss: 0.10671 test_loss: 0.08009 \n",
      "Validation loss decreased (0.108807 --> 0.106714).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09542 valid_loss: 0.10822 test_loss: 0.08023 \n",
      "[ 81/100] train_loss: 0.09456 valid_loss: 0.10694 test_loss: 0.08200 \n",
      "[ 82/100] train_loss: 0.09321 valid_loss: 0.10738 test_loss: 0.08142 \n",
      "[ 83/100] train_loss: 0.09337 valid_loss: 0.10650 test_loss: 0.08063 \n",
      "Validation loss decreased (0.106714 --> 0.106495).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09338 valid_loss: 0.10678 test_loss: 0.08296 \n",
      "[ 85/100] train_loss: 0.09506 valid_loss: 0.10784 test_loss: 0.08017 \n",
      "[ 86/100] train_loss: 0.09520 valid_loss: 0.10487 test_loss: 0.08039 \n",
      "Validation loss decreased (0.106495 --> 0.104874).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09099 valid_loss: 0.10663 test_loss: 0.07833 \n",
      "[ 88/100] train_loss: 0.09286 valid_loss: 0.10911 test_loss: 0.08079 \n",
      "[ 89/100] train_loss: 0.09277 valid_loss: 0.10927 test_loss: 0.07831 \n",
      "[ 90/100] train_loss: 0.08987 valid_loss: 0.10565 test_loss: 0.07897 \n",
      "[ 91/100] train_loss: 0.09102 valid_loss: 0.10645 test_loss: 0.08031 \n",
      "[ 92/100] train_loss: 0.09302 valid_loss: 0.10830 test_loss: 0.08028 \n",
      "[ 93/100] train_loss: 0.08904 valid_loss: 0.10577 test_loss: 0.07954 \n",
      "[ 94/100] train_loss: 0.08990 valid_loss: 0.10226 test_loss: 0.07550 \n",
      "Validation loss decreased (0.104874 --> 0.102258).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09372 valid_loss: 0.10302 test_loss: 0.07756 \n",
      "[ 96/100] train_loss: 0.09032 valid_loss: 0.10344 test_loss: 0.07646 \n",
      "[ 97/100] train_loss: 0.08948 valid_loss: 0.10301 test_loss: 0.07593 \n",
      "[ 98/100] train_loss: 0.08933 valid_loss: 0.10256 test_loss: 0.07785 \n",
      "[ 99/100] train_loss: 0.09148 valid_loss: 0.10416 test_loss: 0.07802 \n",
      "[100/100] train_loss: 0.08834 valid_loss: 0.10214 test_loss: 0.07579 \n",
      "Validation loss decreased (0.102258 --> 0.102137).  Saving model ...\n",
      "TRAINING MODEL 7\n",
      "[  1/100] train_loss: 0.60481 valid_loss: 0.50615 test_loss: 0.45293 \n",
      "Validation loss decreased (inf --> 0.506149).  Saving model ...\n",
      "[  2/100] train_loss: 0.42126 valid_loss: 0.37298 test_loss: 0.31291 \n",
      "Validation loss decreased (0.506149 --> 0.372978).  Saving model ...\n",
      "[  3/100] train_loss: 0.32915 valid_loss: 0.31431 test_loss: 0.24807 \n",
      "Validation loss decreased (0.372978 --> 0.314308).  Saving model ...\n",
      "[  4/100] train_loss: 0.27172 valid_loss: 0.27586 test_loss: 0.21349 \n",
      "Validation loss decreased (0.314308 --> 0.275865).  Saving model ...\n",
      "[  5/100] train_loss: 0.23753 valid_loss: 0.24958 test_loss: 0.18506 \n",
      "Validation loss decreased (0.275865 --> 0.249580).  Saving model ...\n",
      "[  6/100] train_loss: 0.22149 valid_loss: 0.22917 test_loss: 0.16171 \n",
      "Validation loss decreased (0.249580 --> 0.229170).  Saving model ...\n",
      "[  7/100] train_loss: 0.20330 valid_loss: 0.21664 test_loss: 0.15507 \n",
      "Validation loss decreased (0.229170 --> 0.216636).  Saving model ...\n",
      "[  8/100] train_loss: 0.19748 valid_loss: 0.20839 test_loss: 0.14377 \n",
      "Validation loss decreased (0.216636 --> 0.208387).  Saving model ...\n",
      "[  9/100] train_loss: 0.18493 valid_loss: 0.19732 test_loss: 0.13789 \n",
      "Validation loss decreased (0.208387 --> 0.197325).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17645 valid_loss: 0.19194 test_loss: 0.13190 \n",
      "Validation loss decreased (0.197325 --> 0.191942).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16972 valid_loss: 0.18752 test_loss: 0.12707 \n",
      "Validation loss decreased (0.191942 --> 0.187517).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16749 valid_loss: 0.17744 test_loss: 0.12237 \n",
      "Validation loss decreased (0.187517 --> 0.177443).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15813 valid_loss: 0.17093 test_loss: 0.11727 \n",
      "Validation loss decreased (0.177443 --> 0.170928).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15296 valid_loss: 0.17353 test_loss: 0.11748 \n",
      "[ 15/100] train_loss: 0.15126 valid_loss: 0.16756 test_loss: 0.11720 \n",
      "Validation loss decreased (0.170928 --> 0.167557).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14593 valid_loss: 0.16143 test_loss: 0.11257 \n",
      "Validation loss decreased (0.167557 --> 0.161428).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14383 valid_loss: 0.16229 test_loss: 0.11242 \n",
      "[ 18/100] train_loss: 0.14335 valid_loss: 0.15615 test_loss: 0.10796 \n",
      "Validation loss decreased (0.161428 --> 0.156147).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13955 valid_loss: 0.15064 test_loss: 0.10716 \n",
      "Validation loss decreased (0.156147 --> 0.150640).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13709 valid_loss: 0.15107 test_loss: 0.10778 \n",
      "[ 21/100] train_loss: 0.13733 valid_loss: 0.15212 test_loss: 0.10799 \n",
      "[ 22/100] train_loss: 0.13366 valid_loss: 0.14786 test_loss: 0.10427 \n",
      "Validation loss decreased (0.150640 --> 0.147863).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13204 valid_loss: 0.14435 test_loss: 0.09951 \n",
      "Validation loss decreased (0.147863 --> 0.144347).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12683 valid_loss: 0.14317 test_loss: 0.10141 \n",
      "Validation loss decreased (0.144347 --> 0.143172).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12485 valid_loss: 0.14402 test_loss: 0.09941 \n",
      "[ 26/100] train_loss: 0.12897 valid_loss: 0.14000 test_loss: 0.09986 \n",
      "Validation loss decreased (0.143172 --> 0.140001).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12413 valid_loss: 0.13967 test_loss: 0.09707 \n",
      "Validation loss decreased (0.140001 --> 0.139670).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12328 valid_loss: 0.14293 test_loss: 0.09928 \n",
      "[ 29/100] train_loss: 0.12120 valid_loss: 0.13756 test_loss: 0.10050 \n",
      "Validation loss decreased (0.139670 --> 0.137558).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12054 valid_loss: 0.13461 test_loss: 0.09545 \n",
      "Validation loss decreased (0.137558 --> 0.134605).  Saving model ...\n",
      "[ 31/100] train_loss: 0.11651 valid_loss: 0.13598 test_loss: 0.09699 \n",
      "[ 32/100] train_loss: 0.12307 valid_loss: 0.13288 test_loss: 0.09298 \n",
      "Validation loss decreased (0.134605 --> 0.132881).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12074 valid_loss: 0.13249 test_loss: 0.09716 \n",
      "Validation loss decreased (0.132881 --> 0.132489).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11490 valid_loss: 0.12981 test_loss: 0.09313 \n",
      "Validation loss decreased (0.132489 --> 0.129811).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11733 valid_loss: 0.12794 test_loss: 0.09380 \n",
      "Validation loss decreased (0.129811 --> 0.127936).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11527 valid_loss: 0.13035 test_loss: 0.09576 \n",
      "[ 37/100] train_loss: 0.11462 valid_loss: 0.12753 test_loss: 0.09675 \n",
      "Validation loss decreased (0.127936 --> 0.127534).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11447 valid_loss: 0.12945 test_loss: 0.09396 \n",
      "[ 39/100] train_loss: 0.11368 valid_loss: 0.13357 test_loss: 0.09486 \n",
      "[ 40/100] train_loss: 0.11445 valid_loss: 0.12389 test_loss: 0.09026 \n",
      "Validation loss decreased (0.127534 --> 0.123887).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11365 valid_loss: 0.12291 test_loss: 0.08770 \n",
      "Validation loss decreased (0.123887 --> 0.122908).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11321 valid_loss: 0.12769 test_loss: 0.08995 \n",
      "[ 43/100] train_loss: 0.11108 valid_loss: 0.12495 test_loss: 0.09212 \n",
      "[ 44/100] train_loss: 0.11130 valid_loss: 0.12759 test_loss: 0.09148 \n",
      "[ 45/100] train_loss: 0.11016 valid_loss: 0.12239 test_loss: 0.09365 \n",
      "Validation loss decreased (0.122908 --> 0.122392).  Saving model ...\n",
      "[ 46/100] train_loss: 0.10771 valid_loss: 0.12300 test_loss: 0.09299 \n",
      "[ 47/100] train_loss: 0.10749 valid_loss: 0.12226 test_loss: 0.09222 \n",
      "Validation loss decreased (0.122392 --> 0.122263).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10697 valid_loss: 0.12122 test_loss: 0.09191 \n",
      "Validation loss decreased (0.122263 --> 0.121218).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10826 valid_loss: 0.11875 test_loss: 0.09049 \n",
      "Validation loss decreased (0.121218 --> 0.118753).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10485 valid_loss: 0.11882 test_loss: 0.08902 \n",
      "[ 51/100] train_loss: 0.10692 valid_loss: 0.11926 test_loss: 0.08826 \n",
      "[ 52/100] train_loss: 0.10367 valid_loss: 0.11967 test_loss: 0.08979 \n",
      "[ 53/100] train_loss: 0.10545 valid_loss: 0.11880 test_loss: 0.08829 \n",
      "[ 54/100] train_loss: 0.10330 valid_loss: 0.12049 test_loss: 0.09231 \n",
      "[ 55/100] train_loss: 0.10595 valid_loss: 0.11552 test_loss: 0.09108 \n",
      "Validation loss decreased (0.118753 --> 0.115518).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10294 valid_loss: 0.11620 test_loss: 0.09189 \n",
      "[ 57/100] train_loss: 0.10357 valid_loss: 0.11718 test_loss: 0.08983 \n",
      "[ 58/100] train_loss: 0.10429 valid_loss: 0.11843 test_loss: 0.08918 \n",
      "[ 59/100] train_loss: 0.10248 valid_loss: 0.11465 test_loss: 0.08508 \n",
      "Validation loss decreased (0.115518 --> 0.114649).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10017 valid_loss: 0.11488 test_loss: 0.08580 \n",
      "[ 61/100] train_loss: 0.10331 valid_loss: 0.11306 test_loss: 0.08531 \n",
      "Validation loss decreased (0.114649 --> 0.113055).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10345 valid_loss: 0.11321 test_loss: 0.08539 \n",
      "[ 63/100] train_loss: 0.10205 valid_loss: 0.11445 test_loss: 0.08306 \n",
      "[ 64/100] train_loss: 0.10202 valid_loss: 0.11297 test_loss: 0.08540 \n",
      "Validation loss decreased (0.113055 --> 0.112969).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10081 valid_loss: 0.11341 test_loss: 0.08330 \n",
      "[ 66/100] train_loss: 0.10309 valid_loss: 0.11410 test_loss: 0.08355 \n",
      "[ 67/100] train_loss: 0.09895 valid_loss: 0.11225 test_loss: 0.08369 \n",
      "Validation loss decreased (0.112969 --> 0.112250).  Saving model ...\n",
      "[ 68/100] train_loss: 0.09970 valid_loss: 0.11022 test_loss: 0.08202 \n",
      "Validation loss decreased (0.112250 --> 0.110217).  Saving model ...\n",
      "[ 69/100] train_loss: 0.09742 valid_loss: 0.11335 test_loss: 0.08520 \n",
      "[ 70/100] train_loss: 0.09795 valid_loss: 0.11013 test_loss: 0.08266 \n",
      "Validation loss decreased (0.110217 --> 0.110133).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09609 valid_loss: 0.11080 test_loss: 0.08532 \n",
      "[ 72/100] train_loss: 0.09675 valid_loss: 0.11028 test_loss: 0.08109 \n",
      "[ 73/100] train_loss: 0.09615 valid_loss: 0.11025 test_loss: 0.08213 \n",
      "[ 74/100] train_loss: 0.09520 valid_loss: 0.10799 test_loss: 0.08058 \n",
      "Validation loss decreased (0.110133 --> 0.107988).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09379 valid_loss: 0.10965 test_loss: 0.07903 \n",
      "[ 76/100] train_loss: 0.09333 valid_loss: 0.10666 test_loss: 0.08043 \n",
      "Validation loss decreased (0.107988 --> 0.106662).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09540 valid_loss: 0.10883 test_loss: 0.07981 \n",
      "[ 78/100] train_loss: 0.09395 valid_loss: 0.10830 test_loss: 0.08073 \n",
      "[ 79/100] train_loss: 0.09305 valid_loss: 0.10810 test_loss: 0.08082 \n",
      "[ 80/100] train_loss: 0.09576 valid_loss: 0.10711 test_loss: 0.07972 \n",
      "[ 81/100] train_loss: 0.09471 valid_loss: 0.10794 test_loss: 0.07859 \n",
      "[ 82/100] train_loss: 0.09284 valid_loss: 0.10816 test_loss: 0.08281 \n",
      "[ 83/100] train_loss: 0.09439 valid_loss: 0.10658 test_loss: 0.07989 \n",
      "Validation loss decreased (0.106662 --> 0.106585).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09494 valid_loss: 0.10645 test_loss: 0.07998 \n",
      "Validation loss decreased (0.106585 --> 0.106446).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09252 valid_loss: 0.10548 test_loss: 0.08181 \n",
      "Validation loss decreased (0.106446 --> 0.105476).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09072 valid_loss: 0.10481 test_loss: 0.07919 \n",
      "Validation loss decreased (0.105476 --> 0.104815).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09349 valid_loss: 0.10510 test_loss: 0.07754 \n",
      "[ 88/100] train_loss: 0.09033 valid_loss: 0.10420 test_loss: 0.07991 \n",
      "Validation loss decreased (0.104815 --> 0.104203).  Saving model ...\n",
      "[ 89/100] train_loss: 0.08839 valid_loss: 0.10781 test_loss: 0.07874 \n",
      "[ 90/100] train_loss: 0.09182 valid_loss: 0.10360 test_loss: 0.07752 \n",
      "Validation loss decreased (0.104203 --> 0.103600).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09228 valid_loss: 0.10345 test_loss: 0.07848 \n",
      "Validation loss decreased (0.103600 --> 0.103452).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09163 valid_loss: 0.10492 test_loss: 0.07969 \n",
      "[ 93/100] train_loss: 0.08943 valid_loss: 0.10504 test_loss: 0.07900 \n",
      "[ 94/100] train_loss: 0.08934 valid_loss: 0.10467 test_loss: 0.07808 \n",
      "[ 95/100] train_loss: 0.08987 valid_loss: 0.10178 test_loss: 0.07748 \n",
      "Validation loss decreased (0.103452 --> 0.101775).  Saving model ...\n",
      "[ 96/100] train_loss: 0.08732 valid_loss: 0.10435 test_loss: 0.07921 \n",
      "[ 97/100] train_loss: 0.08927 valid_loss: 0.10210 test_loss: 0.07786 \n",
      "[ 98/100] train_loss: 0.08862 valid_loss: 0.10435 test_loss: 0.07901 \n",
      "[ 99/100] train_loss: 0.08708 valid_loss: 0.10212 test_loss: 0.07705 \n",
      "[100/100] train_loss: 0.08598 valid_loss: 0.10258 test_loss: 0.07608 \n",
      "TRAINING MODEL 8\n",
      "[  1/100] train_loss: 0.57950 valid_loss: 0.47718 test_loss: 0.44306 \n",
      "Validation loss decreased (inf --> 0.477178).  Saving model ...\n",
      "[  2/100] train_loss: 0.39450 valid_loss: 0.35866 test_loss: 0.29907 \n",
      "Validation loss decreased (0.477178 --> 0.358665).  Saving model ...\n",
      "[  3/100] train_loss: 0.30385 valid_loss: 0.30541 test_loss: 0.22870 \n",
      "Validation loss decreased (0.358665 --> 0.305406).  Saving model ...\n",
      "[  4/100] train_loss: 0.25841 valid_loss: 0.27271 test_loss: 0.19181 \n",
      "Validation loss decreased (0.305406 --> 0.272715).  Saving model ...\n",
      "[  5/100] train_loss: 0.23321 valid_loss: 0.24687 test_loss: 0.17225 \n",
      "Validation loss decreased (0.272715 --> 0.246866).  Saving model ...\n",
      "[  6/100] train_loss: 0.21337 valid_loss: 0.22558 test_loss: 0.15278 \n",
      "Validation loss decreased (0.246866 --> 0.225579).  Saving model ...\n",
      "[  7/100] train_loss: 0.20441 valid_loss: 0.21289 test_loss: 0.14880 \n",
      "Validation loss decreased (0.225579 --> 0.212892).  Saving model ...\n",
      "[  8/100] train_loss: 0.19136 valid_loss: 0.20740 test_loss: 0.14015 \n",
      "Validation loss decreased (0.212892 --> 0.207399).  Saving model ...\n",
      "[  9/100] train_loss: 0.18239 valid_loss: 0.20276 test_loss: 0.13641 \n",
      "Validation loss decreased (0.207399 --> 0.202757).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18012 valid_loss: 0.20136 test_loss: 0.13397 \n",
      "Validation loss decreased (0.202757 --> 0.201364).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17451 valid_loss: 0.19150 test_loss: 0.12891 \n",
      "Validation loss decreased (0.201364 --> 0.191499).  Saving model ...\n",
      "[ 12/100] train_loss: 0.17058 valid_loss: 0.18877 test_loss: 0.12497 \n",
      "Validation loss decreased (0.191499 --> 0.188769).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16154 valid_loss: 0.18466 test_loss: 0.12514 \n",
      "Validation loss decreased (0.188769 --> 0.184658).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15914 valid_loss: 0.18282 test_loss: 0.12228 \n",
      "Validation loss decreased (0.184658 --> 0.182819).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15696 valid_loss: 0.17400 test_loss: 0.12041 \n",
      "Validation loss decreased (0.182819 --> 0.173999).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14890 valid_loss: 0.16993 test_loss: 0.12166 \n",
      "Validation loss decreased (0.173999 --> 0.169932).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14954 valid_loss: 0.17150 test_loss: 0.11817 \n",
      "[ 18/100] train_loss: 0.14227 valid_loss: 0.16413 test_loss: 0.12121 \n",
      "Validation loss decreased (0.169932 --> 0.164125).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14183 valid_loss: 0.16991 test_loss: 0.13412 \n",
      "[ 20/100] train_loss: 0.14120 valid_loss: 0.16239 test_loss: 0.12426 \n",
      "Validation loss decreased (0.164125 --> 0.162392).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13801 valid_loss: 0.15627 test_loss: 0.12080 \n",
      "Validation loss decreased (0.162392 --> 0.156265).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13413 valid_loss: 0.15586 test_loss: 0.11429 \n",
      "Validation loss decreased (0.156265 --> 0.155861).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13705 valid_loss: 0.16022 test_loss: 0.12683 \n",
      "[ 24/100] train_loss: 0.13264 valid_loss: 0.16058 test_loss: 0.13082 \n",
      "[ 25/100] train_loss: 0.13403 valid_loss: 0.16027 test_loss: 0.13546 \n",
      "[ 26/100] train_loss: 0.13258 valid_loss: 0.17247 test_loss: 0.15117 \n",
      "[ 27/100] train_loss: 0.13080 valid_loss: 0.15936 test_loss: 0.13162 \n",
      "[ 28/100] train_loss: 0.12975 valid_loss: 0.15147 test_loss: 0.12786 \n",
      "Validation loss decreased (0.155861 --> 0.151471).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12807 valid_loss: 0.15672 test_loss: 0.12395 \n",
      "[ 30/100] train_loss: 0.12467 valid_loss: 0.14800 test_loss: 0.12168 \n",
      "Validation loss decreased (0.151471 --> 0.147998).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12340 valid_loss: 0.14391 test_loss: 0.11706 \n",
      "Validation loss decreased (0.147998 --> 0.143908).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12642 valid_loss: 0.14762 test_loss: 0.12396 \n",
      "[ 33/100] train_loss: 0.12292 valid_loss: 0.14253 test_loss: 0.11203 \n",
      "Validation loss decreased (0.143908 --> 0.142527).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12120 valid_loss: 0.15036 test_loss: 0.12161 \n",
      "[ 35/100] train_loss: 0.12199 valid_loss: 0.14771 test_loss: 0.11805 \n",
      "[ 36/100] train_loss: 0.11969 valid_loss: 0.14903 test_loss: 0.11956 \n",
      "[ 37/100] train_loss: 0.11869 valid_loss: 0.14453 test_loss: 0.11920 \n",
      "[ 38/100] train_loss: 0.11633 valid_loss: 0.13916 test_loss: 0.10860 \n",
      "Validation loss decreased (0.142527 --> 0.139161).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11751 valid_loss: 0.14163 test_loss: 0.11588 \n",
      "[ 40/100] train_loss: 0.11576 valid_loss: 0.13484 test_loss: 0.10499 \n",
      "Validation loss decreased (0.139161 --> 0.134843).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11726 valid_loss: 0.13452 test_loss: 0.10798 \n",
      "Validation loss decreased (0.134843 --> 0.134520).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11506 valid_loss: 0.13540 test_loss: 0.10655 \n",
      "[ 43/100] train_loss: 0.11517 valid_loss: 0.13508 test_loss: 0.10928 \n",
      "[ 44/100] train_loss: 0.11393 valid_loss: 0.13387 test_loss: 0.10566 \n",
      "Validation loss decreased (0.134520 --> 0.133872).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11295 valid_loss: 0.13909 test_loss: 0.11550 \n",
      "[ 46/100] train_loss: 0.11169 valid_loss: 0.12972 test_loss: 0.10101 \n",
      "Validation loss decreased (0.133872 --> 0.129722).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11337 valid_loss: 0.13059 test_loss: 0.09933 \n",
      "[ 48/100] train_loss: 0.11024 valid_loss: 0.13338 test_loss: 0.10881 \n",
      "[ 49/100] train_loss: 0.10760 valid_loss: 0.13134 test_loss: 0.10390 \n",
      "[ 50/100] train_loss: 0.10981 valid_loss: 0.13086 test_loss: 0.10523 \n",
      "[ 51/100] train_loss: 0.10489 valid_loss: 0.12927 test_loss: 0.10236 \n",
      "Validation loss decreased (0.129722 --> 0.129272).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10629 valid_loss: 0.12719 test_loss: 0.10136 \n",
      "Validation loss decreased (0.129272 --> 0.127193).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10623 valid_loss: 0.12601 test_loss: 0.09894 \n",
      "Validation loss decreased (0.127193 --> 0.126014).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10670 valid_loss: 0.12315 test_loss: 0.09666 \n",
      "Validation loss decreased (0.126014 --> 0.123152).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10671 valid_loss: 0.12362 test_loss: 0.09486 \n",
      "[ 56/100] train_loss: 0.10621 valid_loss: 0.12293 test_loss: 0.09525 \n",
      "Validation loss decreased (0.123152 --> 0.122931).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10531 valid_loss: 0.12629 test_loss: 0.10073 \n",
      "[ 58/100] train_loss: 0.10407 valid_loss: 0.12244 test_loss: 0.09675 \n",
      "Validation loss decreased (0.122931 --> 0.122442).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10125 valid_loss: 0.12084 test_loss: 0.09545 \n",
      "Validation loss decreased (0.122442 --> 0.120843).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10251 valid_loss: 0.12227 test_loss: 0.09351 \n",
      "[ 61/100] train_loss: 0.10355 valid_loss: 0.12037 test_loss: 0.09198 \n",
      "Validation loss decreased (0.120843 --> 0.120373).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10450 valid_loss: 0.11801 test_loss: 0.09028 \n",
      "Validation loss decreased (0.120373 --> 0.118010).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10240 valid_loss: 0.12018 test_loss: 0.09470 \n",
      "[ 64/100] train_loss: 0.10068 valid_loss: 0.12090 test_loss: 0.09813 \n",
      "[ 65/100] train_loss: 0.10209 valid_loss: 0.11661 test_loss: 0.09171 \n",
      "Validation loss decreased (0.118010 --> 0.116610).  Saving model ...\n",
      "[ 66/100] train_loss: 0.09764 valid_loss: 0.11766 test_loss: 0.09338 \n",
      "[ 67/100] train_loss: 0.09829 valid_loss: 0.11752 test_loss: 0.08998 \n",
      "[ 68/100] train_loss: 0.10229 valid_loss: 0.11458 test_loss: 0.08965 \n",
      "Validation loss decreased (0.116610 --> 0.114579).  Saving model ...\n",
      "[ 69/100] train_loss: 0.09973 valid_loss: 0.11705 test_loss: 0.09079 \n",
      "[ 70/100] train_loss: 0.09844 valid_loss: 0.11683 test_loss: 0.09007 \n",
      "[ 71/100] train_loss: 0.09915 valid_loss: 0.11455 test_loss: 0.08721 \n",
      "Validation loss decreased (0.114579 --> 0.114548).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09577 valid_loss: 0.11631 test_loss: 0.09391 \n",
      "[ 73/100] train_loss: 0.09742 valid_loss: 0.11581 test_loss: 0.09033 \n",
      "[ 74/100] train_loss: 0.09638 valid_loss: 0.11486 test_loss: 0.09022 \n",
      "[ 75/100] train_loss: 0.10022 valid_loss: 0.11820 test_loss: 0.09233 \n",
      "[ 76/100] train_loss: 0.09585 valid_loss: 0.11327 test_loss: 0.08910 \n",
      "Validation loss decreased (0.114548 --> 0.113266).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09643 valid_loss: 0.11646 test_loss: 0.09197 \n",
      "[ 78/100] train_loss: 0.09900 valid_loss: 0.11267 test_loss: 0.08715 \n",
      "Validation loss decreased (0.113266 --> 0.112674).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09628 valid_loss: 0.11306 test_loss: 0.08892 \n",
      "[ 80/100] train_loss: 0.09720 valid_loss: 0.11087 test_loss: 0.08466 \n",
      "Validation loss decreased (0.112674 --> 0.110865).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09561 valid_loss: 0.11036 test_loss: 0.08860 \n",
      "Validation loss decreased (0.110865 --> 0.110359).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09842 valid_loss: 0.10960 test_loss: 0.08604 \n",
      "Validation loss decreased (0.110359 --> 0.109602).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09630 valid_loss: 0.11073 test_loss: 0.08715 \n",
      "[ 84/100] train_loss: 0.09587 valid_loss: 0.11186 test_loss: 0.08751 \n",
      "[ 85/100] train_loss: 0.09289 valid_loss: 0.11158 test_loss: 0.08878 \n",
      "[ 86/100] train_loss: 0.09383 valid_loss: 0.11061 test_loss: 0.08816 \n",
      "[ 87/100] train_loss: 0.09712 valid_loss: 0.11157 test_loss: 0.08984 \n",
      "[ 88/100] train_loss: 0.09367 valid_loss: 0.11058 test_loss: 0.08880 \n",
      "[ 89/100] train_loss: 0.09162 valid_loss: 0.10952 test_loss: 0.08835 \n",
      "Validation loss decreased (0.109602 --> 0.109516).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09365 valid_loss: 0.11483 test_loss: 0.09094 \n",
      "[ 91/100] train_loss: 0.09145 valid_loss: 0.10999 test_loss: 0.08547 \n",
      "[ 92/100] train_loss: 0.09411 valid_loss: 0.11094 test_loss: 0.08993 \n",
      "[ 93/100] train_loss: 0.09188 valid_loss: 0.10579 test_loss: 0.08515 \n",
      "Validation loss decreased (0.109516 --> 0.105785).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09081 valid_loss: 0.10933 test_loss: 0.08683 \n",
      "[ 95/100] train_loss: 0.09030 valid_loss: 0.10854 test_loss: 0.08522 \n",
      "[ 96/100] train_loss: 0.08987 valid_loss: 0.10589 test_loss: 0.08559 \n",
      "[ 97/100] train_loss: 0.09192 valid_loss: 0.10966 test_loss: 0.08696 \n",
      "[ 98/100] train_loss: 0.09138 valid_loss: 0.10698 test_loss: 0.08825 \n",
      "[ 99/100] train_loss: 0.09352 valid_loss: 0.10919 test_loss: 0.08652 \n",
      "[100/100] train_loss: 0.08987 valid_loss: 0.10688 test_loss: 0.08687 \n",
      "TRAINING MODEL 9\n",
      "[  1/100] train_loss: 0.60023 valid_loss: 0.49100 test_loss: 0.49497 \n",
      "Validation loss decreased (inf --> 0.491000).  Saving model ...\n",
      "[  2/100] train_loss: 0.43629 valid_loss: 0.38488 test_loss: 0.33051 \n",
      "Validation loss decreased (0.491000 --> 0.384883).  Saving model ...\n",
      "[  3/100] train_loss: 0.33451 valid_loss: 0.32075 test_loss: 0.25381 \n",
      "Validation loss decreased (0.384883 --> 0.320753).  Saving model ...\n",
      "[  4/100] train_loss: 0.27468 valid_loss: 0.27564 test_loss: 0.20799 \n",
      "Validation loss decreased (0.320753 --> 0.275636).  Saving model ...\n",
      "[  5/100] train_loss: 0.23905 valid_loss: 0.24787 test_loss: 0.18203 \n",
      "Validation loss decreased (0.275636 --> 0.247870).  Saving model ...\n",
      "[  6/100] train_loss: 0.21725 valid_loss: 0.23007 test_loss: 0.16207 \n",
      "Validation loss decreased (0.247870 --> 0.230074).  Saving model ...\n",
      "[  7/100] train_loss: 0.20267 valid_loss: 0.21699 test_loss: 0.15136 \n",
      "Validation loss decreased (0.230074 --> 0.216988).  Saving model ...\n",
      "[  8/100] train_loss: 0.19189 valid_loss: 0.20553 test_loss: 0.14538 \n",
      "Validation loss decreased (0.216988 --> 0.205529).  Saving model ...\n",
      "[  9/100] train_loss: 0.18451 valid_loss: 0.20115 test_loss: 0.14162 \n",
      "Validation loss decreased (0.205529 --> 0.201149).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17692 valid_loss: 0.19245 test_loss: 0.13502 \n",
      "Validation loss decreased (0.201149 --> 0.192447).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17093 valid_loss: 0.19105 test_loss: 0.13200 \n",
      "Validation loss decreased (0.192447 --> 0.191052).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16827 valid_loss: 0.18429 test_loss: 0.13346 \n",
      "Validation loss decreased (0.191052 --> 0.184289).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15958 valid_loss: 0.17663 test_loss: 0.12693 \n",
      "Validation loss decreased (0.184289 --> 0.176627).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15284 valid_loss: 0.17173 test_loss: 0.12341 \n",
      "Validation loss decreased (0.176627 --> 0.171733).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15018 valid_loss: 0.16520 test_loss: 0.11990 \n",
      "Validation loss decreased (0.171733 --> 0.165203).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15025 valid_loss: 0.16192 test_loss: 0.11805 \n",
      "Validation loss decreased (0.165203 --> 0.161917).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14760 valid_loss: 0.15752 test_loss: 0.11784 \n",
      "Validation loss decreased (0.161917 --> 0.157519).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14046 valid_loss: 0.15463 test_loss: 0.11361 \n",
      "Validation loss decreased (0.157519 --> 0.154626).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13900 valid_loss: 0.15973 test_loss: 0.11602 \n",
      "[ 20/100] train_loss: 0.13957 valid_loss: 0.15353 test_loss: 0.11397 \n",
      "Validation loss decreased (0.154626 --> 0.153533).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13694 valid_loss: 0.15169 test_loss: 0.11196 \n",
      "Validation loss decreased (0.153533 --> 0.151686).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13434 valid_loss: 0.14931 test_loss: 0.10981 \n",
      "Validation loss decreased (0.151686 --> 0.149314).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13253 valid_loss: 0.14656 test_loss: 0.10581 \n",
      "Validation loss decreased (0.149314 --> 0.146563).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13166 valid_loss: 0.15303 test_loss: 0.11104 \n",
      "[ 25/100] train_loss: 0.12596 valid_loss: 0.14502 test_loss: 0.10814 \n",
      "Validation loss decreased (0.146563 --> 0.145022).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12522 valid_loss: 0.14262 test_loss: 0.10742 \n",
      "Validation loss decreased (0.145022 --> 0.142617).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12930 valid_loss: 0.14129 test_loss: 0.10639 \n",
      "Validation loss decreased (0.142617 --> 0.141292).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12619 valid_loss: 0.13935 test_loss: 0.10148 \n",
      "Validation loss decreased (0.141292 --> 0.139351).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12702 valid_loss: 0.13971 test_loss: 0.09841 \n",
      "[ 30/100] train_loss: 0.12606 valid_loss: 0.13834 test_loss: 0.10237 \n",
      "Validation loss decreased (0.139351 --> 0.138337).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12209 valid_loss: 0.13697 test_loss: 0.10288 \n",
      "Validation loss decreased (0.138337 --> 0.136974).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12272 valid_loss: 0.14418 test_loss: 0.10927 \n",
      "[ 33/100] train_loss: 0.12206 valid_loss: 0.13845 test_loss: 0.10312 \n",
      "[ 34/100] train_loss: 0.12172 valid_loss: 0.13442 test_loss: 0.10118 \n",
      "Validation loss decreased (0.136974 --> 0.134421).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11622 valid_loss: 0.14067 test_loss: 0.10874 \n",
      "[ 36/100] train_loss: 0.11715 valid_loss: 0.13612 test_loss: 0.10218 \n",
      "[ 37/100] train_loss: 0.11420 valid_loss: 0.13203 test_loss: 0.10050 \n",
      "Validation loss decreased (0.134421 --> 0.132035).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11487 valid_loss: 0.13465 test_loss: 0.09952 \n",
      "[ 39/100] train_loss: 0.11502 valid_loss: 0.13125 test_loss: 0.10636 \n",
      "Validation loss decreased (0.132035 --> 0.131247).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11433 valid_loss: 0.12994 test_loss: 0.09794 \n",
      "Validation loss decreased (0.131247 --> 0.129941).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11570 valid_loss: 0.12977 test_loss: 0.09675 \n",
      "Validation loss decreased (0.129941 --> 0.129773).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11222 valid_loss: 0.12679 test_loss: 0.09607 \n",
      "Validation loss decreased (0.129773 --> 0.126789).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11429 valid_loss: 0.12830 test_loss: 0.09620 \n",
      "[ 44/100] train_loss: 0.10977 valid_loss: 0.12739 test_loss: 0.09595 \n",
      "[ 45/100] train_loss: 0.11116 valid_loss: 0.12790 test_loss: 0.09386 \n",
      "[ 46/100] train_loss: 0.11048 valid_loss: 0.13023 test_loss: 0.09743 \n",
      "[ 47/100] train_loss: 0.10819 valid_loss: 0.12486 test_loss: 0.09794 \n",
      "Validation loss decreased (0.126789 --> 0.124864).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10705 valid_loss: 0.12393 test_loss: 0.09327 \n",
      "Validation loss decreased (0.124864 --> 0.123929).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10856 valid_loss: 0.12110 test_loss: 0.09252 \n",
      "Validation loss decreased (0.123929 --> 0.121105).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10688 valid_loss: 0.12819 test_loss: 0.09550 \n",
      "[ 51/100] train_loss: 0.10472 valid_loss: 0.12432 test_loss: 0.09173 \n",
      "[ 52/100] train_loss: 0.11060 valid_loss: 0.12351 test_loss: 0.09399 \n",
      "[ 53/100] train_loss: 0.10872 valid_loss: 0.12171 test_loss: 0.09053 \n",
      "[ 54/100] train_loss: 0.10665 valid_loss: 0.12244 test_loss: 0.09012 \n",
      "[ 55/100] train_loss: 0.10357 valid_loss: 0.12060 test_loss: 0.08852 \n",
      "Validation loss decreased (0.121105 --> 0.120596).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10519 valid_loss: 0.12052 test_loss: 0.08929 \n",
      "Validation loss decreased (0.120596 --> 0.120518).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10500 valid_loss: 0.11778 test_loss: 0.09054 \n",
      "Validation loss decreased (0.120518 --> 0.117783).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10321 valid_loss: 0.12149 test_loss: 0.08912 \n",
      "[ 59/100] train_loss: 0.10145 valid_loss: 0.12028 test_loss: 0.08933 \n",
      "[ 60/100] train_loss: 0.10604 valid_loss: 0.11953 test_loss: 0.09029 \n",
      "[ 61/100] train_loss: 0.10365 valid_loss: 0.11682 test_loss: 0.08991 \n",
      "Validation loss decreased (0.117783 --> 0.116820).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10282 valid_loss: 0.12056 test_loss: 0.09332 \n",
      "[ 63/100] train_loss: 0.10482 valid_loss: 0.11433 test_loss: 0.08730 \n",
      "Validation loss decreased (0.116820 --> 0.114329).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10191 valid_loss: 0.11484 test_loss: 0.08830 \n",
      "[ 65/100] train_loss: 0.09953 valid_loss: 0.11726 test_loss: 0.08961 \n",
      "[ 66/100] train_loss: 0.10013 valid_loss: 0.12249 test_loss: 0.08932 \n",
      "[ 67/100] train_loss: 0.10129 valid_loss: 0.11703 test_loss: 0.08653 \n",
      "[ 68/100] train_loss: 0.09976 valid_loss: 0.11667 test_loss: 0.08575 \n",
      "[ 69/100] train_loss: 0.09884 valid_loss: 0.11344 test_loss: 0.08637 \n",
      "Validation loss decreased (0.114329 --> 0.113443).  Saving model ...\n",
      "[ 70/100] train_loss: 0.09897 valid_loss: 0.11296 test_loss: 0.08493 \n",
      "Validation loss decreased (0.113443 --> 0.112965).  Saving model ...\n",
      "[ 71/100] train_loss: 0.10101 valid_loss: 0.12000 test_loss: 0.08757 \n",
      "[ 72/100] train_loss: 0.09837 valid_loss: 0.11212 test_loss: 0.08461 \n",
      "Validation loss decreased (0.112965 --> 0.112118).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09783 valid_loss: 0.11300 test_loss: 0.08684 \n",
      "[ 74/100] train_loss: 0.09502 valid_loss: 0.11350 test_loss: 0.08387 \n",
      "[ 75/100] train_loss: 0.09816 valid_loss: 0.11173 test_loss: 0.08392 \n",
      "Validation loss decreased (0.112118 --> 0.111731).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09596 valid_loss: 0.11078 test_loss: 0.08114 \n",
      "Validation loss decreased (0.111731 --> 0.110776).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09686 valid_loss: 0.11334 test_loss: 0.08335 \n",
      "[ 78/100] train_loss: 0.09752 valid_loss: 0.11215 test_loss: 0.08236 \n",
      "[ 79/100] train_loss: 0.09248 valid_loss: 0.11014 test_loss: 0.08300 \n",
      "Validation loss decreased (0.110776 --> 0.110138).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09430 valid_loss: 0.10778 test_loss: 0.08033 \n",
      "Validation loss decreased (0.110138 --> 0.107779).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09575 valid_loss: 0.10785 test_loss: 0.08025 \n",
      "[ 82/100] train_loss: 0.09403 valid_loss: 0.10812 test_loss: 0.08200 \n",
      "[ 83/100] train_loss: 0.09606 valid_loss: 0.11028 test_loss: 0.08447 \n",
      "[ 84/100] train_loss: 0.09312 valid_loss: 0.10806 test_loss: 0.08030 \n",
      "[ 85/100] train_loss: 0.09084 valid_loss: 0.11022 test_loss: 0.08092 \n",
      "[ 86/100] train_loss: 0.09527 valid_loss: 0.11461 test_loss: 0.08246 \n",
      "[ 87/100] train_loss: 0.09455 valid_loss: 0.10796 test_loss: 0.08258 \n",
      "[ 88/100] train_loss: 0.09039 valid_loss: 0.10742 test_loss: 0.07971 \n",
      "Validation loss decreased (0.107779 --> 0.107421).  Saving model ...\n",
      "[ 89/100] train_loss: 0.08911 valid_loss: 0.10553 test_loss: 0.07913 \n",
      "Validation loss decreased (0.107421 --> 0.105526).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09411 valid_loss: 0.10633 test_loss: 0.08072 \n",
      "[ 91/100] train_loss: 0.08961 valid_loss: 0.11054 test_loss: 0.08222 \n",
      "[ 92/100] train_loss: 0.09245 valid_loss: 0.10710 test_loss: 0.08017 \n",
      "[ 93/100] train_loss: 0.09069 valid_loss: 0.10544 test_loss: 0.07794 \n",
      "Validation loss decreased (0.105526 --> 0.105440).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09405 valid_loss: 0.10595 test_loss: 0.08005 \n",
      "[ 95/100] train_loss: 0.09115 valid_loss: 0.10733 test_loss: 0.07946 \n",
      "[ 96/100] train_loss: 0.08912 valid_loss: 0.10366 test_loss: 0.07839 \n",
      "Validation loss decreased (0.105440 --> 0.103664).  Saving model ...\n",
      "[ 97/100] train_loss: 0.08828 valid_loss: 0.10454 test_loss: 0.07936 \n",
      "[ 98/100] train_loss: 0.08892 valid_loss: 0.10611 test_loss: 0.07895 \n",
      "[ 99/100] train_loss: 0.08913 valid_loss: 0.10625 test_loss: 0.08151 \n",
      "[100/100] train_loss: 0.08803 valid_loss: 0.10406 test_loss: 0.08115 \n",
      "TRAINING MODEL 10\n",
      "[  1/100] train_loss: 0.65893 valid_loss: 0.57020 test_loss: 0.54658 \n",
      "Validation loss decreased (inf --> 0.570202).  Saving model ...\n",
      "[  2/100] train_loss: 0.47167 valid_loss: 0.41020 test_loss: 0.35518 \n",
      "Validation loss decreased (0.570202 --> 0.410199).  Saving model ...\n",
      "[  3/100] train_loss: 0.35288 valid_loss: 0.34077 test_loss: 0.27760 \n",
      "Validation loss decreased (0.410199 --> 0.340773).  Saving model ...\n",
      "[  4/100] train_loss: 0.29187 valid_loss: 0.29780 test_loss: 0.22759 \n",
      "Validation loss decreased (0.340773 --> 0.297801).  Saving model ...\n",
      "[  5/100] train_loss: 0.25020 valid_loss: 0.26546 test_loss: 0.19690 \n",
      "Validation loss decreased (0.297801 --> 0.265457).  Saving model ...\n",
      "[  6/100] train_loss: 0.22776 valid_loss: 0.24342 test_loss: 0.17453 \n",
      "Validation loss decreased (0.265457 --> 0.243423).  Saving model ...\n",
      "[  7/100] train_loss: 0.21399 valid_loss: 0.23707 test_loss: 0.16543 \n",
      "Validation loss decreased (0.243423 --> 0.237074).  Saving model ...\n",
      "[  8/100] train_loss: 0.19962 valid_loss: 0.21435 test_loss: 0.15386 \n",
      "Validation loss decreased (0.237074 --> 0.214353).  Saving model ...\n",
      "[  9/100] train_loss: 0.19234 valid_loss: 0.20565 test_loss: 0.14196 \n",
      "Validation loss decreased (0.214353 --> 0.205651).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18427 valid_loss: 0.19496 test_loss: 0.13852 \n",
      "Validation loss decreased (0.205651 --> 0.194964).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17500 valid_loss: 0.19002 test_loss: 0.13473 \n",
      "Validation loss decreased (0.194964 --> 0.190023).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16648 valid_loss: 0.18339 test_loss: 0.13189 \n",
      "Validation loss decreased (0.190023 --> 0.183389).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16341 valid_loss: 0.17906 test_loss: 0.12831 \n",
      "Validation loss decreased (0.183389 --> 0.179065).  Saving model ...\n",
      "[ 14/100] train_loss: 0.16038 valid_loss: 0.17366 test_loss: 0.12488 \n",
      "Validation loss decreased (0.179065 --> 0.173659).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15382 valid_loss: 0.16580 test_loss: 0.11870 \n",
      "Validation loss decreased (0.173659 --> 0.165800).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14953 valid_loss: 0.16239 test_loss: 0.11902 \n",
      "Validation loss decreased (0.165800 --> 0.162385).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14891 valid_loss: 0.16563 test_loss: 0.11504 \n",
      "[ 18/100] train_loss: 0.14328 valid_loss: 0.15617 test_loss: 0.11399 \n",
      "Validation loss decreased (0.162385 --> 0.156172).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14143 valid_loss: 0.15438 test_loss: 0.11328 \n",
      "Validation loss decreased (0.156172 --> 0.154380).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13987 valid_loss: 0.15019 test_loss: 0.10705 \n",
      "Validation loss decreased (0.154380 --> 0.150190).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13854 valid_loss: 0.15225 test_loss: 0.10699 \n",
      "[ 22/100] train_loss: 0.13980 valid_loss: 0.14918 test_loss: 0.10600 \n",
      "Validation loss decreased (0.150190 --> 0.149177).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13332 valid_loss: 0.14667 test_loss: 0.10142 \n",
      "Validation loss decreased (0.149177 --> 0.146675).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13068 valid_loss: 0.14569 test_loss: 0.10194 \n",
      "Validation loss decreased (0.146675 --> 0.145687).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12696 valid_loss: 0.14193 test_loss: 0.10168 \n",
      "Validation loss decreased (0.145687 --> 0.141930).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12520 valid_loss: 0.14123 test_loss: 0.10096 \n",
      "Validation loss decreased (0.141930 --> 0.141233).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12588 valid_loss: 0.14194 test_loss: 0.10033 \n",
      "[ 28/100] train_loss: 0.12171 valid_loss: 0.13577 test_loss: 0.09581 \n",
      "Validation loss decreased (0.141233 --> 0.135767).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12696 valid_loss: 0.13621 test_loss: 0.09643 \n",
      "[ 30/100] train_loss: 0.12597 valid_loss: 0.13531 test_loss: 0.10224 \n",
      "Validation loss decreased (0.135767 --> 0.135314).  Saving model ...\n",
      "[ 31/100] train_loss: 0.11986 valid_loss: 0.13273 test_loss: 0.09383 \n",
      "Validation loss decreased (0.135314 --> 0.132735).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12450 valid_loss: 0.13965 test_loss: 0.10065 \n",
      "[ 33/100] train_loss: 0.12210 valid_loss: 0.13745 test_loss: 0.09989 \n",
      "[ 34/100] train_loss: 0.11730 valid_loss: 0.13004 test_loss: 0.09292 \n",
      "Validation loss decreased (0.132735 --> 0.130043).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11820 valid_loss: 0.12747 test_loss: 0.09353 \n",
      "Validation loss decreased (0.130043 --> 0.127473).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11338 valid_loss: 0.12635 test_loss: 0.09183 \n",
      "Validation loss decreased (0.127473 --> 0.126353).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11714 valid_loss: 0.12802 test_loss: 0.09017 \n",
      "[ 38/100] train_loss: 0.11457 valid_loss: 0.12796 test_loss: 0.09170 \n",
      "[ 39/100] train_loss: 0.11510 valid_loss: 0.12640 test_loss: 0.09084 \n",
      "[ 40/100] train_loss: 0.11154 valid_loss: 0.12568 test_loss: 0.08990 \n",
      "Validation loss decreased (0.126353 --> 0.125680).  Saving model ...\n",
      "[ 41/100] train_loss: 0.10898 valid_loss: 0.12554 test_loss: 0.08949 \n",
      "Validation loss decreased (0.125680 --> 0.125542).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11005 valid_loss: 0.12161 test_loss: 0.08966 \n",
      "Validation loss decreased (0.125542 --> 0.121614).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11068 valid_loss: 0.12317 test_loss: 0.08870 \n",
      "[ 44/100] train_loss: 0.10923 valid_loss: 0.12542 test_loss: 0.09102 \n",
      "[ 45/100] train_loss: 0.10850 valid_loss: 0.12391 test_loss: 0.09005 \n",
      "[ 46/100] train_loss: 0.10546 valid_loss: 0.12020 test_loss: 0.08626 \n",
      "Validation loss decreased (0.121614 --> 0.120203).  Saving model ...\n",
      "[ 47/100] train_loss: 0.10688 valid_loss: 0.11988 test_loss: 0.08652 \n",
      "Validation loss decreased (0.120203 --> 0.119882).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10642 valid_loss: 0.11934 test_loss: 0.08834 \n",
      "Validation loss decreased (0.119882 --> 0.119341).  Saving model ...\n",
      "[ 49/100] train_loss: 0.11093 valid_loss: 0.12413 test_loss: 0.08832 \n",
      "[ 50/100] train_loss: 0.10891 valid_loss: 0.12103 test_loss: 0.08745 \n",
      "[ 51/100] train_loss: 0.10629 valid_loss: 0.11697 test_loss: 0.08549 \n",
      "Validation loss decreased (0.119341 --> 0.116971).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10548 valid_loss: 0.11687 test_loss: 0.08632 \n",
      "Validation loss decreased (0.116971 --> 0.116870).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10629 valid_loss: 0.11978 test_loss: 0.08565 \n",
      "[ 54/100] train_loss: 0.10336 valid_loss: 0.11595 test_loss: 0.08553 \n",
      "Validation loss decreased (0.116870 --> 0.115955).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10603 valid_loss: 0.11734 test_loss: 0.08402 \n",
      "[ 56/100] train_loss: 0.10185 valid_loss: 0.11461 test_loss: 0.08207 \n",
      "Validation loss decreased (0.115955 --> 0.114606).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10343 valid_loss: 0.11631 test_loss: 0.08360 \n",
      "[ 58/100] train_loss: 0.10191 valid_loss: 0.11488 test_loss: 0.08408 \n",
      "[ 59/100] train_loss: 0.10322 valid_loss: 0.11393 test_loss: 0.08340 \n",
      "Validation loss decreased (0.114606 --> 0.113932).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10215 valid_loss: 0.11212 test_loss: 0.08207 \n",
      "Validation loss decreased (0.113932 --> 0.112118).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10212 valid_loss: 0.11142 test_loss: 0.08450 \n",
      "Validation loss decreased (0.112118 --> 0.111425).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10316 valid_loss: 0.11346 test_loss: 0.08154 \n",
      "[ 63/100] train_loss: 0.09958 valid_loss: 0.11248 test_loss: 0.08365 \n",
      "[ 64/100] train_loss: 0.09665 valid_loss: 0.11170 test_loss: 0.08256 \n",
      "[ 65/100] train_loss: 0.10108 valid_loss: 0.11270 test_loss: 0.08168 \n",
      "[ 66/100] train_loss: 0.10200 valid_loss: 0.11055 test_loss: 0.08040 \n",
      "Validation loss decreased (0.111425 --> 0.110548).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10116 valid_loss: 0.11161 test_loss: 0.08417 \n",
      "[ 68/100] train_loss: 0.10031 valid_loss: 0.11317 test_loss: 0.08402 \n",
      "[ 69/100] train_loss: 0.09677 valid_loss: 0.11278 test_loss: 0.08297 \n",
      "[ 70/100] train_loss: 0.09773 valid_loss: 0.11137 test_loss: 0.08187 \n",
      "[ 71/100] train_loss: 0.09804 valid_loss: 0.10941 test_loss: 0.08094 \n",
      "Validation loss decreased (0.110548 --> 0.109409).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09471 valid_loss: 0.11009 test_loss: 0.08242 \n",
      "[ 73/100] train_loss: 0.09594 valid_loss: 0.10899 test_loss: 0.08017 \n",
      "Validation loss decreased (0.109409 --> 0.108986).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09474 valid_loss: 0.10760 test_loss: 0.08166 \n",
      "Validation loss decreased (0.108986 --> 0.107599).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09779 valid_loss: 0.10962 test_loss: 0.08049 \n",
      "[ 76/100] train_loss: 0.09402 valid_loss: 0.10933 test_loss: 0.07966 \n",
      "[ 77/100] train_loss: 0.09467 valid_loss: 0.10697 test_loss: 0.07974 \n",
      "Validation loss decreased (0.107599 --> 0.106973).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09225 valid_loss: 0.10765 test_loss: 0.07855 \n",
      "[ 79/100] train_loss: 0.09389 valid_loss: 0.10664 test_loss: 0.07937 \n",
      "Validation loss decreased (0.106973 --> 0.106638).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09167 valid_loss: 0.10991 test_loss: 0.07938 \n",
      "[ 81/100] train_loss: 0.09296 valid_loss: 0.10664 test_loss: 0.07808 \n",
      "[ 82/100] train_loss: 0.09427 valid_loss: 0.10737 test_loss: 0.07935 \n",
      "[ 83/100] train_loss: 0.09299 valid_loss: 0.10513 test_loss: 0.07681 \n",
      "Validation loss decreased (0.106638 --> 0.105128).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09320 valid_loss: 0.10698 test_loss: 0.07883 \n",
      "[ 85/100] train_loss: 0.09311 valid_loss: 0.10566 test_loss: 0.07750 \n",
      "[ 86/100] train_loss: 0.09244 valid_loss: 0.10484 test_loss: 0.07902 \n",
      "Validation loss decreased (0.105128 --> 0.104839).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09231 valid_loss: 0.10562 test_loss: 0.07721 \n",
      "[ 88/100] train_loss: 0.09037 valid_loss: 0.10420 test_loss: 0.07971 \n",
      "Validation loss decreased (0.104839 --> 0.104202).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09207 valid_loss: 0.10449 test_loss: 0.07850 \n",
      "[ 90/100] train_loss: 0.09120 valid_loss: 0.10549 test_loss: 0.07961 \n",
      "[ 91/100] train_loss: 0.09156 valid_loss: 0.10255 test_loss: 0.07761 \n",
      "Validation loss decreased (0.104202 --> 0.102552).  Saving model ...\n",
      "[ 92/100] train_loss: 0.08858 valid_loss: 0.10460 test_loss: 0.07730 \n",
      "[ 93/100] train_loss: 0.09396 valid_loss: 0.10265 test_loss: 0.07757 \n",
      "[ 94/100] train_loss: 0.08846 valid_loss: 0.10302 test_loss: 0.07762 \n",
      "[ 95/100] train_loss: 0.08910 valid_loss: 0.10373 test_loss: 0.07820 \n",
      "[ 96/100] train_loss: 0.09025 valid_loss: 0.10350 test_loss: 0.07767 \n",
      "[ 97/100] train_loss: 0.08904 valid_loss: 0.10176 test_loss: 0.07672 \n",
      "Validation loss decreased (0.102552 --> 0.101760).  Saving model ...\n",
      "[ 98/100] train_loss: 0.08827 valid_loss: 0.10149 test_loss: 0.07889 \n",
      "Validation loss decreased (0.101760 --> 0.101486).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09059 valid_loss: 0.10252 test_loss: 0.07790 \n",
      "[100/100] train_loss: 0.08987 valid_loss: 0.10100 test_loss: 0.07580 \n",
      "Validation loss decreased (0.101486 --> 0.101005).  Saving model ...\n",
      "TRAINING MODEL 11\n",
      "[  1/100] train_loss: 0.55426 valid_loss: 0.45810 test_loss: 0.42831 \n",
      "Validation loss decreased (inf --> 0.458104).  Saving model ...\n",
      "[  2/100] train_loss: 0.37330 valid_loss: 0.34540 test_loss: 0.28378 \n",
      "Validation loss decreased (0.458104 --> 0.345400).  Saving model ...\n",
      "[  3/100] train_loss: 0.29551 valid_loss: 0.29223 test_loss: 0.22779 \n",
      "Validation loss decreased (0.345400 --> 0.292226).  Saving model ...\n",
      "[  4/100] train_loss: 0.24755 valid_loss: 0.25485 test_loss: 0.18826 \n",
      "Validation loss decreased (0.292226 --> 0.254850).  Saving model ...\n",
      "[  5/100] train_loss: 0.22273 valid_loss: 0.23135 test_loss: 0.16728 \n",
      "Validation loss decreased (0.254850 --> 0.231347).  Saving model ...\n",
      "[  6/100] train_loss: 0.20501 valid_loss: 0.21738 test_loss: 0.14972 \n",
      "Validation loss decreased (0.231347 --> 0.217381).  Saving model ...\n",
      "[  7/100] train_loss: 0.19089 valid_loss: 0.21547 test_loss: 0.15288 \n",
      "Validation loss decreased (0.217381 --> 0.215470).  Saving model ...\n",
      "[  8/100] train_loss: 0.18358 valid_loss: 0.19714 test_loss: 0.13733 \n",
      "Validation loss decreased (0.215470 --> 0.197135).  Saving model ...\n",
      "[  9/100] train_loss: 0.16699 valid_loss: 0.18586 test_loss: 0.13488 \n",
      "Validation loss decreased (0.197135 --> 0.185864).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16586 valid_loss: 0.18355 test_loss: 0.12786 \n",
      "Validation loss decreased (0.185864 --> 0.183550).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16360 valid_loss: 0.17259 test_loss: 0.12169 \n",
      "Validation loss decreased (0.183550 --> 0.172592).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15688 valid_loss: 0.17051 test_loss: 0.11989 \n",
      "Validation loss decreased (0.172592 --> 0.170509).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15067 valid_loss: 0.17311 test_loss: 0.12148 \n",
      "[ 14/100] train_loss: 0.14903 valid_loss: 0.16901 test_loss: 0.11878 \n",
      "Validation loss decreased (0.170509 --> 0.169008).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15048 valid_loss: 0.16885 test_loss: 0.11670 \n",
      "Validation loss decreased (0.169008 --> 0.168853).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14357 valid_loss: 0.15693 test_loss: 0.11515 \n",
      "Validation loss decreased (0.168853 --> 0.156926).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13716 valid_loss: 0.15849 test_loss: 0.11217 \n",
      "[ 18/100] train_loss: 0.14336 valid_loss: 0.15177 test_loss: 0.11000 \n",
      "Validation loss decreased (0.156926 --> 0.151769).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13984 valid_loss: 0.15278 test_loss: 0.11367 \n",
      "[ 20/100] train_loss: 0.13224 valid_loss: 0.16199 test_loss: 0.11272 \n",
      "[ 21/100] train_loss: 0.13524 valid_loss: 0.15038 test_loss: 0.11112 \n",
      "Validation loss decreased (0.151769 --> 0.150382).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13254 valid_loss: 0.15293 test_loss: 0.11157 \n",
      "[ 23/100] train_loss: 0.12970 valid_loss: 0.15365 test_loss: 0.11004 \n",
      "[ 24/100] train_loss: 0.12946 valid_loss: 0.15494 test_loss: 0.11335 \n",
      "[ 25/100] train_loss: 0.12647 valid_loss: 0.14613 test_loss: 0.10698 \n",
      "Validation loss decreased (0.150382 --> 0.146130).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12861 valid_loss: 0.15024 test_loss: 0.11430 \n",
      "[ 27/100] train_loss: 0.12451 valid_loss: 0.14765 test_loss: 0.11047 \n",
      "[ 28/100] train_loss: 0.12681 valid_loss: 0.14186 test_loss: 0.10678 \n",
      "Validation loss decreased (0.146130 --> 0.141856).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12359 valid_loss: 0.14078 test_loss: 0.10973 \n",
      "Validation loss decreased (0.141856 --> 0.140776).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12389 valid_loss: 0.13693 test_loss: 0.10253 \n",
      "Validation loss decreased (0.140776 --> 0.136927).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12285 valid_loss: 0.14251 test_loss: 0.10725 \n",
      "[ 32/100] train_loss: 0.12222 valid_loss: 0.14180 test_loss: 0.10858 \n",
      "[ 33/100] train_loss: 0.12206 valid_loss: 0.13840 test_loss: 0.10373 \n",
      "[ 34/100] train_loss: 0.11892 valid_loss: 0.14069 test_loss: 0.10583 \n",
      "[ 35/100] train_loss: 0.11684 valid_loss: 0.13962 test_loss: 0.10538 \n",
      "[ 36/100] train_loss: 0.11819 valid_loss: 0.13919 test_loss: 0.10341 \n",
      "[ 37/100] train_loss: 0.11803 valid_loss: 0.13965 test_loss: 0.10099 \n",
      "[ 38/100] train_loss: 0.11593 valid_loss: 0.13640 test_loss: 0.10966 \n",
      "Validation loss decreased (0.136927 --> 0.136395).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11367 valid_loss: 0.13590 test_loss: 0.10710 \n",
      "Validation loss decreased (0.136395 --> 0.135898).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11454 valid_loss: 0.13176 test_loss: 0.10639 \n",
      "Validation loss decreased (0.135898 --> 0.131755).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11159 valid_loss: 0.12900 test_loss: 0.10325 \n",
      "Validation loss decreased (0.131755 --> 0.129003).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11383 valid_loss: 0.13525 test_loss: 0.10916 \n",
      "[ 43/100] train_loss: 0.11006 valid_loss: 0.13404 test_loss: 0.10073 \n",
      "[ 44/100] train_loss: 0.11200 valid_loss: 0.12546 test_loss: 0.09474 \n",
      "Validation loss decreased (0.129003 --> 0.125458).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11165 valid_loss: 0.12814 test_loss: 0.09734 \n",
      "[ 46/100] train_loss: 0.10793 valid_loss: 0.12816 test_loss: 0.09595 \n",
      "[ 47/100] train_loss: 0.10763 valid_loss: 0.12818 test_loss: 0.09486 \n",
      "[ 48/100] train_loss: 0.10871 valid_loss: 0.12883 test_loss: 0.09730 \n",
      "[ 49/100] train_loss: 0.10647 valid_loss: 0.12286 test_loss: 0.09343 \n",
      "Validation loss decreased (0.125458 --> 0.122865).  Saving model ...\n",
      "[ 50/100] train_loss: 0.11084 valid_loss: 0.12242 test_loss: 0.09483 \n",
      "Validation loss decreased (0.122865 --> 0.122421).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10664 valid_loss: 0.12513 test_loss: 0.09951 \n",
      "[ 52/100] train_loss: 0.10288 valid_loss: 0.12028 test_loss: 0.09032 \n",
      "Validation loss decreased (0.122421 --> 0.120275).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10567 valid_loss: 0.12047 test_loss: 0.08970 \n",
      "[ 54/100] train_loss: 0.10664 valid_loss: 0.12507 test_loss: 0.09513 \n",
      "[ 55/100] train_loss: 0.10616 valid_loss: 0.12026 test_loss: 0.09096 \n",
      "Validation loss decreased (0.120275 --> 0.120256).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10260 valid_loss: 0.11967 test_loss: 0.09058 \n",
      "Validation loss decreased (0.120256 --> 0.119673).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10328 valid_loss: 0.11976 test_loss: 0.09103 \n",
      "[ 58/100] train_loss: 0.10420 valid_loss: 0.12031 test_loss: 0.09354 \n",
      "[ 59/100] train_loss: 0.10276 valid_loss: 0.11664 test_loss: 0.08776 \n",
      "Validation loss decreased (0.119673 --> 0.116643).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10271 valid_loss: 0.11850 test_loss: 0.09176 \n",
      "[ 61/100] train_loss: 0.10558 valid_loss: 0.12201 test_loss: 0.09498 \n",
      "[ 62/100] train_loss: 0.09849 valid_loss: 0.11696 test_loss: 0.09288 \n",
      "[ 63/100] train_loss: 0.10150 valid_loss: 0.11786 test_loss: 0.08977 \n",
      "[ 64/100] train_loss: 0.09997 valid_loss: 0.11677 test_loss: 0.08966 \n",
      "[ 65/100] train_loss: 0.10144 valid_loss: 0.11542 test_loss: 0.08774 \n",
      "Validation loss decreased (0.116643 --> 0.115417).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10069 valid_loss: 0.11634 test_loss: 0.08837 \n",
      "[ 67/100] train_loss: 0.10014 valid_loss: 0.11574 test_loss: 0.09017 \n",
      "[ 68/100] train_loss: 0.09958 valid_loss: 0.11645 test_loss: 0.09074 \n",
      "[ 69/100] train_loss: 0.09957 valid_loss: 0.11767 test_loss: 0.09136 \n",
      "[ 70/100] train_loss: 0.09864 valid_loss: 0.11448 test_loss: 0.08745 \n",
      "Validation loss decreased (0.115417 --> 0.114479).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09759 valid_loss: 0.11628 test_loss: 0.08765 \n",
      "[ 72/100] train_loss: 0.09707 valid_loss: 0.11536 test_loss: 0.08662 \n",
      "[ 73/100] train_loss: 0.10039 valid_loss: 0.11228 test_loss: 0.08768 \n",
      "Validation loss decreased (0.114479 --> 0.112280).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09590 valid_loss: 0.11374 test_loss: 0.08449 \n",
      "[ 75/100] train_loss: 0.09778 valid_loss: 0.11202 test_loss: 0.08534 \n",
      "Validation loss decreased (0.112280 --> 0.112021).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09677 valid_loss: 0.11304 test_loss: 0.08468 \n",
      "[ 77/100] train_loss: 0.09457 valid_loss: 0.11447 test_loss: 0.08755 \n",
      "[ 78/100] train_loss: 0.09629 valid_loss: 0.11136 test_loss: 0.08321 \n",
      "Validation loss decreased (0.112021 --> 0.111363).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09628 valid_loss: 0.11282 test_loss: 0.08646 \n",
      "[ 80/100] train_loss: 0.09467 valid_loss: 0.10990 test_loss: 0.08364 \n",
      "Validation loss decreased (0.111363 --> 0.109895).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09586 valid_loss: 0.11054 test_loss: 0.08249 \n",
      "[ 82/100] train_loss: 0.09246 valid_loss: 0.10794 test_loss: 0.08201 \n",
      "Validation loss decreased (0.109895 --> 0.107939).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09444 valid_loss: 0.10854 test_loss: 0.08457 \n",
      "[ 84/100] train_loss: 0.09328 valid_loss: 0.11249 test_loss: 0.08579 \n",
      "[ 85/100] train_loss: 0.09159 valid_loss: 0.11250 test_loss: 0.08337 \n",
      "[ 86/100] train_loss: 0.09617 valid_loss: 0.10731 test_loss: 0.08279 \n",
      "Validation loss decreased (0.107939 --> 0.107309).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09274 valid_loss: 0.10601 test_loss: 0.07990 \n",
      "Validation loss decreased (0.107309 --> 0.106005).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09269 valid_loss: 0.10607 test_loss: 0.08162 \n",
      "[ 89/100] train_loss: 0.09210 valid_loss: 0.10584 test_loss: 0.08205 \n",
      "Validation loss decreased (0.106005 --> 0.105843).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09296 valid_loss: 0.11085 test_loss: 0.08286 \n",
      "[ 91/100] train_loss: 0.08920 valid_loss: 0.10856 test_loss: 0.08104 \n",
      "[ 92/100] train_loss: 0.09175 valid_loss: 0.10925 test_loss: 0.08247 \n",
      "[ 93/100] train_loss: 0.09191 valid_loss: 0.10467 test_loss: 0.08217 \n",
      "Validation loss decreased (0.105843 --> 0.104674).  Saving model ...\n",
      "[ 94/100] train_loss: 0.09204 valid_loss: 0.10599 test_loss: 0.08348 \n",
      "[ 95/100] train_loss: 0.09103 valid_loss: 0.11442 test_loss: 0.08482 \n",
      "[ 96/100] train_loss: 0.08741 valid_loss: 0.10887 test_loss: 0.07993 \n",
      "[ 97/100] train_loss: 0.08822 valid_loss: 0.10272 test_loss: 0.07895 \n",
      "Validation loss decreased (0.104674 --> 0.102722).  Saving model ...\n",
      "[ 98/100] train_loss: 0.09365 valid_loss: 0.10389 test_loss: 0.08102 \n",
      "[ 99/100] train_loss: 0.08967 valid_loss: 0.10414 test_loss: 0.08291 \n",
      "[100/100] train_loss: 0.08775 valid_loss: 0.10628 test_loss: 0.08112 \n",
      "TRAINING MODEL 12\n",
      "[  1/100] train_loss: 0.60875 valid_loss: 0.54452 test_loss: 0.47644 \n",
      "Validation loss decreased (inf --> 0.544524).  Saving model ...\n",
      "[  2/100] train_loss: 0.43410 valid_loss: 0.38191 test_loss: 0.31323 \n",
      "Validation loss decreased (0.544524 --> 0.381911).  Saving model ...\n",
      "[  3/100] train_loss: 0.31846 valid_loss: 0.30974 test_loss: 0.23251 \n",
      "Validation loss decreased (0.381911 --> 0.309735).  Saving model ...\n",
      "[  4/100] train_loss: 0.26253 valid_loss: 0.26705 test_loss: 0.19032 \n",
      "Validation loss decreased (0.309735 --> 0.267049).  Saving model ...\n",
      "[  5/100] train_loss: 0.23284 valid_loss: 0.24695 test_loss: 0.17233 \n",
      "Validation loss decreased (0.267049 --> 0.246953).  Saving model ...\n",
      "[  6/100] train_loss: 0.21280 valid_loss: 0.23073 test_loss: 0.15611 \n",
      "Validation loss decreased (0.246953 --> 0.230732).  Saving model ...\n",
      "[  7/100] train_loss: 0.20000 valid_loss: 0.22490 test_loss: 0.15134 \n",
      "Validation loss decreased (0.230732 --> 0.224896).  Saving model ...\n",
      "[  8/100] train_loss: 0.18852 valid_loss: 0.20869 test_loss: 0.14699 \n",
      "Validation loss decreased (0.224896 --> 0.208695).  Saving model ...\n",
      "[  9/100] train_loss: 0.17908 valid_loss: 0.19550 test_loss: 0.13444 \n",
      "Validation loss decreased (0.208695 --> 0.195499).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17335 valid_loss: 0.19283 test_loss: 0.13196 \n",
      "Validation loss decreased (0.195499 --> 0.192835).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16943 valid_loss: 0.18309 test_loss: 0.12872 \n",
      "Validation loss decreased (0.192835 --> 0.183090).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16444 valid_loss: 0.18044 test_loss: 0.12561 \n",
      "Validation loss decreased (0.183090 --> 0.180440).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15909 valid_loss: 0.17301 test_loss: 0.12609 \n",
      "Validation loss decreased (0.180440 --> 0.173007).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15434 valid_loss: 0.16714 test_loss: 0.12238 \n",
      "Validation loss decreased (0.173007 --> 0.167136).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14981 valid_loss: 0.16862 test_loss: 0.11853 \n",
      "[ 16/100] train_loss: 0.14589 valid_loss: 0.16642 test_loss: 0.11719 \n",
      "Validation loss decreased (0.167136 --> 0.166425).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14726 valid_loss: 0.16155 test_loss: 0.11281 \n",
      "Validation loss decreased (0.166425 --> 0.161547).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13938 valid_loss: 0.15635 test_loss: 0.11243 \n",
      "Validation loss decreased (0.161547 --> 0.156347).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14019 valid_loss: 0.15635 test_loss: 0.11116 \n",
      "Validation loss decreased (0.156347 --> 0.156347).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13743 valid_loss: 0.15280 test_loss: 0.10872 \n",
      "Validation loss decreased (0.156347 --> 0.152799).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13652 valid_loss: 0.15167 test_loss: 0.10995 \n",
      "Validation loss decreased (0.152799 --> 0.151672).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13676 valid_loss: 0.15174 test_loss: 0.10573 \n",
      "[ 23/100] train_loss: 0.13583 valid_loss: 0.14829 test_loss: 0.10476 \n",
      "Validation loss decreased (0.151672 --> 0.148288).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13041 valid_loss: 0.14292 test_loss: 0.10388 \n",
      "Validation loss decreased (0.148288 --> 0.142917).  Saving model ...\n",
      "[ 25/100] train_loss: 0.12677 valid_loss: 0.14512 test_loss: 0.10563 \n",
      "[ 26/100] train_loss: 0.12881 valid_loss: 0.14435 test_loss: 0.10319 \n",
      "[ 27/100] train_loss: 0.12991 valid_loss: 0.13849 test_loss: 0.10056 \n",
      "Validation loss decreased (0.142917 --> 0.138485).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12523 valid_loss: 0.14258 test_loss: 0.09824 \n",
      "[ 29/100] train_loss: 0.12320 valid_loss: 0.13776 test_loss: 0.10063 \n",
      "Validation loss decreased (0.138485 --> 0.137760).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12454 valid_loss: 0.13613 test_loss: 0.09917 \n",
      "Validation loss decreased (0.137760 --> 0.136127).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12047 valid_loss: 0.13757 test_loss: 0.09925 \n",
      "[ 32/100] train_loss: 0.12383 valid_loss: 0.13597 test_loss: 0.10069 \n",
      "Validation loss decreased (0.136127 --> 0.135970).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12269 valid_loss: 0.13428 test_loss: 0.09957 \n",
      "Validation loss decreased (0.135970 --> 0.134277).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12144 valid_loss: 0.13187 test_loss: 0.09467 \n",
      "Validation loss decreased (0.134277 --> 0.131867).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11804 valid_loss: 0.13509 test_loss: 0.09903 \n",
      "[ 36/100] train_loss: 0.11686 valid_loss: 0.12930 test_loss: 0.09978 \n",
      "Validation loss decreased (0.131867 --> 0.129296).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11418 valid_loss: 0.12851 test_loss: 0.09557 \n",
      "Validation loss decreased (0.129296 --> 0.128508).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11739 valid_loss: 0.13169 test_loss: 0.09300 \n",
      "[ 39/100] train_loss: 0.11642 valid_loss: 0.12951 test_loss: 0.09523 \n",
      "[ 40/100] train_loss: 0.11368 valid_loss: 0.12816 test_loss: 0.09816 \n",
      "Validation loss decreased (0.128508 --> 0.128158).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11139 valid_loss: 0.12611 test_loss: 0.09284 \n",
      "Validation loss decreased (0.128158 --> 0.126106).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11108 valid_loss: 0.12935 test_loss: 0.09251 \n",
      "[ 43/100] train_loss: 0.10957 valid_loss: 0.12935 test_loss: 0.09026 \n",
      "[ 44/100] train_loss: 0.11049 valid_loss: 0.12580 test_loss: 0.09458 \n",
      "Validation loss decreased (0.126106 --> 0.125804).  Saving model ...\n",
      "[ 45/100] train_loss: 0.10901 valid_loss: 0.12284 test_loss: 0.08965 \n",
      "Validation loss decreased (0.125804 --> 0.122838).  Saving model ...\n",
      "[ 46/100] train_loss: 0.11190 valid_loss: 0.12440 test_loss: 0.09101 \n",
      "[ 47/100] train_loss: 0.11073 valid_loss: 0.12381 test_loss: 0.09256 \n",
      "[ 48/100] train_loss: 0.11042 valid_loss: 0.12893 test_loss: 0.09017 \n",
      "[ 49/100] train_loss: 0.10555 valid_loss: 0.11892 test_loss: 0.09086 \n",
      "Validation loss decreased (0.122838 --> 0.118921).  Saving model ...\n",
      "[ 50/100] train_loss: 0.11049 valid_loss: 0.12489 test_loss: 0.09301 \n",
      "[ 51/100] train_loss: 0.10900 valid_loss: 0.11911 test_loss: 0.08990 \n",
      "[ 52/100] train_loss: 0.10557 valid_loss: 0.12179 test_loss: 0.08995 \n",
      "[ 53/100] train_loss: 0.10740 valid_loss: 0.12235 test_loss: 0.09346 \n",
      "[ 54/100] train_loss: 0.10422 valid_loss: 0.12042 test_loss: 0.09084 \n",
      "[ 55/100] train_loss: 0.10519 valid_loss: 0.11964 test_loss: 0.09022 \n",
      "[ 56/100] train_loss: 0.10477 valid_loss: 0.11935 test_loss: 0.08799 \n",
      "[ 57/100] train_loss: 0.10332 valid_loss: 0.11893 test_loss: 0.08855 \n",
      "[ 58/100] train_loss: 0.10365 valid_loss: 0.11802 test_loss: 0.08997 \n",
      "Validation loss decreased (0.118921 --> 0.118018).  Saving model ...\n",
      "[ 59/100] train_loss: 0.09876 valid_loss: 0.11993 test_loss: 0.08909 \n",
      "[ 60/100] train_loss: 0.10076 valid_loss: 0.11936 test_loss: 0.08732 \n",
      "[ 61/100] train_loss: 0.10419 valid_loss: 0.11628 test_loss: 0.08846 \n",
      "Validation loss decreased (0.118018 --> 0.116279).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10205 valid_loss: 0.12206 test_loss: 0.09026 \n",
      "[ 63/100] train_loss: 0.10213 valid_loss: 0.11601 test_loss: 0.08723 \n",
      "Validation loss decreased (0.116279 --> 0.116015).  Saving model ...\n",
      "[ 64/100] train_loss: 0.09954 valid_loss: 0.11615 test_loss: 0.08467 \n",
      "[ 65/100] train_loss: 0.09961 valid_loss: 0.11973 test_loss: 0.08980 \n",
      "[ 66/100] train_loss: 0.09904 valid_loss: 0.11208 test_loss: 0.08577 \n",
      "Validation loss decreased (0.116015 --> 0.112079).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10009 valid_loss: 0.11579 test_loss: 0.08905 \n",
      "[ 68/100] train_loss: 0.09904 valid_loss: 0.11649 test_loss: 0.08647 \n",
      "[ 69/100] train_loss: 0.10023 valid_loss: 0.11421 test_loss: 0.09210 \n",
      "[ 70/100] train_loss: 0.09783 valid_loss: 0.11054 test_loss: 0.08543 \n",
      "Validation loss decreased (0.112079 --> 0.110543).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09835 valid_loss: 0.10938 test_loss: 0.08340 \n",
      "Validation loss decreased (0.110543 --> 0.109382).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09592 valid_loss: 0.11202 test_loss: 0.08704 \n",
      "[ 73/100] train_loss: 0.09887 valid_loss: 0.11412 test_loss: 0.09011 \n",
      "[ 74/100] train_loss: 0.09551 valid_loss: 0.11228 test_loss: 0.08598 \n",
      "[ 75/100] train_loss: 0.09614 valid_loss: 0.11271 test_loss: 0.09021 \n",
      "[ 76/100] train_loss: 0.09528 valid_loss: 0.10859 test_loss: 0.08437 \n",
      "Validation loss decreased (0.109382 --> 0.108590).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09574 valid_loss: 0.10772 test_loss: 0.08261 \n",
      "Validation loss decreased (0.108590 --> 0.107718).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09407 valid_loss: 0.11326 test_loss: 0.08814 \n",
      "[ 79/100] train_loss: 0.09814 valid_loss: 0.11120 test_loss: 0.08411 \n",
      "[ 80/100] train_loss: 0.09372 valid_loss: 0.11214 test_loss: 0.08735 \n",
      "[ 81/100] train_loss: 0.09276 valid_loss: 0.11020 test_loss: 0.08417 \n",
      "[ 82/100] train_loss: 0.09491 valid_loss: 0.10885 test_loss: 0.08426 \n",
      "[ 83/100] train_loss: 0.09488 valid_loss: 0.10829 test_loss: 0.08429 \n",
      "[ 84/100] train_loss: 0.09548 valid_loss: 0.10772 test_loss: 0.08406 \n",
      "Validation loss decreased (0.107718 --> 0.107716).  Saving model ...\n",
      "[ 85/100] train_loss: 0.09254 valid_loss: 0.10635 test_loss: 0.08247 \n",
      "Validation loss decreased (0.107716 --> 0.106354).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09218 valid_loss: 0.10609 test_loss: 0.08135 \n",
      "Validation loss decreased (0.106354 --> 0.106093).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09457 valid_loss: 0.11172 test_loss: 0.08497 \n",
      "[ 88/100] train_loss: 0.09079 valid_loss: 0.10614 test_loss: 0.08317 \n",
      "[ 89/100] train_loss: 0.09115 valid_loss: 0.10725 test_loss: 0.08346 \n",
      "[ 90/100] train_loss: 0.09136 valid_loss: 0.10640 test_loss: 0.07913 \n",
      "[ 91/100] train_loss: 0.08967 valid_loss: 0.10599 test_loss: 0.08035 \n",
      "Validation loss decreased (0.106093 --> 0.105986).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09109 valid_loss: 0.10686 test_loss: 0.08358 \n",
      "[ 93/100] train_loss: 0.09168 valid_loss: 0.10510 test_loss: 0.08052 \n",
      "Validation loss decreased (0.105986 --> 0.105104).  Saving model ...\n",
      "[ 94/100] train_loss: 0.08930 valid_loss: 0.10587 test_loss: 0.08259 \n",
      "[ 95/100] train_loss: 0.08866 valid_loss: 0.10301 test_loss: 0.08183 \n",
      "Validation loss decreased (0.105104 --> 0.103014).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09279 valid_loss: 0.10704 test_loss: 0.08041 \n",
      "[ 97/100] train_loss: 0.09110 valid_loss: 0.10349 test_loss: 0.08026 \n",
      "[ 98/100] train_loss: 0.09175 valid_loss: 0.10349 test_loss: 0.08050 \n",
      "[ 99/100] train_loss: 0.09311 valid_loss: 0.10396 test_loss: 0.07934 \n",
      "[100/100] train_loss: 0.09000 valid_loss: 0.10262 test_loss: 0.08039 \n",
      "Validation loss decreased (0.103014 --> 0.102622).  Saving model ...\n",
      "TRAINING MODEL 13\n",
      "[  1/100] train_loss: 0.57001 valid_loss: 0.46950 test_loss: 0.43332 \n",
      "Validation loss decreased (inf --> 0.469496).  Saving model ...\n",
      "[  2/100] train_loss: 0.38859 valid_loss: 0.35260 test_loss: 0.28474 \n",
      "Validation loss decreased (0.469496 --> 0.352602).  Saving model ...\n",
      "[  3/100] train_loss: 0.29571 valid_loss: 0.29021 test_loss: 0.21989 \n",
      "Validation loss decreased (0.352602 --> 0.290209).  Saving model ...\n",
      "[  4/100] train_loss: 0.24378 valid_loss: 0.25589 test_loss: 0.18499 \n",
      "Validation loss decreased (0.290209 --> 0.255893).  Saving model ...\n",
      "[  5/100] train_loss: 0.21993 valid_loss: 0.22910 test_loss: 0.16457 \n",
      "Validation loss decreased (0.255893 --> 0.229105).  Saving model ...\n",
      "[  6/100] train_loss: 0.20468 valid_loss: 0.22616 test_loss: 0.15511 \n",
      "Validation loss decreased (0.229105 --> 0.226156).  Saving model ...\n",
      "[  7/100] train_loss: 0.19456 valid_loss: 0.20631 test_loss: 0.14420 \n",
      "Validation loss decreased (0.226156 --> 0.206308).  Saving model ...\n",
      "[  8/100] train_loss: 0.18605 valid_loss: 0.20192 test_loss: 0.14334 \n",
      "Validation loss decreased (0.206308 --> 0.201924).  Saving model ...\n",
      "[  9/100] train_loss: 0.18051 valid_loss: 0.20228 test_loss: 0.14061 \n",
      "[ 10/100] train_loss: 0.17146 valid_loss: 0.18764 test_loss: 0.13194 \n",
      "Validation loss decreased (0.201924 --> 0.187639).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16378 valid_loss: 0.19145 test_loss: 0.13167 \n",
      "[ 12/100] train_loss: 0.15920 valid_loss: 0.18250 test_loss: 0.12505 \n",
      "Validation loss decreased (0.187639 --> 0.182499).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15500 valid_loss: 0.18167 test_loss: 0.12570 \n",
      "Validation loss decreased (0.182499 --> 0.181667).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15566 valid_loss: 0.17250 test_loss: 0.12639 \n",
      "Validation loss decreased (0.181667 --> 0.172501).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15063 valid_loss: 0.16796 test_loss: 0.12174 \n",
      "Validation loss decreased (0.172501 --> 0.167958).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14629 valid_loss: 0.16687 test_loss: 0.12120 \n",
      "Validation loss decreased (0.167958 --> 0.166867).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14500 valid_loss: 0.15726 test_loss: 0.11417 \n",
      "Validation loss decreased (0.166867 --> 0.157259).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14184 valid_loss: 0.16059 test_loss: 0.11477 \n",
      "[ 19/100] train_loss: 0.14121 valid_loss: 0.15513 test_loss: 0.11291 \n",
      "Validation loss decreased (0.157259 --> 0.155129).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13749 valid_loss: 0.15250 test_loss: 0.11085 \n",
      "Validation loss decreased (0.155129 --> 0.152498).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13457 valid_loss: 0.15149 test_loss: 0.11048 \n",
      "Validation loss decreased (0.152498 --> 0.151486).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13281 valid_loss: 0.15331 test_loss: 0.11344 \n",
      "[ 23/100] train_loss: 0.13859 valid_loss: 0.14649 test_loss: 0.11071 \n",
      "Validation loss decreased (0.151486 --> 0.146488).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13280 valid_loss: 0.14848 test_loss: 0.11218 \n",
      "[ 25/100] train_loss: 0.13022 valid_loss: 0.14626 test_loss: 0.11085 \n",
      "Validation loss decreased (0.146488 --> 0.146260).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13147 valid_loss: 0.14540 test_loss: 0.10553 \n",
      "Validation loss decreased (0.146260 --> 0.145405).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13071 valid_loss: 0.14264 test_loss: 0.10715 \n",
      "Validation loss decreased (0.145405 --> 0.142636).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12300 valid_loss: 0.13873 test_loss: 0.10340 \n",
      "Validation loss decreased (0.142636 --> 0.138730).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12858 valid_loss: 0.13812 test_loss: 0.10261 \n",
      "Validation loss decreased (0.138730 --> 0.138122).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12405 valid_loss: 0.13906 test_loss: 0.10347 \n",
      "[ 31/100] train_loss: 0.12482 valid_loss: 0.13910 test_loss: 0.10736 \n",
      "[ 32/100] train_loss: 0.12229 valid_loss: 0.13918 test_loss: 0.10758 \n",
      "[ 33/100] train_loss: 0.11996 valid_loss: 0.13301 test_loss: 0.10266 \n",
      "Validation loss decreased (0.138122 --> 0.133011).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11931 valid_loss: 0.13847 test_loss: 0.10379 \n",
      "[ 35/100] train_loss: 0.12194 valid_loss: 0.13845 test_loss: 0.10362 \n",
      "[ 36/100] train_loss: 0.11739 valid_loss: 0.13218 test_loss: 0.09508 \n",
      "Validation loss decreased (0.133011 --> 0.132180).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11653 valid_loss: 0.13019 test_loss: 0.09746 \n",
      "Validation loss decreased (0.132180 --> 0.130190).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11436 valid_loss: 0.12957 test_loss: 0.09810 \n",
      "Validation loss decreased (0.130190 --> 0.129566).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11554 valid_loss: 0.13135 test_loss: 0.10159 \n",
      "[ 40/100] train_loss: 0.11794 valid_loss: 0.12959 test_loss: 0.10058 \n",
      "[ 41/100] train_loss: 0.11726 valid_loss: 0.13341 test_loss: 0.10294 \n",
      "[ 42/100] train_loss: 0.11287 valid_loss: 0.12997 test_loss: 0.09740 \n",
      "[ 43/100] train_loss: 0.11349 valid_loss: 0.12921 test_loss: 0.09929 \n",
      "Validation loss decreased (0.129566 --> 0.129207).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11368 valid_loss: 0.12664 test_loss: 0.09181 \n",
      "Validation loss decreased (0.129207 --> 0.126636).  Saving model ...\n",
      "[ 45/100] train_loss: 0.10981 valid_loss: 0.12496 test_loss: 0.09352 \n",
      "Validation loss decreased (0.126636 --> 0.124956).  Saving model ...\n",
      "[ 46/100] train_loss: 0.10910 valid_loss: 0.12353 test_loss: 0.09571 \n",
      "Validation loss decreased (0.124956 --> 0.123534).  Saving model ...\n",
      "[ 47/100] train_loss: 0.10951 valid_loss: 0.12760 test_loss: 0.09232 \n",
      "[ 48/100] train_loss: 0.10730 valid_loss: 0.12215 test_loss: 0.09110 \n",
      "Validation loss decreased (0.123534 --> 0.122150).  Saving model ...\n",
      "[ 49/100] train_loss: 0.11055 valid_loss: 0.12508 test_loss: 0.09079 \n",
      "[ 50/100] train_loss: 0.11052 valid_loss: 0.12358 test_loss: 0.09024 \n",
      "[ 51/100] train_loss: 0.10775 valid_loss: 0.12424 test_loss: 0.09185 \n",
      "[ 52/100] train_loss: 0.10652 valid_loss: 0.11936 test_loss: 0.08924 \n",
      "Validation loss decreased (0.122150 --> 0.119356).  Saving model ...\n",
      "[ 53/100] train_loss: 0.10501 valid_loss: 0.11904 test_loss: 0.08800 \n",
      "Validation loss decreased (0.119356 --> 0.119041).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10577 valid_loss: 0.12131 test_loss: 0.09245 \n",
      "[ 55/100] train_loss: 0.10272 valid_loss: 0.12026 test_loss: 0.09208 \n",
      "[ 56/100] train_loss: 0.10476 valid_loss: 0.11710 test_loss: 0.08919 \n",
      "Validation loss decreased (0.119041 --> 0.117098).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10345 valid_loss: 0.12014 test_loss: 0.09120 \n",
      "[ 58/100] train_loss: 0.10554 valid_loss: 0.12209 test_loss: 0.08878 \n",
      "[ 59/100] train_loss: 0.10623 valid_loss: 0.11730 test_loss: 0.08987 \n",
      "[ 60/100] train_loss: 0.10513 valid_loss: 0.12096 test_loss: 0.08685 \n",
      "[ 61/100] train_loss: 0.10091 valid_loss: 0.11449 test_loss: 0.08559 \n",
      "Validation loss decreased (0.117098 --> 0.114487).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10072 valid_loss: 0.11514 test_loss: 0.08526 \n",
      "[ 63/100] train_loss: 0.10776 valid_loss: 0.11541 test_loss: 0.08592 \n",
      "[ 64/100] train_loss: 0.10137 valid_loss: 0.11448 test_loss: 0.08608 \n",
      "Validation loss decreased (0.114487 --> 0.114483).  Saving model ...\n",
      "[ 65/100] train_loss: 0.10054 valid_loss: 0.11209 test_loss: 0.08365 \n",
      "Validation loss decreased (0.114483 --> 0.112093).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10383 valid_loss: 0.12248 test_loss: 0.08801 \n",
      "[ 67/100] train_loss: 0.09886 valid_loss: 0.11138 test_loss: 0.08252 \n",
      "Validation loss decreased (0.112093 --> 0.111376).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10005 valid_loss: 0.11331 test_loss: 0.08626 \n",
      "[ 69/100] train_loss: 0.10191 valid_loss: 0.11443 test_loss: 0.08506 \n",
      "[ 70/100] train_loss: 0.10037 valid_loss: 0.11732 test_loss: 0.08528 \n",
      "[ 71/100] train_loss: 0.09812 valid_loss: 0.11076 test_loss: 0.08173 \n",
      "Validation loss decreased (0.111376 --> 0.110763).  Saving model ...\n",
      "[ 72/100] train_loss: 0.10002 valid_loss: 0.11449 test_loss: 0.08341 \n",
      "[ 73/100] train_loss: 0.09641 valid_loss: 0.11386 test_loss: 0.08471 \n",
      "[ 74/100] train_loss: 0.10073 valid_loss: 0.11634 test_loss: 0.08525 \n",
      "[ 75/100] train_loss: 0.09784 valid_loss: 0.11604 test_loss: 0.08395 \n",
      "[ 76/100] train_loss: 0.09797 valid_loss: 0.11188 test_loss: 0.08273 \n",
      "[ 77/100] train_loss: 0.09420 valid_loss: 0.10861 test_loss: 0.08246 \n",
      "Validation loss decreased (0.110763 --> 0.108608).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09884 valid_loss: 0.10984 test_loss: 0.08203 \n",
      "[ 79/100] train_loss: 0.09589 valid_loss: 0.10748 test_loss: 0.08015 \n",
      "Validation loss decreased (0.108608 --> 0.107475).  Saving model ...\n",
      "[ 80/100] train_loss: 0.09667 valid_loss: 0.11152 test_loss: 0.08172 \n",
      "[ 81/100] train_loss: 0.09480 valid_loss: 0.10734 test_loss: 0.08136 \n",
      "Validation loss decreased (0.107475 --> 0.107342).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09713 valid_loss: 0.11162 test_loss: 0.08326 \n",
      "[ 83/100] train_loss: 0.09503 valid_loss: 0.10891 test_loss: 0.08021 \n",
      "[ 84/100] train_loss: 0.09552 valid_loss: 0.10780 test_loss: 0.08011 \n",
      "[ 85/100] train_loss: 0.09377 valid_loss: 0.10705 test_loss: 0.07889 \n",
      "Validation loss decreased (0.107342 --> 0.107045).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09488 valid_loss: 0.10726 test_loss: 0.07861 \n",
      "[ 87/100] train_loss: 0.09425 valid_loss: 0.10910 test_loss: 0.07917 \n",
      "[ 88/100] train_loss: 0.09279 valid_loss: 0.10861 test_loss: 0.08025 \n",
      "[ 89/100] train_loss: 0.09342 valid_loss: 0.10805 test_loss: 0.08032 \n",
      "[ 90/100] train_loss: 0.09171 valid_loss: 0.11618 test_loss: 0.08277 \n",
      "[ 91/100] train_loss: 0.09184 valid_loss: 0.10633 test_loss: 0.08028 \n",
      "Validation loss decreased (0.107045 --> 0.106329).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09188 valid_loss: 0.10534 test_loss: 0.07819 \n",
      "Validation loss decreased (0.106329 --> 0.105340).  Saving model ...\n",
      "[ 93/100] train_loss: 0.08980 valid_loss: 0.10876 test_loss: 0.07951 \n",
      "[ 94/100] train_loss: 0.08952 valid_loss: 0.10650 test_loss: 0.07770 \n",
      "[ 95/100] train_loss: 0.09134 valid_loss: 0.10574 test_loss: 0.07841 \n",
      "[ 96/100] train_loss: 0.08927 valid_loss: 0.10651 test_loss: 0.07819 \n",
      "[ 97/100] train_loss: 0.09084 valid_loss: 0.10468 test_loss: 0.07855 \n",
      "Validation loss decreased (0.105340 --> 0.104679).  Saving model ...\n",
      "[ 98/100] train_loss: 0.08977 valid_loss: 0.10902 test_loss: 0.07943 \n",
      "[ 99/100] train_loss: 0.09313 valid_loss: 0.10549 test_loss: 0.07799 \n",
      "[100/100] train_loss: 0.08908 valid_loss: 0.10565 test_loss: 0.07824 \n",
      "TRAINING MODEL 14\n",
      "[  1/100] train_loss: 0.66731 valid_loss: 0.59117 test_loss: 0.53587 \n",
      "Validation loss decreased (inf --> 0.591172).  Saving model ...\n",
      "[  2/100] train_loss: 0.49973 valid_loss: 0.43855 test_loss: 0.36587 \n",
      "Validation loss decreased (0.591172 --> 0.438546).  Saving model ...\n",
      "[  3/100] train_loss: 0.37665 valid_loss: 0.35560 test_loss: 0.28964 \n",
      "Validation loss decreased (0.438546 --> 0.355598).  Saving model ...\n",
      "[  4/100] train_loss: 0.30150 valid_loss: 0.30425 test_loss: 0.23164 \n",
      "Validation loss decreased (0.355598 --> 0.304249).  Saving model ...\n",
      "[  5/100] train_loss: 0.26117 valid_loss: 0.27218 test_loss: 0.19415 \n",
      "Validation loss decreased (0.304249 --> 0.272180).  Saving model ...\n",
      "[  6/100] train_loss: 0.23325 valid_loss: 0.24866 test_loss: 0.17600 \n",
      "Validation loss decreased (0.272180 --> 0.248658).  Saving model ...\n",
      "[  7/100] train_loss: 0.21595 valid_loss: 0.23323 test_loss: 0.16227 \n",
      "Validation loss decreased (0.248658 --> 0.233233).  Saving model ...\n",
      "[  8/100] train_loss: 0.20202 valid_loss: 0.22589 test_loss: 0.15765 \n",
      "Validation loss decreased (0.233233 --> 0.225888).  Saving model ...\n",
      "[  9/100] train_loss: 0.19268 valid_loss: 0.20841 test_loss: 0.14410 \n",
      "Validation loss decreased (0.225888 --> 0.208408).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18517 valid_loss: 0.20387 test_loss: 0.14242 \n",
      "Validation loss decreased (0.208408 --> 0.203872).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17929 valid_loss: 0.19982 test_loss: 0.13426 \n",
      "Validation loss decreased (0.203872 --> 0.199823).  Saving model ...\n",
      "[ 12/100] train_loss: 0.17312 valid_loss: 0.19358 test_loss: 0.13396 \n",
      "Validation loss decreased (0.199823 --> 0.193576).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16897 valid_loss: 0.18497 test_loss: 0.12955 \n",
      "Validation loss decreased (0.193576 --> 0.184968).  Saving model ...\n",
      "[ 14/100] train_loss: 0.16429 valid_loss: 0.17638 test_loss: 0.12402 \n",
      "Validation loss decreased (0.184968 --> 0.176379).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15754 valid_loss: 0.17469 test_loss: 0.12292 \n",
      "Validation loss decreased (0.176379 --> 0.174685).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15354 valid_loss: 0.16984 test_loss: 0.11885 \n",
      "Validation loss decreased (0.174685 --> 0.169844).  Saving model ...\n",
      "[ 17/100] train_loss: 0.15304 valid_loss: 0.16488 test_loss: 0.11995 \n",
      "Validation loss decreased (0.169844 --> 0.164883).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14488 valid_loss: 0.17003 test_loss: 0.11870 \n",
      "[ 19/100] train_loss: 0.14578 valid_loss: 0.16002 test_loss: 0.10850 \n",
      "Validation loss decreased (0.164883 --> 0.160020).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13925 valid_loss: 0.15438 test_loss: 0.11430 \n",
      "Validation loss decreased (0.160020 --> 0.154380).  Saving model ...\n",
      "[ 21/100] train_loss: 0.14313 valid_loss: 0.15319 test_loss: 0.10798 \n",
      "Validation loss decreased (0.154380 --> 0.153186).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13807 valid_loss: 0.15759 test_loss: 0.10946 \n",
      "[ 23/100] train_loss: 0.13495 valid_loss: 0.14680 test_loss: 0.10486 \n",
      "Validation loss decreased (0.153186 --> 0.146801).  Saving model ...\n",
      "[ 24/100] train_loss: 0.13172 valid_loss: 0.14968 test_loss: 0.10864 \n",
      "[ 25/100] train_loss: 0.12857 valid_loss: 0.14810 test_loss: 0.10385 \n",
      "[ 26/100] train_loss: 0.12880 valid_loss: 0.14533 test_loss: 0.10643 \n",
      "Validation loss decreased (0.146801 --> 0.145333).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12768 valid_loss: 0.14473 test_loss: 0.10321 \n",
      "Validation loss decreased (0.145333 --> 0.144729).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12773 valid_loss: 0.13862 test_loss: 0.10035 \n",
      "Validation loss decreased (0.144729 --> 0.138619).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12899 valid_loss: 0.14279 test_loss: 0.10208 \n",
      "[ 30/100] train_loss: 0.12774 valid_loss: 0.13880 test_loss: 0.09770 \n",
      "[ 31/100] train_loss: 0.12486 valid_loss: 0.13493 test_loss: 0.09914 \n",
      "Validation loss decreased (0.138619 --> 0.134930).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12257 valid_loss: 0.13564 test_loss: 0.10184 \n",
      "[ 33/100] train_loss: 0.12336 valid_loss: 0.13580 test_loss: 0.09588 \n",
      "[ 34/100] train_loss: 0.12107 valid_loss: 0.13614 test_loss: 0.10035 \n",
      "[ 35/100] train_loss: 0.12300 valid_loss: 0.13619 test_loss: 0.09779 \n",
      "[ 36/100] train_loss: 0.12394 valid_loss: 0.13092 test_loss: 0.09507 \n",
      "Validation loss decreased (0.134930 --> 0.130916).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11810 valid_loss: 0.12919 test_loss: 0.09845 \n",
      "Validation loss decreased (0.130916 --> 0.129190).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11598 valid_loss: 0.13069 test_loss: 0.09636 \n",
      "[ 39/100] train_loss: 0.11969 valid_loss: 0.13196 test_loss: 0.09582 \n",
      "[ 40/100] train_loss: 0.12083 valid_loss: 0.12840 test_loss: 0.09763 \n",
      "Validation loss decreased (0.129190 --> 0.128399).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11388 valid_loss: 0.13051 test_loss: 0.09776 \n",
      "[ 42/100] train_loss: 0.11605 valid_loss: 0.12553 test_loss: 0.09441 \n",
      "Validation loss decreased (0.128399 --> 0.125529).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11583 valid_loss: 0.12550 test_loss: 0.09351 \n",
      "Validation loss decreased (0.125529 --> 0.125505).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11182 valid_loss: 0.13105 test_loss: 0.09587 \n",
      "[ 45/100] train_loss: 0.11467 valid_loss: 0.13469 test_loss: 0.09823 \n",
      "[ 46/100] train_loss: 0.11642 valid_loss: 0.12849 test_loss: 0.09434 \n",
      "[ 47/100] train_loss: 0.11223 valid_loss: 0.12516 test_loss: 0.09500 \n",
      "Validation loss decreased (0.125505 --> 0.125162).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11007 valid_loss: 0.12906 test_loss: 0.09571 \n",
      "[ 49/100] train_loss: 0.10927 valid_loss: 0.12505 test_loss: 0.09710 \n",
      "Validation loss decreased (0.125162 --> 0.125052).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10916 valid_loss: 0.12405 test_loss: 0.09611 \n",
      "Validation loss decreased (0.125052 --> 0.124049).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11074 valid_loss: 0.12177 test_loss: 0.09247 \n",
      "Validation loss decreased (0.124049 --> 0.121766).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10962 valid_loss: 0.12465 test_loss: 0.09366 \n",
      "[ 53/100] train_loss: 0.10919 valid_loss: 0.12285 test_loss: 0.09412 \n",
      "[ 54/100] train_loss: 0.11135 valid_loss: 0.12006 test_loss: 0.09197 \n",
      "Validation loss decreased (0.121766 --> 0.120063).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10694 valid_loss: 0.11942 test_loss: 0.09060 \n",
      "Validation loss decreased (0.120063 --> 0.119425).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10926 valid_loss: 0.12215 test_loss: 0.09253 \n",
      "[ 57/100] train_loss: 0.10748 valid_loss: 0.12295 test_loss: 0.09127 \n",
      "[ 58/100] train_loss: 0.10538 valid_loss: 0.11883 test_loss: 0.09158 \n",
      "Validation loss decreased (0.119425 --> 0.118833).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10818 valid_loss: 0.11996 test_loss: 0.09603 \n",
      "[ 60/100] train_loss: 0.10470 valid_loss: 0.11873 test_loss: 0.09454 \n",
      "Validation loss decreased (0.118833 --> 0.118727).  Saving model ...\n",
      "[ 61/100] train_loss: 0.10556 valid_loss: 0.11990 test_loss: 0.09218 \n",
      "[ 62/100] train_loss: 0.10840 valid_loss: 0.11979 test_loss: 0.09567 \n",
      "[ 63/100] train_loss: 0.10623 valid_loss: 0.11982 test_loss: 0.09469 \n",
      "[ 64/100] train_loss: 0.10569 valid_loss: 0.12140 test_loss: 0.09612 \n",
      "[ 65/100] train_loss: 0.10488 valid_loss: 0.11703 test_loss: 0.09131 \n",
      "Validation loss decreased (0.118727 --> 0.117029).  Saving model ...\n",
      "[ 66/100] train_loss: 0.09988 valid_loss: 0.11744 test_loss: 0.09224 \n",
      "[ 67/100] train_loss: 0.10315 valid_loss: 0.11728 test_loss: 0.09098 \n",
      "[ 68/100] train_loss: 0.10374 valid_loss: 0.11671 test_loss: 0.09290 \n",
      "Validation loss decreased (0.117029 --> 0.116708).  Saving model ...\n",
      "[ 69/100] train_loss: 0.10117 valid_loss: 0.11537 test_loss: 0.09373 \n",
      "Validation loss decreased (0.116708 --> 0.115373).  Saving model ...\n",
      "[ 70/100] train_loss: 0.10435 valid_loss: 0.11631 test_loss: 0.09361 \n",
      "[ 71/100] train_loss: 0.10175 valid_loss: 0.11498 test_loss: 0.09163 \n",
      "Validation loss decreased (0.115373 --> 0.114980).  Saving model ...\n",
      "[ 72/100] train_loss: 0.09867 valid_loss: 0.11381 test_loss: 0.09025 \n",
      "Validation loss decreased (0.114980 --> 0.113810).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09897 valid_loss: 0.11418 test_loss: 0.09383 \n",
      "[ 74/100] train_loss: 0.10150 valid_loss: 0.11295 test_loss: 0.08870 \n",
      "Validation loss decreased (0.113810 --> 0.112947).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09780 valid_loss: 0.11490 test_loss: 0.09122 \n",
      "[ 76/100] train_loss: 0.09854 valid_loss: 0.11287 test_loss: 0.09191 \n",
      "Validation loss decreased (0.112947 --> 0.112873).  Saving model ...\n",
      "[ 77/100] train_loss: 0.10028 valid_loss: 0.11357 test_loss: 0.08985 \n",
      "[ 78/100] train_loss: 0.09702 valid_loss: 0.11315 test_loss: 0.08715 \n",
      "[ 79/100] train_loss: 0.09823 valid_loss: 0.11957 test_loss: 0.09149 \n",
      "[ 80/100] train_loss: 0.09860 valid_loss: 0.11004 test_loss: 0.08878 \n",
      "Validation loss decreased (0.112873 --> 0.110043).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09971 valid_loss: 0.11113 test_loss: 0.08838 \n",
      "[ 82/100] train_loss: 0.09715 valid_loss: 0.11312 test_loss: 0.09199 \n",
      "[ 83/100] train_loss: 0.09774 valid_loss: 0.11485 test_loss: 0.09312 \n",
      "[ 84/100] train_loss: 0.09751 valid_loss: 0.11906 test_loss: 0.09108 \n",
      "[ 85/100] train_loss: 0.09565 valid_loss: 0.11152 test_loss: 0.09205 \n",
      "[ 86/100] train_loss: 0.09635 valid_loss: 0.11171 test_loss: 0.08869 \n",
      "[ 87/100] train_loss: 0.09639 valid_loss: 0.11018 test_loss: 0.08833 \n",
      "[ 88/100] train_loss: 0.09311 valid_loss: 0.11223 test_loss: 0.09152 \n",
      "[ 89/100] train_loss: 0.09664 valid_loss: 0.11009 test_loss: 0.08846 \n",
      "[ 90/100] train_loss: 0.09587 valid_loss: 0.11004 test_loss: 0.08823 \n",
      "Validation loss decreased (0.110043 --> 0.110037).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09726 valid_loss: 0.11115 test_loss: 0.09238 \n",
      "[ 92/100] train_loss: 0.09654 valid_loss: 0.10826 test_loss: 0.08644 \n",
      "Validation loss decreased (0.110037 --> 0.108259).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09501 valid_loss: 0.10922 test_loss: 0.08750 \n",
      "[ 94/100] train_loss: 0.09047 valid_loss: 0.10730 test_loss: 0.08738 \n",
      "Validation loss decreased (0.108259 --> 0.107296).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09216 valid_loss: 0.11041 test_loss: 0.08518 \n",
      "[ 96/100] train_loss: 0.09293 valid_loss: 0.10739 test_loss: 0.08484 \n",
      "[ 97/100] train_loss: 0.09321 valid_loss: 0.10857 test_loss: 0.08694 \n",
      "[ 98/100] train_loss: 0.09319 valid_loss: 0.11061 test_loss: 0.08708 \n",
      "[ 99/100] train_loss: 0.09304 valid_loss: 0.10837 test_loss: 0.08431 \n",
      "[100/100] train_loss: 0.09304 valid_loss: 0.10601 test_loss: 0.08611 \n",
      "Validation loss decreased (0.107296 --> 0.106011).  Saving model ...\n",
      "TRAINING MODEL 15\n",
      "[  1/100] train_loss: 0.62488 valid_loss: 0.52676 test_loss: 0.47473 \n",
      "Validation loss decreased (inf --> 0.526757).  Saving model ...\n",
      "[  2/100] train_loss: 0.43828 valid_loss: 0.39837 test_loss: 0.32307 \n",
      "Validation loss decreased (0.526757 --> 0.398366).  Saving model ...\n",
      "[  3/100] train_loss: 0.33756 valid_loss: 0.33546 test_loss: 0.25585 \n",
      "Validation loss decreased (0.398366 --> 0.335459).  Saving model ...\n",
      "[  4/100] train_loss: 0.28143 valid_loss: 0.28416 test_loss: 0.20722 \n",
      "Validation loss decreased (0.335459 --> 0.284162).  Saving model ...\n",
      "[  5/100] train_loss: 0.24231 valid_loss: 0.26234 test_loss: 0.19133 \n",
      "Validation loss decreased (0.284162 --> 0.262341).  Saving model ...\n",
      "[  6/100] train_loss: 0.22570 valid_loss: 0.24710 test_loss: 0.17170 \n",
      "Validation loss decreased (0.262341 --> 0.247100).  Saving model ...\n",
      "[  7/100] train_loss: 0.21124 valid_loss: 0.22657 test_loss: 0.16239 \n",
      "Validation loss decreased (0.247100 --> 0.226567).  Saving model ...\n",
      "[  8/100] train_loss: 0.20224 valid_loss: 0.21212 test_loss: 0.14841 \n",
      "Validation loss decreased (0.226567 --> 0.212124).  Saving model ...\n",
      "[  9/100] train_loss: 0.19317 valid_loss: 0.20425 test_loss: 0.14178 \n",
      "Validation loss decreased (0.212124 --> 0.204246).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18660 valid_loss: 0.20112 test_loss: 0.14018 \n",
      "Validation loss decreased (0.204246 --> 0.201117).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17680 valid_loss: 0.20392 test_loss: 0.13957 \n",
      "[ 12/100] train_loss: 0.17160 valid_loss: 0.19286 test_loss: 0.13640 \n",
      "Validation loss decreased (0.201117 --> 0.192860).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16953 valid_loss: 0.18813 test_loss: 0.13245 \n",
      "Validation loss decreased (0.192860 --> 0.188132).  Saving model ...\n",
      "[ 14/100] train_loss: 0.16482 valid_loss: 0.18447 test_loss: 0.12959 \n",
      "Validation loss decreased (0.188132 --> 0.184474).  Saving model ...\n",
      "[ 15/100] train_loss: 0.16425 valid_loss: 0.17622 test_loss: 0.12693 \n",
      "Validation loss decreased (0.184474 --> 0.176216).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15515 valid_loss: 0.19007 test_loss: 0.13234 \n",
      "[ 17/100] train_loss: 0.15358 valid_loss: 0.17357 test_loss: 0.12366 \n",
      "Validation loss decreased (0.176216 --> 0.173566).  Saving model ...\n",
      "[ 18/100] train_loss: 0.15096 valid_loss: 0.17351 test_loss: 0.12009 \n",
      "Validation loss decreased (0.173566 --> 0.173514).  Saving model ...\n",
      "[ 19/100] train_loss: 0.15118 valid_loss: 0.16927 test_loss: 0.12292 \n",
      "Validation loss decreased (0.173514 --> 0.169269).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14118 valid_loss: 0.16522 test_loss: 0.11551 \n",
      "Validation loss decreased (0.169269 --> 0.165221).  Saving model ...\n",
      "[ 21/100] train_loss: 0.14572 valid_loss: 0.16773 test_loss: 0.11625 \n",
      "[ 22/100] train_loss: 0.13923 valid_loss: 0.15113 test_loss: 0.11247 \n",
      "Validation loss decreased (0.165221 --> 0.151132).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13489 valid_loss: 0.15410 test_loss: 0.11186 \n",
      "[ 24/100] train_loss: 0.13461 valid_loss: 0.15664 test_loss: 0.10976 \n",
      "[ 25/100] train_loss: 0.13477 valid_loss: 0.14786 test_loss: 0.10862 \n",
      "Validation loss decreased (0.151132 --> 0.147858).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13407 valid_loss: 0.14744 test_loss: 0.10373 \n",
      "Validation loss decreased (0.147858 --> 0.147438).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13416 valid_loss: 0.15097 test_loss: 0.10663 \n",
      "[ 28/100] train_loss: 0.12931 valid_loss: 0.14674 test_loss: 0.10897 \n",
      "Validation loss decreased (0.147438 --> 0.146736).  Saving model ...\n",
      "[ 29/100] train_loss: 0.13016 valid_loss: 0.14602 test_loss: 0.10265 \n",
      "Validation loss decreased (0.146736 --> 0.146020).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12904 valid_loss: 0.15214 test_loss: 0.10871 \n",
      "[ 31/100] train_loss: 0.12387 valid_loss: 0.14103 test_loss: 0.10232 \n",
      "Validation loss decreased (0.146020 --> 0.141034).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12419 valid_loss: 0.14022 test_loss: 0.10718 \n",
      "Validation loss decreased (0.141034 --> 0.140217).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12246 valid_loss: 0.13934 test_loss: 0.10323 \n",
      "Validation loss decreased (0.140217 --> 0.139339).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12409 valid_loss: 0.14059 test_loss: 0.10355 \n",
      "[ 35/100] train_loss: 0.12584 valid_loss: 0.14536 test_loss: 0.10146 \n",
      "[ 36/100] train_loss: 0.11967 valid_loss: 0.13747 test_loss: 0.10084 \n",
      "Validation loss decreased (0.139339 --> 0.137465).  Saving model ...\n",
      "[ 37/100] train_loss: 0.12233 valid_loss: 0.13997 test_loss: 0.10310 \n",
      "[ 38/100] train_loss: 0.12098 valid_loss: 0.13686 test_loss: 0.09662 \n",
      "Validation loss decreased (0.137465 --> 0.136860).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11446 valid_loss: 0.13218 test_loss: 0.09539 \n",
      "Validation loss decreased (0.136860 --> 0.132177).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11524 valid_loss: 0.13492 test_loss: 0.09883 \n",
      "[ 41/100] train_loss: 0.12336 valid_loss: 0.13575 test_loss: 0.10178 \n",
      "[ 42/100] train_loss: 0.11427 valid_loss: 0.12887 test_loss: 0.09476 \n",
      "Validation loss decreased (0.132177 --> 0.128868).  Saving model ...\n",
      "[ 43/100] train_loss: 0.11599 valid_loss: 0.13129 test_loss: 0.10174 \n",
      "[ 44/100] train_loss: 0.11464 valid_loss: 0.13117 test_loss: 0.09767 \n",
      "[ 45/100] train_loss: 0.11700 valid_loss: 0.13724 test_loss: 0.09976 \n",
      "[ 46/100] train_loss: 0.11485 valid_loss: 0.12821 test_loss: 0.09691 \n",
      "Validation loss decreased (0.128868 --> 0.128206).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11436 valid_loss: 0.12931 test_loss: 0.09999 \n",
      "[ 48/100] train_loss: 0.11188 valid_loss: 0.12849 test_loss: 0.09446 \n",
      "[ 49/100] train_loss: 0.11245 valid_loss: 0.12943 test_loss: 0.09493 \n",
      "[ 50/100] train_loss: 0.11267 valid_loss: 0.12439 test_loss: 0.09274 \n",
      "Validation loss decreased (0.128206 --> 0.124390).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11176 valid_loss: 0.12630 test_loss: 0.09520 \n",
      "[ 52/100] train_loss: 0.11521 valid_loss: 0.12605 test_loss: 0.09851 \n",
      "[ 53/100] train_loss: 0.10759 valid_loss: 0.12638 test_loss: 0.09731 \n",
      "[ 54/100] train_loss: 0.11041 valid_loss: 0.13070 test_loss: 0.09571 \n",
      "[ 55/100] train_loss: 0.11439 valid_loss: 0.12400 test_loss: 0.09393 \n",
      "Validation loss decreased (0.124390 --> 0.124004).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10794 valid_loss: 0.12578 test_loss: 0.09288 \n",
      "[ 57/100] train_loss: 0.10868 valid_loss: 0.12306 test_loss: 0.09204 \n",
      "Validation loss decreased (0.124004 --> 0.123056).  Saving model ...\n",
      "[ 58/100] train_loss: 0.10562 valid_loss: 0.12395 test_loss: 0.09205 \n",
      "[ 59/100] train_loss: 0.10657 valid_loss: 0.12007 test_loss: 0.08913 \n",
      "Validation loss decreased (0.123056 --> 0.120071).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10540 valid_loss: 0.12309 test_loss: 0.09017 \n",
      "[ 61/100] train_loss: 0.11044 valid_loss: 0.12296 test_loss: 0.09568 \n",
      "[ 62/100] train_loss: 0.10582 valid_loss: 0.12110 test_loss: 0.09279 \n",
      "[ 63/100] train_loss: 0.10891 valid_loss: 0.11758 test_loss: 0.09118 \n",
      "Validation loss decreased (0.120071 --> 0.117584).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10341 valid_loss: 0.11861 test_loss: 0.09038 \n",
      "[ 65/100] train_loss: 0.10303 valid_loss: 0.11992 test_loss: 0.09123 \n",
      "[ 66/100] train_loss: 0.10363 valid_loss: 0.12016 test_loss: 0.09047 \n",
      "[ 67/100] train_loss: 0.10513 valid_loss: 0.12012 test_loss: 0.09082 \n",
      "[ 68/100] train_loss: 0.10516 valid_loss: 0.12088 test_loss: 0.09105 \n",
      "[ 69/100] train_loss: 0.10328 valid_loss: 0.11584 test_loss: 0.08695 \n",
      "Validation loss decreased (0.117584 --> 0.115842).  Saving model ...\n",
      "[ 70/100] train_loss: 0.10500 valid_loss: 0.11988 test_loss: 0.09230 \n",
      "[ 71/100] train_loss: 0.10474 valid_loss: 0.12206 test_loss: 0.09214 \n",
      "[ 72/100] train_loss: 0.09914 valid_loss: 0.11561 test_loss: 0.08638 \n",
      "Validation loss decreased (0.115842 --> 0.115607).  Saving model ...\n",
      "[ 73/100] train_loss: 0.10590 valid_loss: 0.11878 test_loss: 0.09021 \n",
      "[ 74/100] train_loss: 0.09876 valid_loss: 0.11825 test_loss: 0.08816 \n",
      "[ 75/100] train_loss: 0.10351 valid_loss: 0.11511 test_loss: 0.08726 \n",
      "Validation loss decreased (0.115607 --> 0.115106).  Saving model ...\n",
      "[ 76/100] train_loss: 0.10074 valid_loss: 0.11326 test_loss: 0.08844 \n",
      "Validation loss decreased (0.115106 --> 0.113259).  Saving model ...\n",
      "[ 77/100] train_loss: 0.09941 valid_loss: 0.11505 test_loss: 0.08746 \n",
      "[ 78/100] train_loss: 0.09960 valid_loss: 0.11328 test_loss: 0.08397 \n",
      "[ 79/100] train_loss: 0.09638 valid_loss: 0.11356 test_loss: 0.08607 \n",
      "[ 80/100] train_loss: 0.09466 valid_loss: 0.11096 test_loss: 0.08516 \n",
      "Validation loss decreased (0.113259 --> 0.110964).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09989 valid_loss: 0.11218 test_loss: 0.08620 \n",
      "[ 82/100] train_loss: 0.09834 valid_loss: 0.11193 test_loss: 0.08527 \n",
      "[ 83/100] train_loss: 0.09807 valid_loss: 0.11154 test_loss: 0.08471 \n",
      "[ 84/100] train_loss: 0.09713 valid_loss: 0.11424 test_loss: 0.08665 \n",
      "[ 85/100] train_loss: 0.09632 valid_loss: 0.11162 test_loss: 0.08382 \n",
      "[ 86/100] train_loss: 0.09591 valid_loss: 0.11212 test_loss: 0.08420 \n",
      "[ 87/100] train_loss: 0.09490 valid_loss: 0.11293 test_loss: 0.08534 \n",
      "[ 88/100] train_loss: 0.09913 valid_loss: 0.10956 test_loss: 0.08432 \n",
      "Validation loss decreased (0.110964 --> 0.109565).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09619 valid_loss: 0.11092 test_loss: 0.08641 \n",
      "[ 90/100] train_loss: 0.09796 valid_loss: 0.10793 test_loss: 0.08472 \n",
      "Validation loss decreased (0.109565 --> 0.107931).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09672 valid_loss: 0.10819 test_loss: 0.08407 \n",
      "[ 92/100] train_loss: 0.09239 valid_loss: 0.11207 test_loss: 0.08604 \n",
      "[ 93/100] train_loss: 0.09199 valid_loss: 0.10993 test_loss: 0.08335 \n",
      "[ 94/100] train_loss: 0.09571 valid_loss: 0.11334 test_loss: 0.08793 \n",
      "[ 95/100] train_loss: 0.09372 valid_loss: 0.10966 test_loss: 0.08612 \n",
      "[ 96/100] train_loss: 0.09551 valid_loss: 0.10921 test_loss: 0.08666 \n",
      "[ 97/100] train_loss: 0.09204 valid_loss: 0.10852 test_loss: 0.08555 \n",
      "[ 98/100] train_loss: 0.09119 valid_loss: 0.10761 test_loss: 0.08417 \n",
      "Validation loss decreased (0.107931 --> 0.107611).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09468 valid_loss: 0.10889 test_loss: 0.08545 \n",
      "[100/100] train_loss: 0.09355 valid_loss: 0.10864 test_loss: 0.08205 \n",
      "TRAINING MODEL 16\n",
      "[  1/100] train_loss: 0.66153 valid_loss: 0.58853 test_loss: 0.55205 \n",
      "Validation loss decreased (inf --> 0.588527).  Saving model ...\n",
      "[  2/100] train_loss: 0.48935 valid_loss: 0.42602 test_loss: 0.36212 \n",
      "Validation loss decreased (0.588527 --> 0.426022).  Saving model ...\n",
      "[  3/100] train_loss: 0.36737 valid_loss: 0.34835 test_loss: 0.28557 \n",
      "Validation loss decreased (0.426022 --> 0.348353).  Saving model ...\n",
      "[  4/100] train_loss: 0.29560 valid_loss: 0.29691 test_loss: 0.22573 \n",
      "Validation loss decreased (0.348353 --> 0.296906).  Saving model ...\n",
      "[  5/100] train_loss: 0.25280 valid_loss: 0.26211 test_loss: 0.19299 \n",
      "Validation loss decreased (0.296906 --> 0.262107).  Saving model ...\n",
      "[  6/100] train_loss: 0.23096 valid_loss: 0.24615 test_loss: 0.17455 \n",
      "Validation loss decreased (0.262107 --> 0.246152).  Saving model ...\n",
      "[  7/100] train_loss: 0.21542 valid_loss: 0.22632 test_loss: 0.15881 \n",
      "Validation loss decreased (0.246152 --> 0.226324).  Saving model ...\n",
      "[  8/100] train_loss: 0.20104 valid_loss: 0.21802 test_loss: 0.15243 \n",
      "Validation loss decreased (0.226324 --> 0.218019).  Saving model ...\n",
      "[  9/100] train_loss: 0.18708 valid_loss: 0.20782 test_loss: 0.14621 \n",
      "Validation loss decreased (0.218019 --> 0.207817).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18252 valid_loss: 0.20007 test_loss: 0.13931 \n",
      "Validation loss decreased (0.207817 --> 0.200069).  Saving model ...\n",
      "[ 11/100] train_loss: 0.17428 valid_loss: 0.19664 test_loss: 0.13402 \n",
      "Validation loss decreased (0.200069 --> 0.196638).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16973 valid_loss: 0.18595 test_loss: 0.13347 \n",
      "Validation loss decreased (0.196638 --> 0.185949).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16320 valid_loss: 0.17939 test_loss: 0.13261 \n",
      "Validation loss decreased (0.185949 --> 0.179392).  Saving model ...\n",
      "[ 14/100] train_loss: 0.16037 valid_loss: 0.17386 test_loss: 0.12418 \n",
      "Validation loss decreased (0.179392 --> 0.173856).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15695 valid_loss: 0.17085 test_loss: 0.12132 \n",
      "Validation loss decreased (0.173856 --> 0.170853).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15279 valid_loss: 0.16935 test_loss: 0.12320 \n",
      "Validation loss decreased (0.170853 --> 0.169348).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14990 valid_loss: 0.16311 test_loss: 0.11796 \n",
      "Validation loss decreased (0.169348 --> 0.163110).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14717 valid_loss: 0.15991 test_loss: 0.11618 \n",
      "Validation loss decreased (0.163110 --> 0.159910).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14194 valid_loss: 0.15899 test_loss: 0.11639 \n",
      "Validation loss decreased (0.159910 --> 0.158986).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14358 valid_loss: 0.16103 test_loss: 0.11764 \n",
      "[ 21/100] train_loss: 0.13989 valid_loss: 0.15837 test_loss: 0.12763 \n",
      "Validation loss decreased (0.158986 --> 0.158374).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13974 valid_loss: 0.15172 test_loss: 0.11097 \n",
      "Validation loss decreased (0.158374 --> 0.151715).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13725 valid_loss: 0.15616 test_loss: 0.11610 \n",
      "[ 24/100] train_loss: 0.13373 valid_loss: 0.14856 test_loss: 0.11223 \n",
      "Validation loss decreased (0.151715 --> 0.148563).  Saving model ...\n",
      "[ 25/100] train_loss: 0.13385 valid_loss: 0.14727 test_loss: 0.11478 \n",
      "Validation loss decreased (0.148563 --> 0.147268).  Saving model ...\n",
      "[ 26/100] train_loss: 0.13203 valid_loss: 0.14606 test_loss: 0.11259 \n",
      "Validation loss decreased (0.147268 --> 0.146062).  Saving model ...\n",
      "[ 27/100] train_loss: 0.13005 valid_loss: 0.14393 test_loss: 0.11019 \n",
      "Validation loss decreased (0.146062 --> 0.143935).  Saving model ...\n",
      "[ 28/100] train_loss: 0.13064 valid_loss: 0.14327 test_loss: 0.10488 \n",
      "Validation loss decreased (0.143935 --> 0.143269).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12740 valid_loss: 0.14309 test_loss: 0.10916 \n",
      "Validation loss decreased (0.143269 --> 0.143093).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12519 valid_loss: 0.13929 test_loss: 0.10874 \n",
      "Validation loss decreased (0.143093 --> 0.139292).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12895 valid_loss: 0.14128 test_loss: 0.11164 \n",
      "[ 32/100] train_loss: 0.12795 valid_loss: 0.14058 test_loss: 0.10219 \n",
      "[ 33/100] train_loss: 0.12476 valid_loss: 0.13897 test_loss: 0.10321 \n",
      "Validation loss decreased (0.139292 --> 0.138968).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12467 valid_loss: 0.13777 test_loss: 0.10210 \n",
      "Validation loss decreased (0.138968 --> 0.137772).  Saving model ...\n",
      "[ 35/100] train_loss: 0.12532 valid_loss: 0.13654 test_loss: 0.10897 \n",
      "Validation loss decreased (0.137772 --> 0.136539).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11964 valid_loss: 0.13659 test_loss: 0.09733 \n",
      "[ 37/100] train_loss: 0.12235 valid_loss: 0.13391 test_loss: 0.09969 \n",
      "Validation loss decreased (0.136539 --> 0.133911).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11776 valid_loss: 0.13596 test_loss: 0.10237 \n",
      "[ 39/100] train_loss: 0.12158 valid_loss: 0.13108 test_loss: 0.09787 \n",
      "Validation loss decreased (0.133911 --> 0.131078).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11801 valid_loss: 0.13734 test_loss: 0.09838 \n",
      "[ 41/100] train_loss: 0.11837 valid_loss: 0.13318 test_loss: 0.09690 \n",
      "[ 42/100] train_loss: 0.11905 valid_loss: 0.13278 test_loss: 0.09784 \n",
      "[ 43/100] train_loss: 0.11675 valid_loss: 0.13107 test_loss: 0.09620 \n",
      "Validation loss decreased (0.131078 --> 0.131065).  Saving model ...\n",
      "[ 44/100] train_loss: 0.11839 valid_loss: 0.12969 test_loss: 0.09357 \n",
      "Validation loss decreased (0.131065 --> 0.129689).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11648 valid_loss: 0.13231 test_loss: 0.09990 \n",
      "[ 46/100] train_loss: 0.11521 valid_loss: 0.13003 test_loss: 0.10095 \n",
      "[ 47/100] train_loss: 0.11335 valid_loss: 0.12442 test_loss: 0.09341 \n",
      "Validation loss decreased (0.129689 --> 0.124417).  Saving model ...\n",
      "[ 48/100] train_loss: 0.11073 valid_loss: 0.12657 test_loss: 0.09480 \n",
      "[ 49/100] train_loss: 0.11280 valid_loss: 0.12744 test_loss: 0.09484 \n",
      "[ 50/100] train_loss: 0.11048 valid_loss: 0.12533 test_loss: 0.09852 \n",
      "[ 51/100] train_loss: 0.11085 valid_loss: 0.12364 test_loss: 0.09171 \n",
      "Validation loss decreased (0.124417 --> 0.123636).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10890 valid_loss: 0.12418 test_loss: 0.09481 \n",
      "[ 53/100] train_loss: 0.10767 valid_loss: 0.12197 test_loss: 0.09578 \n",
      "Validation loss decreased (0.123636 --> 0.121973).  Saving model ...\n",
      "[ 54/100] train_loss: 0.10590 valid_loss: 0.12222 test_loss: 0.09082 \n",
      "[ 55/100] train_loss: 0.10679 valid_loss: 0.12011 test_loss: 0.09372 \n",
      "Validation loss decreased (0.121973 --> 0.120109).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10724 valid_loss: 0.12389 test_loss: 0.09394 \n",
      "[ 57/100] train_loss: 0.10850 valid_loss: 0.12215 test_loss: 0.09418 \n",
      "[ 58/100] train_loss: 0.10865 valid_loss: 0.11871 test_loss: 0.09594 \n",
      "Validation loss decreased (0.120109 --> 0.118712).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10530 valid_loss: 0.11994 test_loss: 0.08928 \n",
      "[ 60/100] train_loss: 0.10405 valid_loss: 0.12314 test_loss: 0.09179 \n",
      "[ 61/100] train_loss: 0.10814 valid_loss: 0.11953 test_loss: 0.08946 \n",
      "[ 62/100] train_loss: 0.10820 valid_loss: 0.12485 test_loss: 0.09618 \n",
      "[ 63/100] train_loss: 0.10260 valid_loss: 0.11784 test_loss: 0.08962 \n",
      "Validation loss decreased (0.118712 --> 0.117845).  Saving model ...\n",
      "[ 64/100] train_loss: 0.10377 valid_loss: 0.11810 test_loss: 0.09001 \n",
      "[ 65/100] train_loss: 0.10449 valid_loss: 0.12001 test_loss: 0.08746 \n",
      "[ 66/100] train_loss: 0.10231 valid_loss: 0.11948 test_loss: 0.08632 \n",
      "[ 67/100] train_loss: 0.09938 valid_loss: 0.11432 test_loss: 0.08795 \n",
      "Validation loss decreased (0.117845 --> 0.114318).  Saving model ...\n",
      "[ 68/100] train_loss: 0.10025 valid_loss: 0.11605 test_loss: 0.08785 \n",
      "[ 69/100] train_loss: 0.10219 valid_loss: 0.11533 test_loss: 0.09111 \n",
      "[ 70/100] train_loss: 0.10151 valid_loss: 0.11501 test_loss: 0.08739 \n",
      "[ 71/100] train_loss: 0.09815 valid_loss: 0.11487 test_loss: 0.08713 \n",
      "[ 72/100] train_loss: 0.09919 valid_loss: 0.11415 test_loss: 0.08494 \n",
      "Validation loss decreased (0.114318 --> 0.114147).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09572 valid_loss: 0.11400 test_loss: 0.09001 \n",
      "Validation loss decreased (0.114147 --> 0.113999).  Saving model ...\n",
      "[ 74/100] train_loss: 0.09782 valid_loss: 0.11285 test_loss: 0.08654 \n",
      "Validation loss decreased (0.113999 --> 0.112852).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09645 valid_loss: 0.11676 test_loss: 0.08664 \n",
      "[ 76/100] train_loss: 0.09954 valid_loss: 0.11401 test_loss: 0.08508 \n",
      "[ 77/100] train_loss: 0.09938 valid_loss: 0.11218 test_loss: 0.08515 \n",
      "Validation loss decreased (0.112852 --> 0.112183).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09898 valid_loss: 0.11091 test_loss: 0.08685 \n",
      "Validation loss decreased (0.112183 --> 0.110911).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09993 valid_loss: 0.11256 test_loss: 0.08567 \n",
      "[ 80/100] train_loss: 0.09471 valid_loss: 0.11183 test_loss: 0.08567 \n",
      "[ 81/100] train_loss: 0.09610 valid_loss: 0.10933 test_loss: 0.08238 \n",
      "Validation loss decreased (0.110911 --> 0.109327).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09755 valid_loss: 0.11021 test_loss: 0.08144 \n",
      "[ 83/100] train_loss: 0.09726 valid_loss: 0.10998 test_loss: 0.08274 \n",
      "[ 84/100] train_loss: 0.09474 valid_loss: 0.11261 test_loss: 0.08657 \n",
      "[ 85/100] train_loss: 0.09462 valid_loss: 0.10954 test_loss: 0.08503 \n",
      "[ 86/100] train_loss: 0.09654 valid_loss: 0.10945 test_loss: 0.08359 \n",
      "[ 87/100] train_loss: 0.09421 valid_loss: 0.10962 test_loss: 0.08292 \n",
      "[ 88/100] train_loss: 0.09534 valid_loss: 0.10820 test_loss: 0.08481 \n",
      "Validation loss decreased (0.109327 --> 0.108204).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09605 valid_loss: 0.10895 test_loss: 0.08300 \n",
      "[ 90/100] train_loss: 0.09150 valid_loss: 0.11135 test_loss: 0.08616 \n",
      "[ 91/100] train_loss: 0.09349 valid_loss: 0.10965 test_loss: 0.08388 \n",
      "[ 92/100] train_loss: 0.09636 valid_loss: 0.10808 test_loss: 0.08110 \n",
      "Validation loss decreased (0.108204 --> 0.108085).  Saving model ...\n",
      "[ 93/100] train_loss: 0.09271 valid_loss: 0.11016 test_loss: 0.08162 \n",
      "[ 94/100] train_loss: 0.09163 valid_loss: 0.10765 test_loss: 0.08260 \n",
      "Validation loss decreased (0.108085 --> 0.107653).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09152 valid_loss: 0.10683 test_loss: 0.08337 \n",
      "Validation loss decreased (0.107653 --> 0.106832).  Saving model ...\n",
      "[ 96/100] train_loss: 0.09366 valid_loss: 0.10857 test_loss: 0.08218 \n",
      "[ 97/100] train_loss: 0.09054 valid_loss: 0.10626 test_loss: 0.08120 \n",
      "Validation loss decreased (0.106832 --> 0.106256).  Saving model ...\n",
      "[ 98/100] train_loss: 0.09289 valid_loss: 0.10515 test_loss: 0.08307 \n",
      "Validation loss decreased (0.106256 --> 0.105154).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09189 valid_loss: 0.10429 test_loss: 0.08179 \n",
      "Validation loss decreased (0.105154 --> 0.104286).  Saving model ...\n",
      "[100/100] train_loss: 0.09107 valid_loss: 0.10779 test_loss: 0.08380 \n",
      "TRAINING MODEL 17\n",
      "[  1/100] train_loss: 0.61198 valid_loss: 0.53975 test_loss: 0.49636 \n",
      "Validation loss decreased (inf --> 0.539747).  Saving model ...\n",
      "[  2/100] train_loss: 0.43823 valid_loss: 0.39271 test_loss: 0.31586 \n",
      "Validation loss decreased (0.539747 --> 0.392712).  Saving model ...\n",
      "[  3/100] train_loss: 0.32877 valid_loss: 0.32959 test_loss: 0.24815 \n",
      "Validation loss decreased (0.392712 --> 0.329586).  Saving model ...\n",
      "[  4/100] train_loss: 0.27088 valid_loss: 0.28294 test_loss: 0.20649 \n",
      "Validation loss decreased (0.329586 --> 0.282943).  Saving model ...\n",
      "[  5/100] train_loss: 0.23904 valid_loss: 0.26486 test_loss: 0.18519 \n",
      "Validation loss decreased (0.282943 --> 0.264856).  Saving model ...\n",
      "[  6/100] train_loss: 0.22288 valid_loss: 0.24269 test_loss: 0.16562 \n",
      "Validation loss decreased (0.264856 --> 0.242691).  Saving model ...\n",
      "[  7/100] train_loss: 0.20240 valid_loss: 0.22963 test_loss: 0.15728 \n",
      "Validation loss decreased (0.242691 --> 0.229625).  Saving model ...\n",
      "[  8/100] train_loss: 0.19267 valid_loss: 0.21001 test_loss: 0.14646 \n",
      "Validation loss decreased (0.229625 --> 0.210010).  Saving model ...\n",
      "[  9/100] train_loss: 0.18272 valid_loss: 0.20211 test_loss: 0.13932 \n",
      "Validation loss decreased (0.210010 --> 0.202110).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17531 valid_loss: 0.20067 test_loss: 0.14083 \n",
      "Validation loss decreased (0.202110 --> 0.200669).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16909 valid_loss: 0.19128 test_loss: 0.13830 \n",
      "Validation loss decreased (0.200669 --> 0.191276).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16649 valid_loss: 0.17893 test_loss: 0.12761 \n",
      "Validation loss decreased (0.191276 --> 0.178931).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15988 valid_loss: 0.19284 test_loss: 0.13494 \n",
      "[ 14/100] train_loss: 0.15255 valid_loss: 0.17691 test_loss: 0.12837 \n",
      "Validation loss decreased (0.178931 --> 0.176909).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15234 valid_loss: 0.17910 test_loss: 0.12400 \n",
      "[ 16/100] train_loss: 0.14649 valid_loss: 0.16666 test_loss: 0.12869 \n",
      "Validation loss decreased (0.176909 --> 0.166660).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14717 valid_loss: 0.16331 test_loss: 0.11943 \n",
      "Validation loss decreased (0.166660 --> 0.163313).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14276 valid_loss: 0.16603 test_loss: 0.11982 \n",
      "[ 19/100] train_loss: 0.14142 valid_loss: 0.15975 test_loss: 0.11827 \n",
      "Validation loss decreased (0.163313 --> 0.159750).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13991 valid_loss: 0.15014 test_loss: 0.11003 \n",
      "Validation loss decreased (0.159750 --> 0.150137).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13344 valid_loss: 0.15865 test_loss: 0.11390 \n",
      "[ 22/100] train_loss: 0.13454 valid_loss: 0.15792 test_loss: 0.12039 \n",
      "[ 23/100] train_loss: 0.13624 valid_loss: 0.15111 test_loss: 0.11489 \n",
      "[ 24/100] train_loss: 0.13500 valid_loss: 0.15150 test_loss: 0.10893 \n",
      "[ 25/100] train_loss: 0.12739 valid_loss: 0.15246 test_loss: 0.11473 \n",
      "[ 26/100] train_loss: 0.12798 valid_loss: 0.14543 test_loss: 0.10868 \n",
      "Validation loss decreased (0.150137 --> 0.145435).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12799 valid_loss: 0.14749 test_loss: 0.10881 \n",
      "[ 28/100] train_loss: 0.12196 valid_loss: 0.14216 test_loss: 0.10874 \n",
      "Validation loss decreased (0.145435 --> 0.142165).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12666 valid_loss: 0.14460 test_loss: 0.11231 \n",
      "[ 30/100] train_loss: 0.12333 valid_loss: 0.13934 test_loss: 0.11108 \n",
      "Validation loss decreased (0.142165 --> 0.139336).  Saving model ...\n",
      "[ 31/100] train_loss: 0.12858 valid_loss: 0.14651 test_loss: 0.11083 \n",
      "[ 32/100] train_loss: 0.12144 valid_loss: 0.13664 test_loss: 0.10541 \n",
      "Validation loss decreased (0.139336 --> 0.136642).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12414 valid_loss: 0.13877 test_loss: 0.10349 \n",
      "[ 34/100] train_loss: 0.12213 valid_loss: 0.13420 test_loss: 0.10327 \n",
      "Validation loss decreased (0.136642 --> 0.134197).  Saving model ...\n",
      "[ 35/100] train_loss: 0.12041 valid_loss: 0.13680 test_loss: 0.10906 \n",
      "[ 36/100] train_loss: 0.11924 valid_loss: 0.13338 test_loss: 0.10384 \n",
      "Validation loss decreased (0.134197 --> 0.133385).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11968 valid_loss: 0.13919 test_loss: 0.11314 \n",
      "[ 38/100] train_loss: 0.11999 valid_loss: 0.13290 test_loss: 0.09912 \n",
      "Validation loss decreased (0.133385 --> 0.132902).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11702 valid_loss: 0.13134 test_loss: 0.09950 \n",
      "Validation loss decreased (0.132902 --> 0.131345).  Saving model ...\n",
      "[ 40/100] train_loss: 0.11770 valid_loss: 0.13139 test_loss: 0.09912 \n",
      "[ 41/100] train_loss: 0.11469 valid_loss: 0.13101 test_loss: 0.09940 \n",
      "Validation loss decreased (0.131345 --> 0.131005).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11729 valid_loss: 0.13160 test_loss: 0.10527 \n",
      "[ 43/100] train_loss: 0.10987 valid_loss: 0.13549 test_loss: 0.10194 \n",
      "[ 44/100] train_loss: 0.11757 valid_loss: 0.12772 test_loss: 0.09929 \n",
      "Validation loss decreased (0.131005 --> 0.127715).  Saving model ...\n",
      "[ 45/100] train_loss: 0.11524 valid_loss: 0.12964 test_loss: 0.09658 \n",
      "[ 46/100] train_loss: 0.11199 valid_loss: 0.12670 test_loss: 0.09743 \n",
      "Validation loss decreased (0.127715 --> 0.126696).  Saving model ...\n",
      "[ 47/100] train_loss: 0.11109 valid_loss: 0.12434 test_loss: 0.09772 \n",
      "Validation loss decreased (0.126696 --> 0.124341).  Saving model ...\n",
      "[ 48/100] train_loss: 0.10855 valid_loss: 0.12780 test_loss: 0.10422 \n",
      "[ 49/100] train_loss: 0.10764 valid_loss: 0.12622 test_loss: 0.10227 \n",
      "[ 50/100] train_loss: 0.11298 valid_loss: 0.12180 test_loss: 0.09473 \n",
      "Validation loss decreased (0.124341 --> 0.121801).  Saving model ...\n",
      "[ 51/100] train_loss: 0.11030 valid_loss: 0.12215 test_loss: 0.10018 \n",
      "[ 52/100] train_loss: 0.10801 valid_loss: 0.12507 test_loss: 0.09638 \n",
      "[ 53/100] train_loss: 0.10720 valid_loss: 0.11971 test_loss: 0.09375 \n",
      "Validation loss decreased (0.121801 --> 0.119715).  Saving model ...\n",
      "[ 54/100] train_loss: 0.11055 valid_loss: 0.12410 test_loss: 0.09779 \n",
      "[ 55/100] train_loss: 0.10292 valid_loss: 0.12226 test_loss: 0.09643 \n",
      "[ 56/100] train_loss: 0.10737 valid_loss: 0.12358 test_loss: 0.09927 \n",
      "[ 57/100] train_loss: 0.10790 valid_loss: 0.12032 test_loss: 0.09419 \n",
      "[ 58/100] train_loss: 0.10403 valid_loss: 0.11950 test_loss: 0.09527 \n",
      "Validation loss decreased (0.119715 --> 0.119497).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10852 valid_loss: 0.11869 test_loss: 0.09485 \n",
      "Validation loss decreased (0.119497 --> 0.118686).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10395 valid_loss: 0.12090 test_loss: 0.09341 \n",
      "[ 61/100] train_loss: 0.10540 valid_loss: 0.12302 test_loss: 0.10270 \n",
      "[ 62/100] train_loss: 0.10269 valid_loss: 0.11864 test_loss: 0.09504 \n",
      "Validation loss decreased (0.118686 --> 0.118642).  Saving model ...\n",
      "[ 63/100] train_loss: 0.10061 valid_loss: 0.11906 test_loss: 0.09313 \n",
      "[ 64/100] train_loss: 0.10579 valid_loss: 0.11989 test_loss: 0.09691 \n",
      "[ 65/100] train_loss: 0.10233 valid_loss: 0.11960 test_loss: 0.09447 \n",
      "[ 66/100] train_loss: 0.10212 valid_loss: 0.11939 test_loss: 0.09585 \n",
      "[ 67/100] train_loss: 0.10420 valid_loss: 0.11866 test_loss: 0.09694 \n",
      "[ 68/100] train_loss: 0.09898 valid_loss: 0.11525 test_loss: 0.09243 \n",
      "Validation loss decreased (0.118642 --> 0.115249).  Saving model ...\n",
      "[ 69/100] train_loss: 0.10135 valid_loss: 0.11626 test_loss: 0.09128 \n",
      "[ 70/100] train_loss: 0.09773 valid_loss: 0.11483 test_loss: 0.09211 \n",
      "Validation loss decreased (0.115249 --> 0.114825).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09913 valid_loss: 0.11850 test_loss: 0.09684 \n",
      "[ 72/100] train_loss: 0.10064 valid_loss: 0.11199 test_loss: 0.09405 \n",
      "Validation loss decreased (0.114825 --> 0.111988).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09833 valid_loss: 0.11821 test_loss: 0.09201 \n",
      "[ 74/100] train_loss: 0.09882 valid_loss: 0.11561 test_loss: 0.09888 \n",
      "[ 75/100] train_loss: 0.10161 valid_loss: 0.11595 test_loss: 0.09453 \n",
      "[ 76/100] train_loss: 0.10096 valid_loss: 0.11475 test_loss: 0.09440 \n",
      "[ 77/100] train_loss: 0.09848 valid_loss: 0.11798 test_loss: 0.09413 \n",
      "[ 78/100] train_loss: 0.09797 valid_loss: 0.11234 test_loss: 0.08888 \n",
      "[ 79/100] train_loss: 0.09582 valid_loss: 0.11404 test_loss: 0.09205 \n",
      "[ 80/100] train_loss: 0.09433 valid_loss: 0.11244 test_loss: 0.09328 \n",
      "[ 81/100] train_loss: 0.09720 valid_loss: 0.11361 test_loss: 0.09600 \n",
      "[ 82/100] train_loss: 0.09538 valid_loss: 0.12786 test_loss: 0.09781 \n",
      "[ 83/100] train_loss: 0.09394 valid_loss: 0.11267 test_loss: 0.09484 \n",
      "[ 84/100] train_loss: 0.09839 valid_loss: 0.11459 test_loss: 0.09973 \n",
      "[ 85/100] train_loss: 0.09568 valid_loss: 0.11308 test_loss: 0.09422 \n",
      "[ 86/100] train_loss: 0.09202 valid_loss: 0.11338 test_loss: 0.09555 \n",
      "[ 87/100] train_loss: 0.09536 valid_loss: 0.11036 test_loss: 0.08803 \n",
      "Validation loss decreased (0.111988 --> 0.110359).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09151 valid_loss: 0.11164 test_loss: 0.09120 \n",
      "[ 89/100] train_loss: 0.09184 valid_loss: 0.10906 test_loss: 0.09078 \n",
      "Validation loss decreased (0.110359 --> 0.109063).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09508 valid_loss: 0.10900 test_loss: 0.09247 \n",
      "Validation loss decreased (0.109063 --> 0.109002).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09310 valid_loss: 0.10895 test_loss: 0.09102 \n",
      "Validation loss decreased (0.109002 --> 0.108947).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09339 valid_loss: 0.11038 test_loss: 0.09476 \n",
      "[ 93/100] train_loss: 0.09378 valid_loss: 0.11818 test_loss: 0.09429 \n",
      "[ 94/100] train_loss: 0.09368 valid_loss: 0.10717 test_loss: 0.08984 \n",
      "Validation loss decreased (0.108947 --> 0.107165).  Saving model ...\n",
      "[ 95/100] train_loss: 0.09086 valid_loss: 0.10818 test_loss: 0.09073 \n",
      "[ 96/100] train_loss: 0.09243 valid_loss: 0.10885 test_loss: 0.09206 \n",
      "[ 97/100] train_loss: 0.08817 valid_loss: 0.10906 test_loss: 0.09366 \n",
      "[ 98/100] train_loss: 0.09102 valid_loss: 0.10865 test_loss: 0.09167 \n",
      "[ 99/100] train_loss: 0.08983 valid_loss: 0.11140 test_loss: 0.08961 \n",
      "[100/100] train_loss: 0.09338 valid_loss: 0.10854 test_loss: 0.09363 \n",
      "TRAINING MODEL 18\n",
      "[  1/100] train_loss: 0.61628 valid_loss: 0.52572 test_loss: 0.47473 \n",
      "Validation loss decreased (inf --> 0.525722).  Saving model ...\n",
      "[  2/100] train_loss: 0.43716 valid_loss: 0.39560 test_loss: 0.33023 \n",
      "Validation loss decreased (0.525722 --> 0.395602).  Saving model ...\n",
      "[  3/100] train_loss: 0.33636 valid_loss: 0.32797 test_loss: 0.25510 \n",
      "Validation loss decreased (0.395602 --> 0.327966).  Saving model ...\n",
      "[  4/100] train_loss: 0.27746 valid_loss: 0.28451 test_loss: 0.21200 \n",
      "Validation loss decreased (0.327966 --> 0.284511).  Saving model ...\n",
      "[  5/100] train_loss: 0.23849 valid_loss: 0.25591 test_loss: 0.18212 \n",
      "Validation loss decreased (0.284511 --> 0.255907).  Saving model ...\n",
      "[  6/100] train_loss: 0.22216 valid_loss: 0.23558 test_loss: 0.16521 \n",
      "Validation loss decreased (0.255907 --> 0.235580).  Saving model ...\n",
      "[  7/100] train_loss: 0.20521 valid_loss: 0.22527 test_loss: 0.15390 \n",
      "Validation loss decreased (0.235580 --> 0.225272).  Saving model ...\n",
      "[  8/100] train_loss: 0.19231 valid_loss: 0.21197 test_loss: 0.14736 \n",
      "Validation loss decreased (0.225272 --> 0.211965).  Saving model ...\n",
      "[  9/100] train_loss: 0.18673 valid_loss: 0.20034 test_loss: 0.14220 \n",
      "Validation loss decreased (0.211965 --> 0.200341).  Saving model ...\n",
      "[ 10/100] train_loss: 0.18101 valid_loss: 0.20125 test_loss: 0.14366 \n",
      "[ 11/100] train_loss: 0.17088 valid_loss: 0.19452 test_loss: 0.13395 \n",
      "Validation loss decreased (0.200341 --> 0.194518).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16983 valid_loss: 0.18676 test_loss: 0.13014 \n",
      "Validation loss decreased (0.194518 --> 0.186764).  Saving model ...\n",
      "[ 13/100] train_loss: 0.16699 valid_loss: 0.18511 test_loss: 0.12755 \n",
      "Validation loss decreased (0.186764 --> 0.185113).  Saving model ...\n",
      "[ 14/100] train_loss: 0.16004 valid_loss: 0.17784 test_loss: 0.12687 \n",
      "Validation loss decreased (0.185113 --> 0.177845).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15432 valid_loss: 0.17702 test_loss: 0.12941 \n",
      "Validation loss decreased (0.177845 --> 0.177017).  Saving model ...\n",
      "[ 16/100] train_loss: 0.15463 valid_loss: 0.17051 test_loss: 0.11857 \n",
      "Validation loss decreased (0.177017 --> 0.170511).  Saving model ...\n",
      "[ 17/100] train_loss: 0.15282 valid_loss: 0.16674 test_loss: 0.11747 \n",
      "Validation loss decreased (0.170511 --> 0.166743).  Saving model ...\n",
      "[ 18/100] train_loss: 0.14514 valid_loss: 0.16348 test_loss: 0.11768 \n",
      "Validation loss decreased (0.166743 --> 0.163483).  Saving model ...\n",
      "[ 19/100] train_loss: 0.14323 valid_loss: 0.15875 test_loss: 0.11020 \n",
      "Validation loss decreased (0.163483 --> 0.158748).  Saving model ...\n",
      "[ 20/100] train_loss: 0.14142 valid_loss: 0.16241 test_loss: 0.11115 \n",
      "[ 21/100] train_loss: 0.14027 valid_loss: 0.15334 test_loss: 0.10876 \n",
      "Validation loss decreased (0.158748 --> 0.153338).  Saving model ...\n",
      "[ 22/100] train_loss: 0.13551 valid_loss: 0.14876 test_loss: 0.10359 \n",
      "Validation loss decreased (0.153338 --> 0.148762).  Saving model ...\n",
      "[ 23/100] train_loss: 0.13502 valid_loss: 0.14832 test_loss: 0.10373 \n",
      "Validation loss decreased (0.148762 --> 0.148320).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12965 valid_loss: 0.14881 test_loss: 0.10205 \n",
      "[ 25/100] train_loss: 0.13586 valid_loss: 0.15121 test_loss: 0.10323 \n",
      "[ 26/100] train_loss: 0.13237 valid_loss: 0.14583 test_loss: 0.09947 \n",
      "Validation loss decreased (0.148320 --> 0.145830).  Saving model ...\n",
      "[ 27/100] train_loss: 0.12961 valid_loss: 0.14213 test_loss: 0.09934 \n",
      "Validation loss decreased (0.145830 --> 0.142131).  Saving model ...\n",
      "[ 28/100] train_loss: 0.12490 valid_loss: 0.14133 test_loss: 0.10057 \n",
      "Validation loss decreased (0.142131 --> 0.141326).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12851 valid_loss: 0.13918 test_loss: 0.10032 \n",
      "Validation loss decreased (0.141326 --> 0.139184).  Saving model ...\n",
      "[ 30/100] train_loss: 0.12499 valid_loss: 0.14218 test_loss: 0.10112 \n",
      "[ 31/100] train_loss: 0.12384 valid_loss: 0.13793 test_loss: 0.10227 \n",
      "Validation loss decreased (0.139184 --> 0.137931).  Saving model ...\n",
      "[ 32/100] train_loss: 0.12123 valid_loss: 0.13791 test_loss: 0.09597 \n",
      "Validation loss decreased (0.137931 --> 0.137906).  Saving model ...\n",
      "[ 33/100] train_loss: 0.12071 valid_loss: 0.13669 test_loss: 0.09842 \n",
      "Validation loss decreased (0.137906 --> 0.136692).  Saving model ...\n",
      "[ 34/100] train_loss: 0.12138 valid_loss: 0.13411 test_loss: 0.10018 \n",
      "Validation loss decreased (0.136692 --> 0.134106).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11859 valid_loss: 0.13359 test_loss: 0.09458 \n",
      "Validation loss decreased (0.134106 --> 0.133592).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11786 valid_loss: 0.13305 test_loss: 0.09647 \n",
      "Validation loss decreased (0.133592 --> 0.133053).  Saving model ...\n",
      "[ 37/100] train_loss: 0.12139 valid_loss: 0.13262 test_loss: 0.09648 \n",
      "Validation loss decreased (0.133053 --> 0.132625).  Saving model ...\n",
      "[ 38/100] train_loss: 0.11829 valid_loss: 0.13024 test_loss: 0.09226 \n",
      "Validation loss decreased (0.132625 --> 0.130243).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11796 valid_loss: 0.13584 test_loss: 0.09506 \n",
      "[ 40/100] train_loss: 0.11526 valid_loss: 0.12836 test_loss: 0.09502 \n",
      "Validation loss decreased (0.130243 --> 0.128357).  Saving model ...\n",
      "[ 41/100] train_loss: 0.11482 valid_loss: 0.12703 test_loss: 0.09294 \n",
      "Validation loss decreased (0.128357 --> 0.127035).  Saving model ...\n",
      "[ 42/100] train_loss: 0.11383 valid_loss: 0.13283 test_loss: 0.09393 \n",
      "[ 43/100] train_loss: 0.11225 valid_loss: 0.12858 test_loss: 0.09364 \n",
      "[ 44/100] train_loss: 0.11625 valid_loss: 0.12412 test_loss: 0.09145 \n",
      "Validation loss decreased (0.127035 --> 0.124116).  Saving model ...\n",
      "[ 45/100] train_loss: 0.10731 valid_loss: 0.12548 test_loss: 0.09610 \n",
      "[ 46/100] train_loss: 0.10983 valid_loss: 0.12695 test_loss: 0.09105 \n",
      "[ 47/100] train_loss: 0.10975 valid_loss: 0.12593 test_loss: 0.09225 \n",
      "[ 48/100] train_loss: 0.11090 valid_loss: 0.12335 test_loss: 0.09396 \n",
      "Validation loss decreased (0.124116 --> 0.123351).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10678 valid_loss: 0.12251 test_loss: 0.08993 \n",
      "Validation loss decreased (0.123351 --> 0.122509).  Saving model ...\n",
      "[ 50/100] train_loss: 0.10877 valid_loss: 0.12413 test_loss: 0.09229 \n",
      "[ 51/100] train_loss: 0.10759 valid_loss: 0.12269 test_loss: 0.09286 \n",
      "[ 52/100] train_loss: 0.10786 valid_loss: 0.12303 test_loss: 0.09175 \n",
      "[ 53/100] train_loss: 0.10747 valid_loss: 0.12382 test_loss: 0.09275 \n",
      "[ 54/100] train_loss: 0.10591 valid_loss: 0.12108 test_loss: 0.08788 \n",
      "Validation loss decreased (0.122509 --> 0.121078).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10664 valid_loss: 0.12318 test_loss: 0.09346 \n",
      "[ 56/100] train_loss: 0.10684 valid_loss: 0.12159 test_loss: 0.09147 \n",
      "[ 57/100] train_loss: 0.10759 valid_loss: 0.12191 test_loss: 0.09272 \n",
      "[ 58/100] train_loss: 0.10726 valid_loss: 0.11851 test_loss: 0.09354 \n",
      "Validation loss decreased (0.121078 --> 0.118505).  Saving model ...\n",
      "[ 59/100] train_loss: 0.10574 valid_loss: 0.12000 test_loss: 0.09498 \n",
      "[ 60/100] train_loss: 0.10308 valid_loss: 0.12110 test_loss: 0.09318 \n",
      "[ 61/100] train_loss: 0.10434 valid_loss: 0.11841 test_loss: 0.09238 \n",
      "Validation loss decreased (0.118505 --> 0.118414).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10645 valid_loss: 0.11738 test_loss: 0.08957 \n",
      "Validation loss decreased (0.118414 --> 0.117379).  Saving model ...\n",
      "[ 63/100] train_loss: 0.09866 valid_loss: 0.11821 test_loss: 0.09062 \n",
      "[ 64/100] train_loss: 0.10436 valid_loss: 0.11910 test_loss: 0.09516 \n",
      "[ 65/100] train_loss: 0.09874 valid_loss: 0.11682 test_loss: 0.09010 \n",
      "Validation loss decreased (0.117379 --> 0.116817).  Saving model ...\n",
      "[ 66/100] train_loss: 0.10265 valid_loss: 0.11339 test_loss: 0.08746 \n",
      "Validation loss decreased (0.116817 --> 0.113388).  Saving model ...\n",
      "[ 67/100] train_loss: 0.10018 valid_loss: 0.11542 test_loss: 0.08994 \n",
      "[ 68/100] train_loss: 0.09878 valid_loss: 0.11693 test_loss: 0.08825 \n",
      "[ 69/100] train_loss: 0.10044 valid_loss: 0.11556 test_loss: 0.08969 \n",
      "[ 70/100] train_loss: 0.09764 valid_loss: 0.11252 test_loss: 0.08596 \n",
      "Validation loss decreased (0.113388 --> 0.112516).  Saving model ...\n",
      "[ 71/100] train_loss: 0.09954 valid_loss: 0.11405 test_loss: 0.08711 \n",
      "[ 72/100] train_loss: 0.09831 valid_loss: 0.11362 test_loss: 0.08918 \n",
      "[ 73/100] train_loss: 0.09778 valid_loss: 0.11473 test_loss: 0.08847 \n",
      "[ 74/100] train_loss: 0.09845 valid_loss: 0.11157 test_loss: 0.08523 \n",
      "Validation loss decreased (0.112516 --> 0.111566).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09798 valid_loss: 0.11202 test_loss: 0.08817 \n",
      "[ 76/100] train_loss: 0.09584 valid_loss: 0.11224 test_loss: 0.08921 \n",
      "[ 77/100] train_loss: 0.09722 valid_loss: 0.11281 test_loss: 0.08890 \n",
      "[ 78/100] train_loss: 0.09458 valid_loss: 0.11120 test_loss: 0.08977 \n",
      "Validation loss decreased (0.111566 --> 0.111200).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09523 valid_loss: 0.11268 test_loss: 0.09001 \n",
      "[ 80/100] train_loss: 0.09572 valid_loss: 0.11312 test_loss: 0.08964 \n",
      "[ 81/100] train_loss: 0.09535 valid_loss: 0.11018 test_loss: 0.08741 \n",
      "Validation loss decreased (0.111200 --> 0.110179).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09469 valid_loss: 0.10956 test_loss: 0.08843 \n",
      "Validation loss decreased (0.110179 --> 0.109563).  Saving model ...\n",
      "[ 83/100] train_loss: 0.09394 valid_loss: 0.11390 test_loss: 0.08718 \n",
      "[ 84/100] train_loss: 0.09480 valid_loss: 0.11347 test_loss: 0.09115 \n",
      "[ 85/100] train_loss: 0.09289 valid_loss: 0.11122 test_loss: 0.08582 \n",
      "[ 86/100] train_loss: 0.09500 valid_loss: 0.10953 test_loss: 0.09043 \n",
      "Validation loss decreased (0.109563 --> 0.109527).  Saving model ...\n",
      "[ 87/100] train_loss: 0.09497 valid_loss: 0.10921 test_loss: 0.08717 \n",
      "Validation loss decreased (0.109527 --> 0.109210).  Saving model ...\n",
      "[ 88/100] train_loss: 0.09143 valid_loss: 0.10789 test_loss: 0.08675 \n",
      "Validation loss decreased (0.109210 --> 0.107885).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09304 valid_loss: 0.10951 test_loss: 0.08956 \n",
      "[ 90/100] train_loss: 0.09115 valid_loss: 0.10710 test_loss: 0.08798 \n",
      "Validation loss decreased (0.107885 --> 0.107096).  Saving model ...\n",
      "[ 91/100] train_loss: 0.09296 valid_loss: 0.10668 test_loss: 0.08716 \n",
      "Validation loss decreased (0.107096 --> 0.106683).  Saving model ...\n",
      "[ 92/100] train_loss: 0.09185 valid_loss: 0.10929 test_loss: 0.08657 \n",
      "[ 93/100] train_loss: 0.09067 valid_loss: 0.10864 test_loss: 0.08546 \n",
      "[ 94/100] train_loss: 0.09070 valid_loss: 0.10826 test_loss: 0.08622 \n",
      "[ 95/100] train_loss: 0.09169 valid_loss: 0.10699 test_loss: 0.08652 \n",
      "[ 96/100] train_loss: 0.09205 valid_loss: 0.10651 test_loss: 0.08240 \n",
      "Validation loss decreased (0.106683 --> 0.106512).  Saving model ...\n",
      "[ 97/100] train_loss: 0.09153 valid_loss: 0.10696 test_loss: 0.08073 \n",
      "[ 98/100] train_loss: 0.09260 valid_loss: 0.10622 test_loss: 0.08240 \n",
      "Validation loss decreased (0.106512 --> 0.106220).  Saving model ...\n",
      "[ 99/100] train_loss: 0.09255 valid_loss: 0.10632 test_loss: 0.08340 \n",
      "[100/100] train_loss: 0.08974 valid_loss: 0.10873 test_loss: 0.08568 \n",
      "TRAINING MODEL 19\n",
      "[  1/100] train_loss: 0.57311 valid_loss: 0.51130 test_loss: 0.45914 \n",
      "Validation loss decreased (inf --> 0.511299).  Saving model ...\n",
      "[  2/100] train_loss: 0.41186 valid_loss: 0.37120 test_loss: 0.30319 \n",
      "Validation loss decreased (0.511299 --> 0.371202).  Saving model ...\n",
      "[  3/100] train_loss: 0.31288 valid_loss: 0.30301 test_loss: 0.22727 \n",
      "Validation loss decreased (0.371202 --> 0.303010).  Saving model ...\n",
      "[  4/100] train_loss: 0.25698 valid_loss: 0.26947 test_loss: 0.19097 \n",
      "Validation loss decreased (0.303010 --> 0.269473).  Saving model ...\n",
      "[  5/100] train_loss: 0.23448 valid_loss: 0.25229 test_loss: 0.16677 \n",
      "Validation loss decreased (0.269473 --> 0.252286).  Saving model ...\n",
      "[  6/100] train_loss: 0.21308 valid_loss: 0.23322 test_loss: 0.15399 \n",
      "Validation loss decreased (0.252286 --> 0.233219).  Saving model ...\n",
      "[  7/100] train_loss: 0.19932 valid_loss: 0.22019 test_loss: 0.14529 \n",
      "Validation loss decreased (0.233219 --> 0.220190).  Saving model ...\n",
      "[  8/100] train_loss: 0.18892 valid_loss: 0.21182 test_loss: 0.14044 \n",
      "Validation loss decreased (0.220190 --> 0.211817).  Saving model ...\n",
      "[  9/100] train_loss: 0.18198 valid_loss: 0.19341 test_loss: 0.12993 \n",
      "Validation loss decreased (0.211817 --> 0.193415).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17496 valid_loss: 0.18755 test_loss: 0.12963 \n",
      "Validation loss decreased (0.193415 --> 0.187549).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16566 valid_loss: 0.17955 test_loss: 0.12234 \n",
      "Validation loss decreased (0.187549 --> 0.179554).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16261 valid_loss: 0.18087 test_loss: 0.11927 \n",
      "[ 13/100] train_loss: 0.15549 valid_loss: 0.17626 test_loss: 0.11684 \n",
      "Validation loss decreased (0.179554 --> 0.176260).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14669 valid_loss: 0.16381 test_loss: 0.11663 \n",
      "Validation loss decreased (0.176260 --> 0.163812).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14841 valid_loss: 0.16685 test_loss: 0.11378 \n",
      "[ 16/100] train_loss: 0.14436 valid_loss: 0.16487 test_loss: 0.11384 \n",
      "[ 17/100] train_loss: 0.13963 valid_loss: 0.15844 test_loss: 0.11484 \n",
      "Validation loss decreased (0.163812 --> 0.158439).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13985 valid_loss: 0.16313 test_loss: 0.10998 \n",
      "[ 19/100] train_loss: 0.13629 valid_loss: 0.15019 test_loss: 0.10811 \n",
      "Validation loss decreased (0.158439 --> 0.150187).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13223 valid_loss: 0.15029 test_loss: 0.10856 \n",
      "[ 21/100] train_loss: 0.12951 valid_loss: 0.14397 test_loss: 0.10605 \n",
      "Validation loss decreased (0.150187 --> 0.143966).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12793 valid_loss: 0.14312 test_loss: 0.10162 \n",
      "Validation loss decreased (0.143966 --> 0.143123).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12523 valid_loss: 0.14217 test_loss: 0.10702 \n",
      "Validation loss decreased (0.143123 --> 0.142174).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12835 valid_loss: 0.14266 test_loss: 0.10620 \n",
      "[ 25/100] train_loss: 0.12256 valid_loss: 0.13812 test_loss: 0.10339 \n",
      "Validation loss decreased (0.142174 --> 0.138122).  Saving model ...\n",
      "[ 26/100] train_loss: 0.12460 valid_loss: 0.13942 test_loss: 0.10525 \n",
      "[ 27/100] train_loss: 0.12380 valid_loss: 0.14263 test_loss: 0.11000 \n",
      "[ 28/100] train_loss: 0.12206 valid_loss: 0.13521 test_loss: 0.10304 \n",
      "Validation loss decreased (0.138122 --> 0.135208).  Saving model ...\n",
      "[ 29/100] train_loss: 0.12052 valid_loss: 0.13903 test_loss: 0.09961 \n",
      "[ 30/100] train_loss: 0.11652 valid_loss: 0.13275 test_loss: 0.09542 \n",
      "Validation loss decreased (0.135208 --> 0.132754).  Saving model ...\n",
      "[ 31/100] train_loss: 0.11619 valid_loss: 0.13295 test_loss: 0.09767 \n",
      "[ 32/100] train_loss: 0.11368 valid_loss: 0.13192 test_loss: 0.09781 \n",
      "Validation loss decreased (0.132754 --> 0.131921).  Saving model ...\n",
      "[ 33/100] train_loss: 0.11453 valid_loss: 0.13158 test_loss: 0.09599 \n",
      "Validation loss decreased (0.131921 --> 0.131581).  Saving model ...\n",
      "[ 34/100] train_loss: 0.11487 valid_loss: 0.12974 test_loss: 0.09694 \n",
      "Validation loss decreased (0.131581 --> 0.129739).  Saving model ...\n",
      "[ 35/100] train_loss: 0.11178 valid_loss: 0.12794 test_loss: 0.09480 \n",
      "Validation loss decreased (0.129739 --> 0.127937).  Saving model ...\n",
      "[ 36/100] train_loss: 0.11481 valid_loss: 0.12533 test_loss: 0.09498 \n",
      "Validation loss decreased (0.127937 --> 0.125326).  Saving model ...\n",
      "[ 37/100] train_loss: 0.11407 valid_loss: 0.12824 test_loss: 0.09630 \n",
      "[ 38/100] train_loss: 0.11435 valid_loss: 0.12498 test_loss: 0.09208 \n",
      "Validation loss decreased (0.125326 --> 0.124979).  Saving model ...\n",
      "[ 39/100] train_loss: 0.11042 valid_loss: 0.12903 test_loss: 0.09787 \n",
      "[ 40/100] train_loss: 0.10797 valid_loss: 0.12614 test_loss: 0.09800 \n",
      "[ 41/100] train_loss: 0.11179 valid_loss: 0.12134 test_loss: 0.08947 \n",
      "Validation loss decreased (0.124979 --> 0.121345).  Saving model ...\n",
      "[ 42/100] train_loss: 0.10990 valid_loss: 0.12553 test_loss: 0.09408 \n",
      "[ 43/100] train_loss: 0.10584 valid_loss: 0.12444 test_loss: 0.08889 \n",
      "[ 44/100] train_loss: 0.10584 valid_loss: 0.12485 test_loss: 0.09143 \n",
      "[ 45/100] train_loss: 0.10679 valid_loss: 0.12665 test_loss: 0.08851 \n",
      "[ 46/100] train_loss: 0.10486 valid_loss: 0.12018 test_loss: 0.09140 \n",
      "Validation loss decreased (0.121345 --> 0.120182).  Saving model ...\n",
      "[ 47/100] train_loss: 0.10675 valid_loss: 0.12541 test_loss: 0.09018 \n",
      "[ 48/100] train_loss: 0.10668 valid_loss: 0.11928 test_loss: 0.08745 \n",
      "Validation loss decreased (0.120182 --> 0.119280).  Saving model ...\n",
      "[ 49/100] train_loss: 0.10526 valid_loss: 0.12136 test_loss: 0.09044 \n",
      "[ 50/100] train_loss: 0.10414 valid_loss: 0.11755 test_loss: 0.08716 \n",
      "Validation loss decreased (0.119280 --> 0.117548).  Saving model ...\n",
      "[ 51/100] train_loss: 0.10527 valid_loss: 0.11714 test_loss: 0.08821 \n",
      "Validation loss decreased (0.117548 --> 0.117137).  Saving model ...\n",
      "[ 52/100] train_loss: 0.10418 valid_loss: 0.11930 test_loss: 0.08847 \n",
      "[ 53/100] train_loss: 0.10405 valid_loss: 0.11790 test_loss: 0.08512 \n",
      "[ 54/100] train_loss: 0.10037 valid_loss: 0.11656 test_loss: 0.08670 \n",
      "Validation loss decreased (0.117137 --> 0.116563).  Saving model ...\n",
      "[ 55/100] train_loss: 0.10176 valid_loss: 0.11611 test_loss: 0.08684 \n",
      "Validation loss decreased (0.116563 --> 0.116111).  Saving model ...\n",
      "[ 56/100] train_loss: 0.10379 valid_loss: 0.11494 test_loss: 0.08550 \n",
      "Validation loss decreased (0.116111 --> 0.114938).  Saving model ...\n",
      "[ 57/100] train_loss: 0.10080 valid_loss: 0.11686 test_loss: 0.08836 \n",
      "[ 58/100] train_loss: 0.10032 valid_loss: 0.11375 test_loss: 0.08597 \n",
      "Validation loss decreased (0.114938 --> 0.113748).  Saving model ...\n",
      "[ 59/100] train_loss: 0.09960 valid_loss: 0.11343 test_loss: 0.08707 \n",
      "Validation loss decreased (0.113748 --> 0.113426).  Saving model ...\n",
      "[ 60/100] train_loss: 0.10025 valid_loss: 0.11675 test_loss: 0.08815 \n",
      "[ 61/100] train_loss: 0.09907 valid_loss: 0.11331 test_loss: 0.08474 \n",
      "Validation loss decreased (0.113426 --> 0.113305).  Saving model ...\n",
      "[ 62/100] train_loss: 0.10121 valid_loss: 0.11513 test_loss: 0.08476 \n",
      "[ 63/100] train_loss: 0.10036 valid_loss: 0.11467 test_loss: 0.08157 \n",
      "[ 64/100] train_loss: 0.09763 valid_loss: 0.11568 test_loss: 0.08229 \n",
      "[ 65/100] train_loss: 0.09729 valid_loss: 0.11232 test_loss: 0.08453 \n",
      "Validation loss decreased (0.113305 --> 0.112322).  Saving model ...\n",
      "[ 66/100] train_loss: 0.09628 valid_loss: 0.10972 test_loss: 0.08226 \n",
      "Validation loss decreased (0.112322 --> 0.109723).  Saving model ...\n",
      "[ 67/100] train_loss: 0.09450 valid_loss: 0.11682 test_loss: 0.08347 \n",
      "[ 68/100] train_loss: 0.09722 valid_loss: 0.11193 test_loss: 0.08358 \n",
      "[ 69/100] train_loss: 0.09759 valid_loss: 0.11113 test_loss: 0.08359 \n",
      "[ 70/100] train_loss: 0.09373 valid_loss: 0.11469 test_loss: 0.08530 \n",
      "[ 71/100] train_loss: 0.09755 valid_loss: 0.11094 test_loss: 0.08296 \n",
      "[ 72/100] train_loss: 0.09736 valid_loss: 0.10915 test_loss: 0.08293 \n",
      "Validation loss decreased (0.109723 --> 0.109152).  Saving model ...\n",
      "[ 73/100] train_loss: 0.09491 valid_loss: 0.10941 test_loss: 0.08458 \n",
      "[ 74/100] train_loss: 0.09496 valid_loss: 0.10821 test_loss: 0.07974 \n",
      "Validation loss decreased (0.109152 --> 0.108215).  Saving model ...\n",
      "[ 75/100] train_loss: 0.09411 valid_loss: 0.10810 test_loss: 0.08182 \n",
      "Validation loss decreased (0.108215 --> 0.108099).  Saving model ...\n",
      "[ 76/100] train_loss: 0.09711 valid_loss: 0.10860 test_loss: 0.08007 \n",
      "[ 77/100] train_loss: 0.09520 valid_loss: 0.10730 test_loss: 0.08035 \n",
      "Validation loss decreased (0.108099 --> 0.107303).  Saving model ...\n",
      "[ 78/100] train_loss: 0.09619 valid_loss: 0.10712 test_loss: 0.07964 \n",
      "Validation loss decreased (0.107303 --> 0.107124).  Saving model ...\n",
      "[ 79/100] train_loss: 0.09341 valid_loss: 0.10841 test_loss: 0.07870 \n",
      "[ 80/100] train_loss: 0.09310 valid_loss: 0.10598 test_loss: 0.07789 \n",
      "Validation loss decreased (0.107124 --> 0.105977).  Saving model ...\n",
      "[ 81/100] train_loss: 0.09229 valid_loss: 0.10593 test_loss: 0.07747 \n",
      "Validation loss decreased (0.105977 --> 0.105929).  Saving model ...\n",
      "[ 82/100] train_loss: 0.09249 valid_loss: 0.10709 test_loss: 0.07881 \n",
      "[ 83/100] train_loss: 0.08810 valid_loss: 0.10508 test_loss: 0.07719 \n",
      "Validation loss decreased (0.105929 --> 0.105078).  Saving model ...\n",
      "[ 84/100] train_loss: 0.09241 valid_loss: 0.10610 test_loss: 0.07736 \n",
      "[ 85/100] train_loss: 0.09343 valid_loss: 0.10372 test_loss: 0.07683 \n",
      "Validation loss decreased (0.105078 --> 0.103717).  Saving model ...\n",
      "[ 86/100] train_loss: 0.09068 valid_loss: 0.10488 test_loss: 0.07716 \n",
      "[ 87/100] train_loss: 0.08967 valid_loss: 0.10475 test_loss: 0.07728 \n",
      "[ 88/100] train_loss: 0.09039 valid_loss: 0.10354 test_loss: 0.07674 \n",
      "Validation loss decreased (0.103717 --> 0.103541).  Saving model ...\n",
      "[ 89/100] train_loss: 0.09292 valid_loss: 0.10207 test_loss: 0.07491 \n",
      "Validation loss decreased (0.103541 --> 0.102070).  Saving model ...\n",
      "[ 90/100] train_loss: 0.09026 valid_loss: 0.10471 test_loss: 0.07760 \n",
      "[ 91/100] train_loss: 0.08932 valid_loss: 0.10497 test_loss: 0.07904 \n",
      "[ 92/100] train_loss: 0.09114 valid_loss: 0.10494 test_loss: 0.07902 \n",
      "[ 93/100] train_loss: 0.08922 valid_loss: 0.10320 test_loss: 0.07652 \n",
      "[ 94/100] train_loss: 0.08916 valid_loss: 0.10279 test_loss: 0.07635 \n",
      "[ 95/100] train_loss: 0.08925 valid_loss: 0.10510 test_loss: 0.07707 \n",
      "[ 96/100] train_loss: 0.08792 valid_loss: 0.10231 test_loss: 0.07686 \n",
      "[ 97/100] train_loss: 0.08620 valid_loss: 0.10454 test_loss: 0.07790 \n",
      "[ 98/100] train_loss: 0.08655 valid_loss: 0.10175 test_loss: 0.07459 \n",
      "Validation loss decreased (0.102070 --> 0.101751).  Saving model ...\n",
      "[ 99/100] train_loss: 0.08754 valid_loss: 0.10534 test_loss: 0.07712 \n",
      "[100/100] train_loss: 0.08849 valid_loss: 0.10121 test_loss: 0.07513 \n",
      "Validation loss decreased (0.101751 --> 0.101213).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train_unseen\n",
    "valid_loader = dl_valid_unseen\n",
    "test_loader = dl_test_unseen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model = PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_unseen_status_%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.plot(test_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTPNet(\n",
       "  (encoder1): Encoder(\n",
       "    (conv): Conv1d(1, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder2): Encoder(\n",
       "    (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder3): Encoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder4): Encoder(\n",
       "    (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool1): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool2): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool3): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(20,), stride=(20,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool4): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): ConvTranspose1d(512, 32, kernel_size=(8,), stride=(8,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (activation): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PTPNet(1,3,32).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hats_seen = [[] for i in range(3)]\n",
    "s_hats_unseen = [[] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKDALE_seen_status_11.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.876\n",
      "Precision : 0.862\n",
      "Recall    : 0.890\n",
      "Accuracy  : 0.886\n",
      "MCC       : 0.770\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.921\n",
      "Precision : 0.949\n",
      "Recall    : 0.895\n",
      "Accuracy  : 0.996\n",
      "MCC       : 0.920\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.981\n",
      "Precision : 0.975\n",
      "Recall    : 0.986\n",
      "Accuracy  : 0.997\n",
      "MCC       : 0.979\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "\n",
    "thr = 0.5\n",
    "for i in range(1):\n",
    "    filename = 'UKDALE_seen_status_11.pth'\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        \n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        \n",
    "        s_hats_seen[a] = copy.deepcopy(s_hat)\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f' %(np.mean(scores[i]['F1'])))\n",
    "    print('Precision : %.3f' %(np.mean(scores[i]['Precision'])))\n",
    "    print('Recall    : %.3f' %(np.mean(scores[i]['Recall'])))\n",
    "    print('Accuracy  : %.3f' %(np.mean(scores[i]['Accuracy'])))\n",
    "    print('MCC       : %.3f' %(np.mean(scores[i]['MCC'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKDALE_unseen_status_19.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.883\n",
      "Precision : 0.891\n",
      "Recall    : 0.875\n",
      "Accuracy  : 0.912\n",
      "MCC       : 0.812\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.850\n",
      "Precision : 0.829\n",
      "Recall    : 0.873\n",
      "Accuracy  : 0.991\n",
      "MCC       : 0.846\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.859\n",
      "Precision : 0.940\n",
      "Recall    : 0.791\n",
      "Accuracy  : 0.997\n",
      "MCC       : 0.861\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for i in range(1):\n",
    "    filename = 'UKDALE_unseen_status_19.pth'\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        \n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        \n",
    "        s_hats_unseen[a] = copy.deepcopy(s_hat)\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f' %(np.mean(scores[i]['F1'])))\n",
    "    print('Precision : %.3f' %(np.mean(scores[i]['Precision'])))\n",
    "    print('Recall    : %.3f' %(np.mean(scores[i]['Recall'])))\n",
    "    print('Accuracy  : %.3f' %(np.mean(scores[i]['Accuracy'])))\n",
    "    print('MCC       : %.3f' %(np.mean(scores[i]['MCC'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_test_status(dl, s_hats, appliance=0, type='seen', break_at=0):\n",
    "    dataiter = iter(dl)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    break_index = None\n",
    "    for idx, (x, y, s) in enumerate(dataiter):\n",
    "        if y[0, :, appliance].sum() > 0 and cnt >= break_at:\n",
    "            break_index = idx\n",
    "            break\n",
    "        if s[0, :, appliance].sum() > 0 and cnt >= break_at:\n",
    "            break_index = idx\n",
    "            break\n",
    "        cnt += 1\n",
    "        \n",
    "    print(\"Break index:\", break_index)\n",
    "\n",
    "    # Assuming s_hat is the nparray you want to plot\n",
    "    start_index = break_index * 480\n",
    "    end_index = start_index + 480\n",
    "\n",
    "    plt.plot(np.arange(-BORDER, SEQ_LEN + BORDER), x[0, :].detach().numpy(), 'k-', label='Aggregate')\n",
    "    plt.plot(s[0, :, appliance].detach().numpy(), label='Actual Status')\n",
    "    plt.plot(np.arange(start_index - start_index, end_index - start_index), s_hats[appliance][start_index:end_index], 'r-', label='Predicted Status')\n",
    "\n",
    "    plt.title('Washing Machine Actual Status vs Predicted Status [unseen]')\n",
    "    plt.xlabel('Window Size')\n",
    "    plt.ylabel('Status')\n",
    "\n",
    "    plt.ylim([-0.5, 1.5])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House 2 unseen Washing Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 13\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_test_status(dl_house_total[1], s_hats_unseen, 2, 'unseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House 2 unseen Fridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_test_status(dl_house_total[1], s_hats_unseen, 0, 'unseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House 2 unseen Dish Washer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_test_status(dl_house_total[1], s_hats_unseen, 1, 'unseen', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House 1 seen Dish Washer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_test_status(dl_house_test[0], s_hats_seen, 1, 'seen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House 1 seen Fridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_test_status(dl_house_test[0], s_hats_seen, 0, 'seen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House 1 seen Washing Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break index: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_test_status(dl_house_test[0], s_hats_seen, 2, 'seen', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TPNILM_UKDALE_run.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
